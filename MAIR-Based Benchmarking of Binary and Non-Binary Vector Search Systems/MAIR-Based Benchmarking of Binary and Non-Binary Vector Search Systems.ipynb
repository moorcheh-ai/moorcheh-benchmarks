{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM4qzDSaSOevVINGt+ZxryR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MAIR Dataset Download and Organization"],"metadata":{"id":"ut9T_OAl5i5E"}},{"cell_type":"markdown","source":["## Download MAIR Datasets and Queries from Hugging Face to Google Drive"],"metadata":{"id":"rxcdoojQ5eq8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K50BV_Is5N51"},"outputs":[],"source":["# 1ï¸âƒ£ Mount Google Drive to a new empty folder\n","# This step allows Colab to access files stored in your Google Drive.\n","# A new directory '/content/gdrive' is created, which will contain your Drive files.\n","# If you've previously mounted your Drive, Colab might remember the authentication.\n","from google.colab import drive\n","drive.mount('/content/gdrive')  # Use a new mount point to avoid conflicts\n","\n","# 2ï¸âƒ£ Install Hugging Face hub CLI\n","# The Hugging Face Hub Command Line Interface (CLI) is used to interact with the Hugging Face Hub,\n","# which hosts various datasets and models. We upgrade it to ensure we have the latest features.\n","# This installation only needs to be done once per Colab session.\n","!pip install huggingface-hub --upgrade\n","\n","# 3ï¸âƒ£ Set local directories in your Google Drive\n","# These variables define the paths within your Google Drive where the downloaded datasets will be stored.\n","# You can customize these paths if you prefer a different location within your 'My Drive' folder.\n","# For example: \"/content/drive/MyDrive/MyProject/MAIR_Datasets/MAIR-Queries\"\n","# âž¡ï¸ USER CONFIGURATION: Modify these paths if you want to store the data elsewhere in your Google Drive.\n","queries_dir = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Queries\"\n","docs_dir = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Docs\"\n","\n","# 4ï¸âƒ£ Download datasets using HF CLI\n","# This command uses the Hugging Face CLI to download two datasets from the 'MAIR-Bench' organization:\n","# - 'MAIR-Bench/MAIR-Queries': Contains query data for various benchmarks.\n","# - 'MAIR-Bench/MAIR-Docs': Contains document data that corresponds to the queries.\n","# The '--repo-type dataset' flag specifies that we are downloading datasets.\n","# The '--local-dir' flag specifies the local path where the datasets will be saved within your mounted Google Drive.\n","# This might take some time depending on your internet connection and the size of the datasets.\n","# Progress will be displayed in the output.\n","!hf download MAIR-Bench/MAIR-Queries --repo-type dataset --local-dir \"{queries_dir}\"\n","!hf download MAIR-Bench/MAIR-Docs --repo-type dataset --local-dir \"{docs_dir}\"\n","\n","print(\"âœ… Download completed!\")"]},{"cell_type":"markdown","source":["## Combine MAIR Docs and Queries from Google Drive into a Single Directory"],"metadata":{"id":"yvBZTGPD-efi"}},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import shutil\n","\n","# 1ï¸âƒ£ Mount Google Drive\n","# This step allows Colab to access files stored in your Google Drive.\n","# 'force_remount=True' ensures that the drive is always mounted, even if it was previously mounted,\n","# which can be useful if you restart the Colab runtime.\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 2ï¸âƒ£ Define paths for MAIR datasets and the combined output\n","# These paths specify where the raw downloaded data is and where the processed, combined data will be saved.\n","# You can customize SAVE_PATH to store the combined datasets in a different location in your Google Drive.\n","MAIR_DOCS_PATH = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Docs\"\n","MAIR_QUERIES_PATH = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Queries\"\n","SAVE_PATH = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Combined\"\n","\n","# 3ï¸âƒ£ Create the save folder if it doesn't already exist\n","# os.makedirs creates directories recursively. 'exist_ok=True' prevents an error if the directory already exists.\n","# This ensures the target directory for combined datasets is ready.\n","os.makedirs(SAVE_PATH, exist_ok=True)\n","\n","# 4ï¸âƒ£ List subfolders (individual datasets) within the Docs and Queries paths\n","# This identifies each specific MAIR dataset (e.g., 'TREC_DL_2019', 'NQ') present in both locations.\n","# It ensures we only process datasets for which both documents and queries are available.\n","docs_folders = [f for f in os.listdir(MAIR_DOCS_PATH) if os.path.isdir(os.path.join(MAIR_DOCS_PATH, f))]\n","queries_folders = [f for f in os.listdir(MAIR_QUERIES_PATH) if os.path.isdir(os.path.join(MAIR_QUERIES_PATH, f))]\n","\n","# 5ï¸âƒ£ Find common datasets that exist in both Docs and Queries folders\n","# We only combine datasets for which both document and query data are available.\n","# This prevents errors from incomplete datasets. The list is sorted for consistent processing.\n","common_datasets = sorted(list(set(docs_folders) & set(queries_folders)))\n","print(\"Common MAIR Datasets found:\", common_datasets)\n","\n","# 6ï¸âƒ£ Iterate through each common dataset and copy its contents\n","# This loop processes each dataset found in both 'MAIR-Docs' and 'MAIR-Queries',\n","# creating a unified structure in the 'MAIR-Combined' folder.\n","# Each dataset gets its own folder containing 'docs' and 'queries' subdirectories.\n","for dataset in common_datasets:\n","    # Define the target path for the current combined dataset\n","    dataset_save_path = os.path.join(SAVE_PATH, dataset)\n","    # Create the dataset-specific directory within the combined save path\n","    os.makedirs(dataset_save_path, exist_ok=True)\n","\n","    # Copy docs folder\n","    src_docs = os.path.join(MAIR_DOCS_PATH, dataset) # Source path for documents of the current dataset\n","    dst_docs = os.path.join(dataset_save_path, \"docs\") # Destination path for documents within the combined folder\n","    # If the destination docs folder already exists, remove it to ensure a clean copy\n","    if os.path.exists(dst_docs):\n","        shutil.rmtree(dst_docs)\n","    # Copy the entire directory tree from source to destination, including all files and subfolders.\n","    print(f\"Copying documents for {dataset} from {src_docs} to {dst_docs}\")\n","    shutil.copytree(src_docs, dst_docs)\n","\n","    # Copy queries folder\n","    src_queries = os.path.join(MAIR_QUERIES_PATH, dataset) # Source path for queries of the current dataset\n","    dst_queries = os.path.join(dataset_save_path, \"queries\") # Destination path for queries within the combined folder\n","    # If the destination queries folder already exists, remove it to ensure a clean copy\n","    if os.path.exists(dst_queries):\n","        shutil.rmtree(dst_queries)\n","    # Copy the entire directory tree from source to destination.\n","    print(f\"Copying queries for {dataset} from {src_queries} to {dst_queries}\")\n","    shutil.copytree(src_queries, dst_queries)\n","\n","print(f\"\\nâœ… All common datasets copied successfully with their original 'docs' and 'queries' subfolders to:\\n{SAVE_PATH}\")"],"metadata":{"id":"27u3npMT-hQs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vector (Binary) Benchmarking Based on MAIR Dataset"],"metadata":{"id":"OUqThJVEAJgS"}},{"cell_type":"markdown","source":["## Vector (Binary) Search in Moorcheh + Pinecone (with Cohere) + Elasticsearch"],"metadata":{"id":"RFAFI9jzBDbz"}},{"cell_type":"code","source":["# ============================================================\n","# BEIR + MAIR Benchmark - Binary Embeddings Only\n","# Moorcheh vs Pinecone vs Elasticsearch Vector Comparison\n","# Sign-based 1-bit Binarization (>=0 -> 1, <0 -> 0)\n","# ============================================================\n","\n","# -------------------- 1. Install Necessary Libraries --------------------\n","# This section ensures all required Python packages are installed in the Colab environment.\n","# The installation might take a few moments, especially on the first run.\n","\n","# `beir`: A comprehensive benchmarking framework for information retrieval tasks.\n","# `moorcheh-sdk`: The official SDK for interacting with the Moorcheh vector database.\n","# `cohere`: Used for generating high-quality text embeddings and potentially reranking results.\n","# `pinecone`: The client library for connecting to the Pinecone vector database.\n","# `elasticsearch`: The client library for interacting with Elasticsearch, used here for vector search capabilities.\n","# `numpy`: A fundamental library for numerical computing in Python, essential for handling embedding arrays.\n","!pip install beir moorcheh-sdk cohere pinecone elasticsearch numpy\n","\n","import os\n","import gc\n","import time\n","import statistics\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from beir import util\n","from beir.datasets.data_loader import GenericDataLoader\n","from beir.retrieval.evaluation import EvaluateRetrieval\n","\n","# -------------------- 2. User / Environment Setup --------------------\n","# This section configures paths and retrieves API keys, adapting to either Google Colab or a local environment.\n","\n","# DRIVE_PATH: Defines where benchmark results will be saved. Default is the current directory.\n","# If running in Google Colab, it will be automatically updated to a path in your mounted Google Drive.\n","DRIVE_PATH = \".\"\n","\n","# MAIR_COMBINED_PATH: **User Customizable Path**\n","# This is the path to your combined MAIR datasets in Google Drive. It should match the SAVE_PATH from the\n","# 'Combine MAIR Docs and Queries' step. Ensure this path is correct for your setup.\n","MAIR_COMBINED_PATH = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Combined\"\n","\n","# BEIR_PATH: Local directory where BEIR datasets will be downloaded and stored.\n","# You typically don't need to change this unless you want BEIR datasets stored elsewhere locally.\n","BEIR_PATH = \"./datasets\"\n","\n","# Initialize an empty dictionary to store API keys and configurations for various services.\n","api_keys = {}\n","\n","try:\n","    # This block attempts to configure for Google Colab, leveraging its `drive` and `userdata` (Secrets) features.\n","    from google.colab import drive, userdata as colab_userdata\n","\n","    try:\n","        # Mount Google Drive to allow Colab to access your files. If you've mounted it recently,\n","        # Colab might remember the authentication. `force_remount=True` ensures a fresh mount if needed.\n","        drive.mount('/content/gdrive') # Using /content/gdrive as the mount point\n","        print(\"âœ… Google Drive mounted successfully at /content/gdrive.\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Drive mount attempt raised: {e}. Continuing without Drive mount, results will be saved locally if Drive path is not writable.\")\n","\n","    # DRIVE_PATH: **User Customizable Path**\n","    # If running in Colab, results will be saved here within your Google Drive.\n","    # You can customize this path (e.g., '/content/gdrive/MyDrive/MyProject/BenchmarkResults')\n","    # to organize your benchmark outputs effectively.\n","    DRIVE_PATH = '/content/gdrive/MyDrive/Moorcheh/Benchmark_Results/Pinecone.Binary'\n","    os.makedirs(DRIVE_PATH, exist_ok=True) # Creates the directory if it doesn't exist.\n","    print(f\"âœ… Running in Colab. Results will be saved to: {DRIVE_PATH}\")\n","\n","    # Retrieve API keys from Colab secrets. **User Action Required**:\n","    # To use this feature, you MUST add your API keys to Colab's \"Secrets\" panel.\n","    # Look for the key icon (ðŸ”‘) on the left sidebar of your Colab notebook.\n","    # Name your secrets EXACTLY as follows:\n","    # - `MOORCHEH_API_KEY` for Moorcheh\n","    # - `COHERE_API_KEY` for Cohere (essential for embedding generation)\n","    # - `PINECONE_API_KEY` or `PINECONE_API_KEY2` for Pinecone\n","    # - `ELASTIC_URL` for Elasticsearch endpoint (e.g., 'https://your-es-cluster.es.io:9243')\n","    # - `ELASTIC_API_KEY` for Elasticsearch API Key (preferred) OR\n","    # - `ELASTIC_USERNAME` and `ELASTIC_PASSWORD` for Elasticsearch Basic Auth.\n","    api_keys = {\n","        'moorcheh': colab_userdata.get('MOORCHEH_API_KEY'),\n","        'cohere': colab_userdata.get('COHERE_API_KEY'),\n","        'pinecone': colab_userdata.get('PINECONE_API_KEY') or colab_userdata.get('PINECONE_API_KEY2'), # Fallback for Pinecone\n","        'elasticsearch': {\n","            'url': colab_userdata.get('ELASTIC_URL'),\n","            'api_key': colab_userdata.get('ELASTIC_API_KEY'),\n","            'username': colab_userdata.get('ELASTIC_USERNAME'),\n","            'password': colab_userdata.get('ELASTIC_PASSWORD')\n","        }\n","    }\n","    # If any Elasticsearch credential is null, set the entire elasticsearch dict to None to indicate missing config.\n","    # This simplifies checks later for whether Elasticsearch is configured.\n","    if not any(api_keys['elasticsearch'].values()):\n","        api_keys['elasticsearch'] = None\n","\n","except ImportError:\n","    # This block executes if not in Google Colab (e.g., a local Python environment).\n","    DRIVE_PATH = \".\" # Results will be saved in the current working directory.\n","    # In a local environment, API keys are typically read from environment variables.\n","    # **User Action Required**: Set these environment variables before running the script.\n","    # - `MOORCHEH_API_KEY`\n","    # - `COHERE_API_KEY`\n","    # - `PINECONE_API_KEY`\n","    # - `ELASTIC_URL` (e.g., \"http://localhost:9200\" for local ES)\n","    # - `ELASTIC_API_KEY` OR `ELASTIC_USERNAME`, `ELASTIC_PASSWORD`\n","    api_keys = {\n","        'moorcheh': os.environ.get('MOORCHEH_API_KEY'),\n","        'cohere': os.environ.get('COHERE_API_KEY'),\n","        'pinecone': os.environ.get('PINECONE_API_KEY'),\n","        'elasticsearch': {\n","            'url': os.environ.get('ELASTIC_URL') or \"http://localhost:9200\", # Defaults to localhost for local setup\n","            'api_key': os.environ.get('ELASTIC_API_KEY'),\n","            'username': os.environ.get('ELASTIC_USERNAME') or \"elastic\",\n","            'password': os.environ.get('ELASTIC_PASSWORD')\n","        }\n","    }\n","    # Similar to Colab, if ES config is incomplete, mark it as None.\n","    if not any(api_keys['elasticsearch'].values()):\n","        api_keys['elasticsearch'] = None\n","    print(\"âš ï¸ Not running in Google Colab. Saving results locally. Ensure environment variables are set.\")\n","\n","# -------------------- 3. General Benchmark Configuration --------------------\n","# These parameters control various aspects of the benchmark. Users can adjust these values\n","# to customize the benchmark's behavior, performance, and resource usage.\n","\n","# BEIR_DATASETS_SORTED_DISPLAY: A list of BEIR dataset names formatted for user-friendly display.\n","# This list includes estimated corpus sizes to help users select appropriate datasets.\n","BEIR_DATASETS_SORTED_DISPLAY = [\n","    \"1. nfcorpus (3,633)\",\n","    \"2. scifact (5,183)\",\n","    \"3. arguana (8,674)\",\n","    \"4. scidocs (25,657)\",\n","    \"5. fiqa (57,638)\",\n","    \"6. trec-covid (171,332)\",\n","    \"7. webis-touche2020 (382,545)\",\n","    \"8. quora (522,931)\",\n","]\n","\n","# BEIR_DATASETS: The actual programmatic names of BEIR datasets used by the `beir` library.\n","# These correspond to the display names above.\n","BEIR_DATASETS = [\n","    \"nfcorpus\", \"scifact\", \"arguana\", \"scidocs\",\n","    \"fiqa\", \"trec-covid\", \"webis-touche2020\", \"quora\",\n","]\n","\n","# TOP_K_SEARCH: **User Customizable Value**\n","# The number of top-ranked results to retrieve from the vector database for each query.\n","# A higher value might improve recall but generally increases search latency and resource usage.\n","TOP_K_SEARCH = 100\n","\n","# K_VALUES: **User Customizable Value**\n","# A list of 'k' values at which retrieval metrics (NDCG, MAP, Recall, Precision) will be calculated.\n","# These values define the cut-off points for evaluation (e.g., NDCG@1, MAP@10, Recall@100).\n","# You can add or remove values based on your evaluation needs.\n","K_VALUES = [1, 3, 5, 10, 100]\n","\n","# DATA_ROOT: The local directory where BEIR datasets will be downloaded. This path is relative.\n","DATA_ROOT = \"./datasets\"\n","\n","# MAX_UPLOAD_DOCS: **User Customizable Value**\n","# Limits the number of documents uploaded to vector databases. This is crucial for managing costs\n","# and execution time, especially with very large datasets. Set to a lower number (e.g., 10000)\n","# for quick tests or a very high number (e.g., 700000) for comprehensive runs. Set to `None`\n","# or a number greater than your dataset size to upload all documents.\n","MAX_UPLOAD_DOCS = 700000 # Set to 700000 to process up to 700K documents; adjust as needed.\n","\n","# BATCH_SIZE: **User Customizable Value**\n","# The number of embeddings to process or upload in a single API request/batch.\n","# Adjusting this value can significantly impact performance, memory usage, and API rate limits.\n","# Larger batches are generally faster due to reduced overhead but consume more RAM.\n","BATCH_SIZE = 100\n","\n","# EMBEDDING_MODEL: **User Customizable Value**\n","# The Cohere model used to generate the initial dense float embeddings.\n","# 'embed-v4.0' is recommended for its performance and higher dimensionality. Other options include\n","# 'embed-english-v3.0', 'embed-multilingual-v3.0', etc. Changing this will require adjusting VECTOR_DIMENSION.\n","EMBEDDING_MODEL = \"embed-v4.0\"\n","\n","# INPUT_TYPE_CORPUS: Specifies the input type for corpus documents to Cohere's embedding model.\n","# This helps Cohere optimize embedding generation for different content types (e.g., 'search_document').\n","INPUT_TYPE_CORPUS = \"search_document\"\n","\n","# INPUT_TYPE_QUERY: Specifies the input type for queries to Cohere's embedding model.\n","# Similar to corpus input type, this optimizes query embedding generation (e.g., 'search_query').\n","INPUT_TYPE_QUERY = \"search_query\"\n","\n","# VECTOR_DIMENSION: **User Customizable Value (Must match EMBEDDING_MODEL)**\n","# The dimensionality of the generated embeddings. This value is critical and MUST match the output\n","# dimension of the `EMBEDDING_MODEL` you choose. Cohere's 'embed-v4.0' has 1536 dimensions;\n","# 'embed-v3.0' has 1024 dimensions. Incorrect dimension will lead to errors in vector databases.\n","VECTOR_DIMENSION = 1536\n","\n","# DATASET_SOURCE: This variable determines whether to use BEIR or MAIR datasets.\n","# It will be set interactively by the user later in the script.\n","DATASET_SOURCE = \"beir\"\n","\n","# CSV_PATH: **User Customizable Path**\n","# The full path where the final benchmark results will be saved in CSV format.\n","# This file will be created or appended to in your Google Drive (or local directory).\n","CSV_PATH = os.path.join(DRIVE_PATH, \"BEIR.MAIR.Binary.Embeddings.All.Providers.csv\")\n","\n","# -------------------- 4. MAIR Dataset Helper Functions --------------------\n","# These functions are specifically designed to assist in loading and processing\n","# MAIR datasets, handling their diverse structure and relevance judgments (qrels).\n","\n","def load_jsonl(filepath):\n","    \"\"\"Loads data from a JSONL (JSON Lines) file into a dictionary.\n","    It intelligently identifies various common ID field names to create a mapping\n","    from item ID to its content. This is flexible for MAIR's varied formats.\n","    \"\"\"\n","    data = {}\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip(): # Ensures that empty lines are skipped.\n","                    item = json.loads(line)\n","                    # Attempts to find a unique identifier for the item (document, query, etc.).\n","                    # Prioritizes common ID fields: '_id', 'id', 'query_id', 'doc_id'.\n","                    item_id = item.get('_id') or item.get('id') or item.get('query_id') or item.get('doc_id')\n","                    if item_id:\n","                        data[str(item_id)] = item # Stores with stringified ID for consistent keying.\n","                    else:\n","                        # Fallback: if no standard ID is found, use an incremental integer.\n","                        # This is less ideal but prevents data loss for malformed entries.\n","                        data[str(len(data))] = item\n","        print(f\"   Loaded {len(data)} items from {os.path.basename(filepath)}\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Error loading {filepath}: {e}\") # Logs specific file loading errors.\n","    return data\n","\n","def extract_qrels_from_queries(queries):\n","    \"\"\"Extracts Qrels (query-relevance judgments) from query data structures.\n","    This function is tailored for MAIR datasets where qrels might be embedded\n","    within query files, often in fields like 'labels', 'relevance', or 'qrels'.\n","    It supports different formats for these fields (list of dicts, list of IDs, or dict).\n","    \"\"\"\n","    qrels = {}\n","    for qid, q in queries.items():\n","        if isinstance(q, dict):\n","            # Checks for common field names that indicate relevance judgments.\n","            labels = q.get('labels') or q.get('relevance') or q.get('qrels')\n","            if labels:\n","                qrels[str(qid)] = {} # Initializes the qrels dictionary for the current query ID.\n","                if isinstance(labels, list):\n","                    # Handles cases where labels are a list (e.g., [{'id': 'doc1', 'score': 1}]).\n","                    for label_item in labels:\n","                        if isinstance(label_item, dict):\n","                            doc_id = label_item.get('id') or label_item.get('doc_id')\n","                            score = label_item.get('score', 1) # Defaults score to 1 if not specified.\n","                            if doc_id:\n","                                qrels[str(qid)][str(doc_id)] = score # Stores doc_id and its relevance score.\n","                        else:\n","                            # Handles simple lists of doc_ids (assumes a default relevance score of 1).\n","                            qrels[str(qid)][str(label_item)] = 1\n","                elif isinstance(labels, dict):\n","                    # Handles dictionaries where keys are doc_ids and values are scores.\n","                    for doc_id, score in labels.items():\n","                        qrels[str(qid)][str(doc_id)] = score\n","    return qrels\n","\n","def get_mair_datasets():\n","    \"\"\"Discovers and categorizes MAIR datasets available in the specified combined path.\n","    It also estimates the number of documents in each dataset by counting lines in JSONL files,\n","    providing a comprehensive overview for user selection.\n","    \"\"\"\n","    # Predefined categories for MAIR datasets, used to organize their display to the user.\n","    # This mapping helps in presenting a structured and navigable list of available datasets.\n","    DATASET_CATEGORIES = {\n","        \"Legal & Regulatory\": [\"ACORDAR\", \"AILA2019-Case\", \"AILA2019-Statutes\", \"CUAD\", \"LeCaRDv2\", \"LegalQuAD\"],\n","        \"Medical & Clinical\": [\"CliniDS-2014\", \"CliniDS-2015\", \"CliniDS-2016\", \"ClinicalTrials-2021\", \"NFCorpus\"],\n","        \"Code & Programming\": [\"APPS\", \"CodeEditSearch\", \"CodeSearchNet\", \"Conala\", \"LeetCode\", \"MBPP\"],\n","        \"Financial\": [\"ConvFinQA\", \"FiQA\", \"FinQA\", \"FinanceBench\"],\n","        \"Academic & Scientific\": [\"ArguAna\", \"LitSearch\", \"ProofWiki-Proof\", \"Competition-Math\"],\n","        \"Conversational & Dialog\": [\"CAsT-2019\", \"CAsT-2020\", \"CAsT-2021\", \"ProCIS-Dialog\", \"SParC\", \"Quora\"],\n","        \"News & Social Media\": [\"ChroniclingAmericaQA\", \"Microblog-2011\", \"Microblog-2012\", \"News21\"],\n","        \"API Documentation\": [\"Apple\", \"FoodAPI\", \"HuggingfaceAPI\", \"PytorchAPI\"],\n","        \"Others\": [\"BSARD\", \"BillSum\", \"CARE\", \"CPCD\", \"CQADupStack\", \"DD\", \"ELI5\"]\n","    }\n","\n","    datasets_by_category = {category: [] for category in DATASET_CATEGORIES.keys()}\n","    all_datasets = []\n","    dataset_sizes = {}\n","\n","    # Verifies that the MAIR_COMBINED_PATH exists before attempting to scan.\n","    # This path is configured in the \"User / Environment Setup\" section.\n","    if os.path.exists(MAIR_COMBINED_PATH):\n","        print(f\"ðŸ” Scanning MAIR datasets in: {MAIR_COMBINED_PATH}\")\n","        try:\n","            # Iterates through each item (directory) within the combined MAIR directory.\n","            # Items are sorted alphabetically for consistent display.\n","            for item in sorted(os.listdir(MAIR_COMBINED_PATH)): # Sort for consistent order\n","                if item.startswith('.'): # Skips hidden files/directories (e.g., .ipynb_checkpoints).\n","                    continue\n","                dataset_path = os.path.join(MAIR_COMBINED_PATH, item)\n","                if not os.path.isdir(dataset_path): # Confirms the item is a directory (representing a dataset).\n","                    continue\n","                docs_path = os.path.join(dataset_path, 'docs')\n","                queries_path = os.path.join(dataset_path, 'queries')\n","                # Checks for the existence of both 'docs' and 'queries' subdirectories,\n","                # and at least one JSONL file in each, to ensure a complete dataset for benchmarking.\n","                if os.path.exists(docs_path) and os.path.exists(queries_path):\n","                    if any(f.endswith('.jsonl') for f in os.listdir(docs_path)) and any(f.endswith('.jsonl') for f in os.listdir(queries_path)):\n","                        all_datasets.append(item)\n","                        # Assigns the dataset to its predefined category for organized display.\n","                        category = \"Others\"\n","                        for cat, datasets in DATASET_CATEGORIES.items():\n","                            if item in datasets:\n","                                category = cat\n","                                break\n","                        datasets_by_category[category].append(item)\n","                        # Counts documents by counting non-empty lines in all JSONL files within the 'docs' folder.\n","                        doc_count = 0\n","                        for file in os.listdir(docs_path):\n","                            if file.endswith('.jsonl'):\n","                                with open(os.path.join(docs_path, file), 'r') as f:\n","                                    doc_count += sum(1 for line in f if line.strip()) # Counts non-empty lines.\n","                        dataset_sizes[item] = doc_count\n","                        print(f\"   âœ… Found MAIR dataset: {item} - {doc_count} docs\")\n","        except Exception as e:\n","            print(f\"   âŒ Error during MAIR dataset scanning: {e}\")\n","    else:\n","        print(f\"âš ï¸ MAIR combined path not found: {MAIR_COMBINED_PATH}. Please ensure the previous step to combine datasets was executed successfully.\")\n","\n","    return all_datasets, datasets_by_category, dataset_sizes\n","\n","# -------------------- 5. Binarization and Utility Functions --------------------\n","# These functions are central to the binary embedding benchmark, handling the\n","# conversion of dense float embeddings to their binary representation and providing\n","# common utilities for performance measurement and result management.\n","\n","def binarize_embeddings(embeddings):\n","    \"\"\"Binarizes float embeddings using a simple sign-based method:\n","    - Any embedding component (dimension) >= 0 becomes 1.\n","    - Any embedding component (dimension) < 0 becomes 0.\n","    The output is a NumPy array of float32 (either 0.0 or 1.0). This format is chosen\n","    to maintain compatibility with vector databases that primarily expect float vectors,\n","    while still representing the binary nature of the data. This allows for testing\n","    binary-like representations in systems not explicitly designed for bit vectors.\n","    \"\"\"\n","    embeddings = np.array(embeddings, dtype=np.float32) # Ensures input is a NumPy float32 array.\n","    # Applies the binarization logic. The result is a boolean array, which is then\n","    # cast to float32 (True becomes 1.0, False becomes 0.0).\n","    binary = (embeddings >= 0).astype(np.float32)\n","    return binary\n","\n","def save_binary_embeddings(doc_ids, corpus_embeddings_binary, query_ids, query_embeddings_binary, dataset_name, output_dir):\n","    \"\"\"Saves the binarized embeddings to disk in multiple common formats.\n","    This allows users to inspect, share, or use the binarized data with other tools or for offline analysis.\n","    Supported formats include NumPy (.npy), CSV (.csv), JSON (.json), HDF5 (.h5), and Parquet (.parquet).\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True) # Ensures the target output directory exists.\n","\n","    print(f\"\\nðŸ’¾ Saving Binarized Embeddings for {dataset_name} to {output_dir}...\")\n","\n","    # ========== NumPy Format (.npy) ==========\n","    # NumPy's native format is highly efficient for storing and loading numerical arrays,\n","    # often resulting in the smallest file sizes and fastest I/O for array-based data.\n","    print(f\"\\n  1ï¸âƒ£  Numpy Format (.npy)\")\n","    corpus_npy_path = os.path.join(output_dir, f\"{dataset_name}_corpus_binary.npy\")\n","    query_npy_path = os.path.join(output_dir, f\"{dataset_name}_query_binary.npy\")\n","\n","    np.save(corpus_npy_path, corpus_embeddings_binary)\n","    np.save(query_npy_path, query_embeddings_binary)\n","    print(f\"     âœ… Corpus: {corpus_npy_path} ({corpus_embeddings_binary.nbytes / (1024*1024):.2f} MB)\")\n","    print(f\"     âœ… Query:  {query_npy_path} ({query_embeddings_binary.nbytes / (1024*1024):.2f} MB)\")\n","\n","    # ========== CSV Format (.csv) ==========\n","    # CSV is a widely compatible, human-readable format, easily opened in spreadsheet software.\n","    # Each vector's components are joined into a comma-separated string within a single cell.\n","    print(f\"\\n  2ï¸âƒ£  CSV Format (.csv)\")\n","    corpus_csv_path = os.path.join(output_dir, f\"{dataset_name}_corpus_binary.csv\")\n","    query_csv_path = os.path.join(output_dir, f\"{dataset_name}_query_binary.csv\")\n","\n","    # Creates a Pandas DataFrame for corpus embeddings, converting each vector to a string.\n","    corpus_df = pd.DataFrame({\n","        'doc_id': doc_ids,\n","        'embedding_binary': [','.join(map(str, row)) for row in corpus_embeddings_binary]\n","    })\n","    corpus_df.to_csv(corpus_csv_path, index=False)\n","    print(f\"     âœ… Corpus: {corpus_csv_path}\")\n","\n","    # Creates a Pandas DataFrame for query embeddings.\n","    query_df = pd.DataFrame({\n","        'query_id': query_ids,\n","        'embedding_binary': [','.join(map(str, row)) for row in query_embeddings_binary]\n","    })\n","    query_df.to_csv(query_csv_path, index=False)\n","    print(f\"     âœ… Query:  {query_csv_path}\")\n","\n","    # ========== JSON Format (.json) ==========\n","    # JSON is a lightweight, human-readable data interchange format, widely used in web applications.\n","    print(f\"\\n  3ï¸âƒ£  JSON Format (.json)\")\n","    corpus_json_path = os.path.join(output_dir, f\"{dataset_name}_corpus_binary.json\")\n","    query_json_path = os.path.join(output_dir, f\"{dataset_name}_query_binary.json\")\n","\n","    # Prepares corpus data as a dictionary: {doc_id: vector_list}.\n","    corpus_json = {\n","        'dataset': dataset_name,\n","        'type': 'corpus',\n","        'count': len(doc_ids),\n","        'dimension': VECTOR_DIMENSION,\n","        'embeddings': {doc_id: vec.tolist() for doc_id, vec in zip(doc_ids, corpus_embeddings_binary)}\n","    }\n","\n","    # Prepares query data as a dictionary: {query_id: vector_list}.\n","    query_json = {\n","        'dataset': dataset_name,\n","        'type': 'query',\n","        'count': len(query_ids),\n","        'dimension': VECTOR_DIMENSION,\n","        'embeddings': {qid: vec.tolist() for qid, vec in zip(query_ids, query_embeddings_binary)}\n","    }\n","\n","    with open(corpus_json_path, 'w') as f:\n","        json.dump(corpus_json, f) # Saves the corpus JSON data.\n","    print(f\"     âœ… Corpus: {corpus_json_path}\")\n","\n","    with open(query_json_path, 'w') as f:\n","        json.dump(query_json, f) # Saves the query JSON data.\n","    print(f\"     âœ… Query:  {query_json_path}\")\n","\n","    # ========== HDF5 Format (.h5) (if available) ==========\n","    # HDF5 is designed for storing and managing very large numerical datasets efficiently,\n","    # supporting compression and hierarchical data structures. Requires `h5py` library.\n","    try:\n","        import h5py\n","        print(f\"\\n  4ï¸âƒ£  HDF5 Format (.h5)\")\n","        h5_path = os.path.join(output_dir, f\"{dataset_name}_binary.h5\")\n","\n","        with h5py.File(h5_path, 'w') as f:\n","            f.create_dataset('corpus_embeddings', data=corpus_embeddings_binary, compression='gzip') # Stores with gzip compression.\n","            f.create_dataset('query_embeddings', data=query_embeddings_binary, compression='gzip')\n","            # Stores IDs as fixed-length strings for compatibility with HDF5 datasets.\n","            f.create_dataset('doc_ids', data=np.array(doc_ids, dtype=h5py.string_dtype()), dtype=h5py.string_dtype())\n","            f.create_dataset('query_ids', data=np.array(query_ids, dtype=h5py.string_dtype()), dtype=h5py.string_dtype())\n","            f.attrs['dimension'] = VECTOR_DIMENSION # Adds metadata attributes.\n","            f.attrs['dataset'] = dataset_name\n","\n","        print(f\"     âœ… HDF5: {h5_path} ({os.path.getsize(h5_path) / (1024*1024):.2f} MB)\")\n","    except ImportError:\n","        print(f\"     âš ï¸  HDF5 not available (install 'h5py' package to save in this format: `pip install h5py`)\")\n","\n","    # ========== Parquet Format (.parquet) (if available) ==========\n","    # Parquet is a columnar storage format, highly optimized for analytical queries and big data ecosystems.\n","    # It provides efficient compression and encoding schemes. Requires `pyarrow` and optionally `fastparquet`.\n","    try:\n","        print(f\"\\n  5ï¸âƒ£  Parquet Format (.parquet)\")\n","        corpus_parquet_path = os.path.join(output_dir, f\"{dataset_name}_corpus_binary.parquet\")\n","        query_parquet_path = os.path.join(output_dir, f\"{dataset_name}_query_binary.parquet\")\n","\n","        # Creates Pandas DataFrames for Parquet, converting vectors to lists.\n","        corpus_parquet_df = pd.DataFrame({\n","            'doc_id': doc_ids,\n","            'embedding': [list(vec) for vec in corpus_embeddings_binary]\n","        })\n","        corpus_parquet_df.to_parquet(corpus_parquet_path, compression='snappy') # Uses Snappy compression.\n","\n","        query_parquet_df = pd.DataFrame({\n","            'query_id': query_ids,\n","            'embedding': [list(vec) for vec in query_embeddings_binary]\n","        })\n","        query_parquet_df.to_parquet(query_parquet_path, compression='snappy')\n","\n","        print(f\"     âœ… Corpus: {corpus_parquet_path}\")\n","        print(f\"     âœ… Query:  {query_parquet_path}\")\n","    except Exception as e:\n","        print(f\"     âš ï¸  Parquet not available or failed to save (install 'pyarrow' and 'fastparquet', or check error: {e})\")\n","\n","    # ========== SUMMARY OF SAVED EMBEDDINGS ==========\n","    # Provides a brief summary of the saved binary embeddings and their respective formats.\n","    print(f\"\\nðŸ“Š Binary Embeddings Download Summary:\")\n","    print(f\"   Dataset: {dataset_name}\")\n","    print(f\"   Corpus documents: {len(doc_ids)}\")\n","    print(f\"   Query embeddings: {len(query_ids)}\")\n","    print(f\"   Dimension: {VECTOR_DIMENSION}\")\n","    print(f\"   Saved to directory: {output_dir}\")\n","    print(f\"\\n   Format Summary (choose the format best suited for your needs):\")\n","    print(f\"   â€¢ .npy   - Fast loading with numpy (often smallest file size)\")\n","    print(f\"   â€¢ .csv   - Human-readable, spreadsheet compatible (vectors as strings)\")\n","    print(f\"   â€¢ .json  - Human-readable, portable (vectors as lists)\")\n","    print(f\"   â€¢ .h5    - HDF5 with compression (for large numerical datasets, if h5py installed)\")\n","    print(f\"   â€¢ .parquet - Columnar storage (efficient for analytical queries, if pyarrow installed)\")\n","\n","    return {\n","        'corpus_npy': corpus_npy_path,\n","        'query_npy': query_npy_path,\n","        'corpus_csv': corpus_csv_path,\n","        'query_csv': query_csv_path,\n","        'corpus_json': corpus_json_path,\n","        'query_json': query_json_path,\n","        'output_dir': output_dir\n","    }\n","\n","def calculate_timing_stats(timing_list):\n","    \"\"\"Calculates basic descriptive statistics (mean, median, min, max, std dev, total sum)\n","    for a given list of numerical timings. This is used to summarize performance metrics\n","    like search times, upload durations, etc., providing a quick overview of performance.\n","    \"\"\"\n","    if not timing_list:\n","        return {\n","            \"mean\": 0.0, \"median\": 0.0, \"min\": 0.0,\n","            \"max\": 0.0, \"std\": 0.0, \"total\": 0.0\n","        }\n","    return {\n","        \"mean\": statistics.mean(timing_list),\n","        \"median\": statistics.median(timing_list),\n","        \"min\": min(timing_list),\n","        \"max\": max(timing_list),\n","        \"std\": statistics.stdev(timing_list) if len(timing_list) > 1 else 0.0, # std dev requires at least two data points.\n","        \"total\": sum(timing_list)\n","    }\n","\n","def format_and_print_metrics(ndcg, _map, recall, precision, ks=K_VALUES):\n","    \"\"\"Formats and prints retrieval metrics (NDCG, MAP, Recall, Precision) in a clean,\n","    tabular format for specified K values. This provides an immediate, human-readable\n","    summary of the retrieval quality for a benchmark run.\n","    \"\"\"\n","    print(\"\\nRetrieval Metrics:\")\n","    print(\"-------------------\")\n","    for k in ks:\n","        print(f\"NDCG@{k}: {ndcg.get(f'NDCG@{k}', 0.0):.4f} | \"\n","              f\"MAP@{k}: {_map.get(f'MAP@{k}', 0.0):.4f} | \"\n","              f\"Recall@{k}: {recall.get(f'Recall@{k}', 0.0):.4f} | \"\n","              f\"P@{k}: {precision.get(f'P@{k}', 0.0):.4f}\") # Includes Precision for completeness.\n","\n","def extract_all_metrics(ndcg, _map, recall, precision, ks=K_VALUES):\n","    \"\"\"Extracts all relevant retrieval metrics into a single dictionary.\n","    This structured format is ideal for storage, particularly for CSV output,\n","    ensuring all evaluation results are consistently captured.\n","    \"\"\"\n","    metrics = {}\n","    for k in ks:\n","        metrics[f\"NDCG@{k}\"] = float(ndcg.get(f\"NDCG@{k}\", 0.0)) # Ensures float conversion for consistency.\n","        metrics[f\"MAP@{k}\"] = float(_map.get(f\"MAP@{k}\", 0.0))\n","        metrics[f\"Recall@{k}\"] = float(recall.get(f\"Recall@{k}\", 0.0))\n","        metrics[f\"P@{k}\"] = float(precision.get(f\"P@{k}\", 0.0))\n","    return metrics\n","\n","def save_results_to_csv(new_result: dict, csv_path: str):\n","    \"\"\"Appends a new benchmark result entry to a CSV file. If the file doesn't exist,\n","    it creates it along with the header. Otherwise, it appends the new data,\n","    ensuring data integrity and continuity of results over multiple runs.\n","    \"\"\"\n","    new_df = pd.DataFrame([new_result]) # Converts the new result dictionary into a single-row DataFrame.\n","    os.makedirs(os.path.dirname(csv_path), exist_ok=True) # Ensures the directory for the CSV file exists.\n","    write_header = not os.path.exists(csv_path) # Checks if the file already exists to determine if a header is needed.\n","\n","    try:\n","        if write_header:\n","            new_df.to_csv(csv_path, mode='w', header=True, index=False) # Writes to a new file with header.\n","        else:\n","            # Reads existing CSV, concatenates with new data, then overwrites to maintain header and data consistency.\n","            existing_df = pd.read_csv(csv_path)\n","            combined_df = pd.concat([existing_df, new_df], ignore_index=True) # Appends new results.\n","            combined_df.to_csv(csv_path, mode='w', header=True, index=False) # Overwrites with combined data.\n","        print(f\"ðŸ’¾ Results saved to: {csv_path}\")\n","    except Exception as e:\n","        print(f\"âŒ CSV save failed: {e}. Please check file permissions or path validity.\")\n","\n","def clean_memory():\n","    \"\"\"Forces Python's garbage collector to release memory.\n","    This is particularly important in resource-constrained environments like Colab,\n","    especially when processing large datasets, to prevent out-of-memory errors.\n","    \"\"\"\n","    gc.collect()\n","\n","def should_cleanup_namespace(provider_name, dataset_name):\n","    \"\"\"Interactively prompts the user whether to delete the created vector database\n","    index/namespace after benchmarking. This gives the user control over resource\n","    management, helping to prevent unintended cloud costs.\n","    \"\"\"\n","    response = input(f\"\\nâ“ Delete {provider_name} namespace/index for {dataset_name} after benchmarking? (y/n): \").strip().lower()\n","    return response in ['y', 'yes'] # Returns True if user confirms 'yes', False otherwise.\n","\n","# -------------------- 6. Interactive Dataset Source Selection --------------------\n","# This section allows the user to choose between using standard BEIR datasets\n","# or the more diverse MAIR datasets for benchmarking. The choice affects which\n","# datasets are displayed and how they are loaded.\n","print(\"\\nðŸ“š Select Dataset Source for Benchmarking:\")\n","print(\"  1) BEIR - Benchmark for Information Retrieval (standard datasets like nfcorpus, scifact)\")\n","print(\"  2) MAIR - Multi-domain Adversarial Information Retrieval (diverse datasets from various domains)\")\n","dataset_source_choice = input(\"âž¡ï¸ Enter your choice [1/2] (default is 1): \").strip() or \"1\"\n","DATASET_SOURCE = \"mair\" if dataset_source_choice == \"2\" else \"beir\"\n","print(f\"âœ… Selected dataset source: {DATASET_SOURCE.upper()}\")\n","\n","# -------------------- 7. Interactive Provider Selection and API Key Status --------------------\n","# This section informs the user about available vector database providers, checks the status\n","# of their API keys (indicating if they are configured correctly), and allows the user to\n","# select which providers to include in the current benchmark run.\n","print(\"\\nðŸ”§ Available Providers for BINARY Embeddings Benchmarking:\")\n","print(f\"  1. Moorcheh (Vector namespace - {VECTOR_DIMENSION}D binary vector search)\")\n","print(f\"  2. Pinecone (Vector index - {VECTOR_DIMENSION}D binary vector search)\")\n","print(f\"  3. Elasticsearch (Dense vector - {VECTOR_DIMENSION}D binary vector search)\")\n","\n","print(\"\\nðŸ”‘ API Keys Status (ensure these are set in Colab secrets or environment variables):\")\n","for provider in ['moorcheh', 'cohere', 'pinecone']:\n","    key = api_keys.get(provider)\n","    status = \"âœ… Found\" if key else \"âŒ Missing\" # Indicates if the API key was successfully loaded.\n","    display_name = provider.capitalize()\n","    if provider == 'cohere':\n","        display_name += \" (for embeddings)\" # Adds context for Cohere API key.\n","    print(f\"  {display_name}: {status}\")\n","\n","# Checks Elasticsearch connectivity/credentials status separately.\n","# It's considered configured if a URL is provided AND either an API key or username/password are present.\n","es_config = api_keys.get('elasticsearch', {})\n","es_configured = bool(es_config and es_config.get('url') and (es_config.get('api_key') or (es_config.get('username') and es_config.get('password'))))\n","print(f\"  Elasticsearch: {'âœ… Configured' if es_configured else 'âŒ Not fully configured (URL or credentials missing)'}\")\n","\n","print(f\"\\nâš™ï¸  Current Benchmark Configuration (BINARY Embeddings):\")\n","print(f\"  â€¢ Embedding Model: Cohere {EMBEDDING_MODEL} (used to generate initial float embeddings)\")\n","print(f\"  â€¢ Vector Dimension: {VECTOR_DIMENSION}D (float) will be converted to {VECTOR_DIMENSION} bits (binary)\")\n","print(f\"  â€¢ Binarization Method: Sign-based (values >= 0 become 1, values < 0 become 0)\")\n","print(f\"  â€¢ Batch Size for API calls/uploads: {BATCH_SIZE}\")\n","print(f\"  â€¢ This benchmark focuses on BINARY embeddings only (no float comparison in this run)\")\n","print(f\"  â€¢ Expected Space Savings after binarization: ~32x compression (from float32 to binary representation)\")\n","\n","# User selects which providers to test interactively.\n","# User can enter '1,2' for Moorcheh and Pinecone, or 'all' for all available providers.\n","provider_choice = input(\"\\nâž¡ï¸ Select providers to test (e.g., '1,2,3' or 'all') (default is 'all'): \").strip().lower() or \"all\"\n","if provider_choice == 'all':\n","    selected_providers = []\n","    if api_keys.get('moorcheh'): selected_providers.append('moorcheh')\n","    if api_keys.get('pinecone'): selected_providers.append('pinecone')\n","    if es_configured: selected_providers.append('elasticsearch')\n","else:\n","    provider_map = {'1': 'moorcheh', '2': 'pinecone', '3': 'elasticsearch'}\n","    selected_providers = [provider_map[p.strip()] for p in provider_choice.split(',') if p.strip() in provider_map]\n","\n","# Filters out providers for which API keys are missing or not configured, informing the user.\n","selected_providers_filtered = []\n","for p in selected_providers:\n","    if p == 'moorcheh' and not api_keys.get('moorcheh'):\n","        print(f\"âš ï¸ Moorcheh not included: API key missing.\")\n","    elif p == 'pinecone' and not api_keys.get('pinecone'):\n","        print(f\"âš ï¸ Pinecone not included: API key missing.\")\n","    elif p == 'elasticsearch' and not es_configured:\n","        print(f\"âš ï¸ Elasticsearch not included: Configuration incomplete.\")\n","    else:\n","        selected_providers_filtered.append(p)\n","\n","selected_providers = selected_providers_filtered\n","\n","if not selected_providers:\n","    print(\"âŒ No providers selected or configured. Please check your API keys and try again.\")\n","    exit(1) # Exits if no providers can be benchmarked, as there's nothing to test.\n","\n","print(f\"âœ… Selected providers for benchmarking: {', '.join(selected_providers).capitalize()}\")\n","\n","# -------------------- 8. Initialize API Clients --------------------\n","# This section initializes the API clients for the selected vector database providers\n","# and Cohere (for embeddings). Clients are only initialized if their respective\n","# API keys are available, ensuring secure and functional connections.\n","clients = {}\n","es_client = None\n","\n","# Initializes Cohere client, which is essential for generating embeddings for ALL providers.\n","# The script will exit if the Cohere API key is not found.\n","if api_keys['cohere']:\n","    import cohere\n","    cohere_client = cohere.Client(api_keys['cohere']) # Uses the retrieved Cohere API key.\n","    print(f\"\\nðŸ§  Cohere client initialized successfully for embedding generation (model: {EMBEDDING_MODEL})\")\n","else:\n","    print(\"\\nâŒ Cohere API key required! Please set 'COHERE_API_KEY' in Colab secrets or environment variables.\")\n","    exit(1) # Stops execution if Cohere API key is missing.\n","\n","# Initializes Moorcheh client if it was selected and its API key is present.\n","if 'moorcheh' in selected_providers and api_keys['moorcheh']:\n","    from moorcheh_sdk import MoorchehClient, ConflictError # Imports Moorcheh specific exceptions.\n","    clients['moorcheh'] = MoorchehClient(api_key=api_keys['moorcheh']) # Initializes with API key.\n","    print(f\"âœ… Moorcheh client initialized.\")\n","\n","# Initializes Pinecone client if it was selected and its API key is present.\n","if 'pinecone' in selected_providers and api_keys['pinecone']:\n","    try:\n","        # Attempts to import Pinecone v2 style if available, with fallback to older v1 syntax.\n","        from pinecone import Pinecone, ServerlessSpec\n","    except ImportError:\n","        import pinecone as pc\n","        Pinecone = pc.Pinecone\n","        ServerlessSpec = pc.ServerlessSpec\n","\n","    clients['pinecone'] = Pinecone(api_key=api_keys['pinecone']) # Initializes with API key.\n","    print(f\"âœ… Pinecone client initialized.\")\n","\n","# Initializes Elasticsearch client if it was selected and properly configured.\n","if 'elasticsearch' in selected_providers and es_configured:\n","    try:\n","        from elasticsearch import Elasticsearch\n","        es_config = api_keys['elasticsearch'] # Retrieves Elasticsearch specific configuration.\n","\n","        # Connects to Elasticsearch using either API key or basic authentication, prioritizing API key.\n","        if es_config.get('api_key'):\n","            es_client = Elasticsearch(es_config['url'], api_key=es_config['api_key'], request_timeout=60) # API key authentication.\n","        elif es_config.get('username') and es_config.get('password'):\n","            es_client = Elasticsearch(es_config['url'], basic_auth=(es_config['username'], es_config['password']), request_timeout=60) # Basic authentication.\n","        else:\n","            # Fallback for local Elasticsearch without authentication (less secure, for local testing only).\n","            es_client = Elasticsearch(es_config['url'], request_timeout=60)\n","\n","        if es_client and es_client.ping(): # Tests the connection to Elasticsearch.\n","            print(\"âœ… Elasticsearch connected successfully.\")\n","        else:\n","            es_client = None\n","            print(\"âš ï¸ Elasticsearch connection failed. Please check URL and credentials.\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Elasticsearch client initialization failed: {e}. Please ensure Elasticsearch is running and accessible.\")\n","        es_client = None # Sets client to None if initialization fails.\n","\n","\n","# -------------------- 9. Vector Database Provider Classes --------------------\n","# These classes encapsulate the specific logic for interacting with each vector\n","# database provider (Moorcheh, Pinecone, Elasticsearch). Each class handles\n","# operations such as vector upload, search queries, and resource cleanup,\n","# tailored to the provider's API and specifically for binary embeddings.\n","\n","class MoorchehBinaryProvider:\n","    \"\"\"Manages interaction with the Moorcheh vector database for binary vector benchmarking.\n","    This class handles the creation of namespaces, uploading of binarized vectors,\n","    performing similarity searches, and cleaning up resources within Moorcheh.\n","    \"\"\"\n","    def __init__(self, client, namespace_name, precomputed_vectors, query_embeddings):\n","        self.client = client # Moorcheh API client instance.\n","        self.namespace_name = namespace_name # Unique name for the Moorcheh namespace to be used.\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and binarized vectors.\n","        self.query_embeddings = query_embeddings # List of binarized query embeddings.\n","        self.upload_timings = {\n","            \"server_upload_time_s\": 0.0, # Accumulates total server-side time for vector uploads.\n","            \"batch_details\": [] # Stores detailed timings and info for each batch upload.\n","        }\n","        self.search_timings = [] # Stores timings for individual search queries.\n","\n","    def upload(self):\n","        \"\"\"Uploads binarized vectors to a Moorcheh vector namespace.\n","        It attempts to create a namespace, and if it already exists, proceeds to use it.\n","        Vectors are uploaded in batches to optimize performance.\n","        \"\"\"\n","        from moorcheh_sdk import ConflictError # Imports specific Moorcheh exception for existing namespaces.\n","        try:\n","            # Attempts to create a new namespace. Moorcheh organizes vectors into namespaces.\n","            # If a namespace with the same name already exists, a ConflictError is typically raised.\n","            self.client.create_namespace(\n","                namespace_name=self.namespace_name,\n","                type=\"vector\", # Specifies that this is a vector namespace.\n","                vector_dimension=VECTOR_DIMENSION # Defines the expected dimension of vectors.\n","            )\n","            print(f\"âœ… Created Moorcheh namespace: {self.namespace_name}\")\n","        except ConflictError:\n","            print(f\"âš ï¸ Moorcheh namespace '{self.namespace_name}' already exists, proceeding to use it.\")\n","        except Exception as e:\n","            print(f\"âŒ Error creating Moorcheh namespace: {e}\")\n","            raise # Re-raises critical errors (e.g., authentication issues) to halt execution.\n","\n","        print(f\"\\nðŸ“Š Binary Upload Chunks Details for Moorcheh:\")\n","        print(f\"   Total vectors to upload: {len(self.precomputed_vectors)}\")\n","        print(f\"   Batch size for uploads: {BATCH_SIZE}\")\n","        print(f\"   Total number of batches: {(len(self.precomputed_vectors) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n","        print(f\"\\n   Batch Breakdown:\")\n","        print(f\"   {'Batch':<8} {'Vectors':<12} {'First ID':<15} {'Sample Vector (first 10 dims)':<50} {'Server Time (s)':<8}\")\n","        print(f\"   {'-'*100}\")\n","\n","        batch_num = 0\n","        # Iterates through the precomputed vectors in batches, displaying a progress bar.\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading binary vectors to Moorcheh\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE] # Gets a slice of vectors for the current batch.\n","            batch_num += 1\n","\n","            try:\n","                # Calls the Moorcheh SDK to upload the batch of vectors.\n","                # The 'vectors' argument expects a list of dictionaries like {'id': 'doc1', 'vector': [0.0, 1.0, ...]}.\n","                response = self.client.upload_vectors(\n","                    namespace_name=self.namespace_name,\n","                    vectors=batch\n","                )\n","\n","                # Extracts server-side execution time from the response for performance analysis.\n","                server_time = 0.0\n","                if isinstance(response, dict):\n","                    server_time = response.get(\"execution_time\", 0.0) # Primary timing key.\n","                    if \"timings\" in response: # Checks for more detailed nested timings.\n","                        server_time = response[\"timings\"].get(\"total\", server_time)\n","\n","                # Gathers sample vector information for logging and verification purposes.\n","                first_id = batch[0]['id']\n","                sample_vector = batch[0]['vector'][:10]  # Takes the first 10 dimensions for brevity.\n","                sample_str = f\"[{', '.join([f'{v:.1f}' for v in sample_vector])}...]\".ljust(50) # Formats for display.\n","\n","                self.upload_timings[\"server_upload_time_s\"] += server_time # Accumulates total server upload time.\n","                self.upload_timings[\"batch_details\"].append({\n","                    \"batch_num\": batch_num,\n","                    \"batch_size\": len(batch),\n","                    \"first_id\": first_id,\n","                    \"server_time_s\": server_time,\n","                    \"sample_vector\": sample_vector # Stores sample for debugging/verification.\n","                }) # Stores details for each batch.\n","\n","                print(f\"   {batch_num:<8} {len(batch):<12} {first_id:<15} {sample_str:<50} {server_time:<8.4f}\")\n","\n","            except Exception as e:\n","                print(f\"\\nâŒ Batch {batch_num} failed to upload to Moorcheh: {e}\") # Logs any batch upload errors.\n","\n","        print(f\"\\nâ±ï¸  Moorcheh Upload Summary (Server-Side Timings):\")\n","        print(f\"    Total server-side upload time: {self.upload_timings['server_upload_time_s']:.4f}s\")\n","        if batch_num > 0:\n","            print(f\"    Average server-side time per batch: {self.upload_timings['server_upload_time_s'] / batch_num:.4f}s\")\n","\n","        return len(self.precomputed_vectors) # Returns the total number of vectors intended for upload.\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a vector search with a binarized query vector against Moorcheh.\n","        It retrieves the top_k most similar documents based on the stored binary embeddings.\n","        \"\"\"\n","        query_embedding = self.query_embeddings[query_idx] # Retrieves the specific query embedding.\n","\n","        # Executes the search query using the Moorcheh SDK.\n","        resp = self.client.search(\n","            namespaces=[self.namespace_name], # Specifies the namespace(s) to search within.\n","            query=query_embedding, # The binarized query vector.\n","            top_k=top_k # The number of top results to retrieve.\n","        )\n","\n","        # Extracts server-side search time from the response.\n","        server_time = resp.get(\"execution_time\", 0.0) if isinstance(resp, dict) else 0.0\n","\n","        # Extracts more detailed timing components if provided in the Moorcheh response.\n","        timing_detail = {\"server_time_s\": server_time}\n","        if isinstance(resp, dict) and \"timings\" in resp:\n","            timings = resp[\"timings\"]\n","            for key, value in timings.items():\n","                if isinstance(value, (int, float)): # Only logs numerical timing values.\n","                    timing_detail[f\"moorcheh_{key}_s\"] = value # Stores detailed timings with a prefix.\n","\n","        self.search_timings.append(timing_detail) # Records timings for this search operation.\n","\n","        # Formats search results into a dictionary of {doc_id: score} for evaluation compatibility.\n","        hits = resp.get(\"results\", []) if isinstance(resp, dict) else []\n","        return {str(r[\"id\"]): float(r[\"score\"]) for r in hits} # Returns retrieved document IDs and their scores.\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for Moorcheh, including detailed component breakdowns.\n","        This provides insights into the time spent on different stages of the search process (e.g., indexing, reranking).\n","        \"\"\"\n","        server_times = [t[\"server_time_s\"] for t in self.search_timings] # Extracts overall server times.\n","        stats = {\"overall\": calculate_timing_stats(server_times)} # Calculates stats for overall server time.\n","\n","        # Calculates statistics for each detailed timing component provided by Moorcheh.\n","        # This helps in understanding where the time is spent during a search operation.\n","        if self.search_timings and len(self.search_timings) > 0:\n","            first_timing = self.search_timings[0] # Uses the first search timing entry to identify available component keys.\n","            # Collects all unique detailed timing keys (e.g., 'moorcheh_indexing_s', 'moorcheh_reranking_s').\n","            timing_keys = [k for k in first_timing.keys() if k != \"server_time_s\"]\n","\n","            for key in timing_keys:\n","                values = [t.get(key, 0.0) for t in self.search_timings if key in t] # Gathers values for each component.\n","                if values:\n","                    stats[key] = calculate_timing_stats(values) # Calculates stats for each component.\n","\n","        return stats\n","\n","    def cleanup(self):\n","        \"\"\"Deletes the Moorcheh namespace created for the benchmark.\n","        This helps in managing cloud resources and cleaning up temporary data.\n","        \"\"\"\n","        try:\n","            self.client.delete_namespace(self.namespace_name) # Calls the SDK to delete the namespace.\n","            print(f\"ðŸ§¹ Deleted Moorcheh namespace: {self.namespace_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete Moorcheh namespace '{self.namespace_name}': {e}. You may need to delete it manually via the Moorcheh dashboard if it persists.\")\n","\n","\n","class PineconeBinaryProvider:\n","    \"\"\"Manages interaction with the Pinecone vector database for binary vector benchmarking.\n","    This class handles index creation, upserting of binarized vectors, performing searches,\n","    and cleaning up the Pinecone index.\n","    \"\"\"\n","    def __init__(self, client, index_name, precomputed_vectors, query_embeddings):\n","        self.client = client # Pinecone API client instance.\n","        self.index_name = index_name # Unique name for the Pinecone index to be used.\n","        self.index = None # Will store the Pinecone Index object after creation.\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and binarized vectors.\n","        self.query_embeddings = query_embeddings # List of binarized query embeddings.\n","        self.upload_timings = {\n","            \"index_creation_s\": 0.0, # Time taken to create the Pinecone index.\n","            \"upsert_time_s\": 0.0, # Total time for upserting (uploading) vectors.\n","            \"batch_details\": [] # Detailed timings for each batch upsert operation.\n","        }\n","        self.search_timings = [] # Timings for individual search queries.\n","\n","    def upload(self):\n","        \"\"\"Uploads binarized vectors to a Pinecone index.\n","        It first checks for and deletes any existing index with the same name to ensure a clean run,\n","        then creates a new index and upserts vectors in batches.\n","        \"\"\"\n","        from pinecone import ServerlessSpec # Imports Pinecone's specification for index creation (e.g., cloud and region).\n","\n","        t0 = time.perf_counter() # Starts timer for index creation.\n","        # Checks if an index with the same name already exists and deletes it.\n","        # This prevents issues with stale data or conflicting configurations from previous runs.\n","        if self.index_name in [idx.name for idx in self.client.list_indexes()]:\n","            print(f\"ðŸ—‘ï¸ Deleting existing Pinecone index: {self.index_name}\")\n","            self.client.delete_index(self.index_name)\n","            time.sleep(5) # Pauses to allow Pinecone to complete the index deletion.\n","\n","        # Creates a new Pinecone index configured for binary-like vectors.\n","        # 'metric': 'cosine' is a common similarity metric, often used even for binarized vectors when treated as floats.\n","        # 'spec': ServerlessSpec defines the cloud provider and region for a managed serverless index.\n","        print(f\"âœ¨ Creating new Pinecone index: {self.index_name} (Dimension: {VECTOR_DIMENSION}, Metric: cosine)\")\n","        self.client.create_index(\n","            name=self.index_name,\n","            dimension=VECTOR_DIMENSION,\n","            metric='cosine', # Cosine similarity is a suitable choice for comparing vector directions.\n","            spec=ServerlessSpec(cloud='aws', region='us-east-1') # **User Customizable**: Adjust cloud/region as needed.\n","        )\n","\n","        # Waits for the newly created index to be ready before proceeding with data upsert operations.\n","        # Index creation can take a few moments to provision resources.\n","        print(f\"â³ Waiting for Pinecone index '{self.index_name}' to be ready...\")\n","        while not self.client.describe_index(self.index_name).status['ready']:\n","            time.sleep(1) # Polls every second until the index status is 'ready'.\n","\n","        t1 = time.perf_counter() # Ends timer for index creation.\n","        self.upload_timings[\"index_creation_s\"] = t1 - t0 # Records the duration of index creation.\n","\n","        self.index = self.client.Index(self.index_name) # Gets the Pinecone Index object for data operations.\n","\n","        print(f\"\\nðŸ“Š Binary Upload Chunks Details for Pinecone:\")\n","        print(f\"   Total vectors to upsert: {len(self.precomputed_vectors)}\")\n","        print(f\"   Batch size for upserts: {BATCH_SIZE}\")\n","        print(f\"   Total number of batches: {(len(self.precomputed_vectors) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n","        print(f\"\\n   Batch Breakdown:\")\n","        print(f\"   {'Batch':<8} {'Vectors':<12} {'First ID':<15} {'Sample Vector (first 10 dims)':<50} {'Client Time (s)':<8}\")\n","        print(f\"   {'-'*100}\")\n","\n","        batch_num = 0\n","        # Iterates through the precomputed vectors in batches and upserts them to Pinecone.\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading binary vectors to Pinecone\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE] # Gets the current batch of vectors.\n","            # Pinecone's `upsert` method expects a list of (id, vector) tuples.\n","            vectors = [(v['id'], v['vector']) for v in batch]\n","\n","            batch_num += 1\n","            t_batch_start = time.perf_counter() # Starts client-side timer for the batch upsert.\n","            self.index.upsert(vectors=vectors) # Performs the upsert operation.\n","            t_batch_end = time.perf_counter() # Ends client-side timer.\n","            batch_time = t_batch_end - t_batch_start # Calculates the duration of the batch upsert.\n","\n","            # Logs batch details for monitoring progress and verifying data.\n","            first_id = batch[0]['id']\n","            sample_vector = batch[0]['vector'][:10]\n","            sample_str = f\"[{','.join([f'{v:.1f}' for v in sample_vector])}...]\".ljust(50)\n","\n","            self.upload_timings[\"upsert_time_s\"] += batch_time # Accumulates total upsert time.\n","            self.upload_timings[\"batch_details\"].append({\n","                \"batch_num\": batch_num,\n","                \"batch_size\": len(batch),\n","                \"first_id\": first_id,\n","                \"upsert_time_s\": batch_time,\n","                \"sample_vector\": sample_vector\n","            }) # Stores details for each batch.\n","\n","            print(f\"   {batch_num:<8} {len(batch):<12} {first_id:<15} {sample_str:<50} {batch_time:<8.4f}\")\n","\n","        print(f\"\\nâ±ï¸  Pinecone Upload Summary:\")\n","        print(f\"    Index Creation Time: {self.upload_timings['index_creation_s']:.4f}s\")\n","        print(f\"    Total Upsert Time: {self.upload_timings['upsert_time_s']:.4f}s\")\n","        if batch_num > 0:\n","            print(f\"    Average Upsert Time per batch: {self.upload_timings['upsert_time_s'] / batch_num:.4f}s\")\n","\n","        return len(self.precomputed_vectors) # Returns the total number of vectors intended for upsert.\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a vector similarity search with a binarized query vector against Pinecone.\n","        It retrieves the top_k most similar documents based on cosine similarity.\n","        \"\"\"\n","        query_embedding = self.query_embeddings[query_idx] # Retrieves the specific query embedding.\n","\n","        t0 = time.perf_counter() # Starts client-side timer for the query operation.\n","        results = self.index.query(vector=query_embedding, top_k=top_k) # Executes the Pinecone query.\n","        t1 = time.perf_counter() # Ends client-side timer.\n","\n","        query_time = t1 - t0 # Calculates client-side query time.\n","        self.search_timings.append({\n","            \"query_time_s\": query_time,\n","            \"query_time_ms\": query_time * 1000 # Stores in milliseconds for easier readability.\n","        }) # Records timings for this search operation.\n","\n","        # Formats results into a dictionary of {doc_id: score} for evaluation compatibility.\n","        # Pinecone returns results in a 'matches' list, each with 'id' and 'score'.\n","        return {str(match['id']): float(match['score']) for match in results.get('matches', [])}\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for Pinecone (client-side durations).\n","        This provides performance metrics for the vector search operations.\n","        \"\"\"\n","        query_times_s = [t[\"query_time_s\"] for t in self.search_timings] # Extracts query times in seconds.\n","        query_times_ms = [t[\"query_time_ms\"] for t in self.search_timings] # Extracts query times in milliseconds.\n","        return {\n","            \"query_time_s\": calculate_timing_stats(query_times_s), # Stats for seconds.\n","            \"query_time_ms\": calculate_timing_stats(query_times_ms) # Stats for milliseconds.\n","        }\n","\n","    def cleanup(self):\n","        \"\"\"Deletes the Pinecone index created for the benchmark.\n","        This is important for managing cloud resources and avoiding unnecessary charges.\n","        \"\"\"\n","        try:\n","            self.client.delete_index(self.index_name) # Calls the SDK to delete the index.\n","            print(f\"ðŸ§¹ Deleted Pinecone index: {self.index_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete Pinecone index '{self.index_name}': {e}. You may need to delete it manually via the Pinecone console if it persists.\")\n","\n","\n","class ElasticsearchBinaryProvider:\n","    \"\"\"Manages interaction with Elasticsearch for binary vector benchmarking,\n","    using its `dense_vector` field type. This class handles index creation,\n","    bulk ingestion of binarized vectors, k-Nearest Neighbor (kNN) searches,\n","    and index cleanup.\n","    \"\"\"\n","    def __init__(self, client, index_name, precomputed_vectors, query_embeddings):\n","        self.client = client # Elasticsearch API client instance.\n","        self.index_name = index_name # Unique name for the Elasticsearch index to be used.\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and binarized vectors.\n","        self.query_embeddings = query_embeddings # List of binarized query embeddings.\n","        self.upload_timings = {\n","            \"index_creation_s\": 0.0, # Time taken to create the Elasticsearch index.\n","            \"server_bulk_time_ms\": 0.0, # Total server-side time for bulk ingestion.\n","            \"client_total_time_s\": 0.0, # Total client-side time for sending bulk requests.\n","            \"batch_details\": [] # Detailed timings for each bulk batch.\n","        }\n","        self.search_timings = [] # Timings for individual search queries.\n","\n","    def upload(self):\n","        \"\"\"Uploads binarized vectors to an Elasticsearch index using a `dense_vector` field.\n","        It first attempts to delete any existing index, then creates a new one with a specific mapping,\n","        and finally ingests vectors in batches using Elasticsearch's bulk API.\n","        \"\"\"\n","        try:\n","            # Deletes an existing index if it's found, ensuring a fresh start for the benchmark.\n","            print(f\"ðŸ—‘ï¸ Attempting to delete existing Elasticsearch index: {self.index_name}\")\n","            # `ignore=[400, 404]` prevents errors if the index doesn't exist or deletion fails for non-critical reasons.\n","            self.client.indices.delete(index=self.index_name, ignore=[400, 404])\n","            time.sleep(2) # Pauses to allow Elasticsearch to process the deletion.\n","        except Exception as e:\n","            print(f\"âš ï¸ Could not delete Elasticsearch index '{self.index_name}' (it might not exist or there was an error): {e}\")\n","\n","        t0 = time.perf_counter() # Starts timer for index creation.\n","\n","        # Defines the index mapping with a `dense_vector` field for storing embeddings.\n","        # 'dims' specifies the vector dimension. 'similarity' can be 'cosine', 'dot_product', or 'l2_norm'.\n","        # 'index': True enables vector indexing for efficient kNN search (HNSW by default in recent ES versions).\n","        index_config = {\n","            \"mappings\": {\n","                \"properties\": {\n","                    \"doc_id\": {\"type\": \"keyword\"}, # Stores document ID as a keyword for exact matching.\n","                    \"embedding\": {\n","                        \"type\": \"dense_vector\",\n","                        \"dims\": VECTOR_DIMENSION, # Sets the dimension to match the binarized embeddings.\n","                        \"index\": True, # Enables vector indexing for kNN search for performance.\n","                        \"similarity\": \"cosine\", # Uses cosine similarity, suitable for binarized float vectors.\n","                    }\n","                }\n","            },\n","            \"settings\": {\n","                \"number_of_shards\": 1, # **User Customizable**: Number of primary shards (1 is common for single-node).\n","                \"number_of_replicas\": 0 # **User Customizable**: Number of replica shards (0 for dev, >0 for prod HA).\n","            }\n","        }\n","\n","        try:\n","            self.client.indices.create(index=self.index_name, body=index_config) # Creates the index with the defined mapping.\n","            print(f\"âœ… Created Elasticsearch index: {self.index_name} (Dimension: {VECTOR_DIMENSION}D, Type: BINARY)\")\n","        except Exception as e:\n","            print(f\"âŒ Elasticsearch index creation error: {e}\")\n","            raise # Re-raises critical errors (e.g., invalid mapping) to halt execution.\n","\n","        t1 = time.perf_counter() # Ends timer for index creation.\n","        self.upload_timings[\"index_creation_s\"] = t1 - t0 # Records index creation time.\n","\n","        print(f\"\\nðŸ“Š Binary Upload Chunks Details for Elasticsearch:\")\n","        print(f\"   Total vectors to index: {len(self.precomputed_vectors)}\")\n","        print(f\"   Batch size for bulk requests: {BATCH_SIZE}\")\n","        print(f\"   Total number of batches: {(len(self.precomputed_vectors) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n","\n","        batch_num = 0\n","        total_uploaded = 0\n","        t_upload_start = time.perf_counter() # Starts timer for total client-side upload duration.\n","\n","        # Uses Elasticsearch's bulk API for efficient ingestion of many documents.\n","        # The bulk API expects newline-delimited JSON for action/metadata and source data.\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading binary vectors to Elasticsearch\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE] # Gets a slice of vectors for the current batch.\n","            batch_num += 1\n","\n","            bulk_body = []\n","            for vec in batch:\n","                # Each item in a bulk request consists of an action (index) and the document itself.\n","                bulk_body.append(json.dumps({\"index\": {\"_index\": self.index_name, \"_id\": vec[\"id\"]}})) # Index action with document ID.\n","                bulk_body.append(json.dumps({\"doc_id\": vec[\"id\"], \"embedding\": vec[\"vector\"]})) # Document source data.\n","\n","            bulk_data = \"\\n\".join(bulk_body) + \"\\n\" # Bulk API requires a newline at the end of each JSON object and a final newline.\n","\n","            try:\n","                t_batch_start = time.perf_counter() # Starts client-side timer for this bulk request.\n","                # `refresh=False` improves ingestion performance; the index will be refreshed manually later.\n","                response = self.client.bulk(body=bulk_data, refresh=False)\n","                t_batch_end = time.perf_counter() # Ends client-side timer.\n","                batch_client_time = t_batch_end - t_batch_start # Records client-side time for this batch.\n","\n","                server_took_ms = response.get(\"took\", 0) # Time taken by Elasticsearch server to process the bulk request.\n","                self.upload_timings[\"server_bulk_time_ms\"] += server_took_ms # Accumulates total server-side bulk time.\n","\n","                # Counts successfully indexed items by checking the status codes in the response.\n","                batch_success = sum(1 for item in response.get(\"items\", [])\n","                                  if \"index\" in item and item[\"index\"].get(\"status\") in [200, 201])\n","                total_uploaded += batch_success\n","\n","                self.upload_timings[\"batch_details\"].append({\n","                    \"batch_num\": batch_num,\n","                    \"batch_size\": len(batch),\n","                    \"server_time_ms\": server_took_ms,\n","                    \"client_time_s\": batch_client_time\n","                }) # Stores details for each batch.\n","            except Exception as e:\n","                print(f\"\\nâŒ Batch {batch_num} failed to upload to Elasticsearch: {e}\") # Logs any bulk upload errors.\n","\n","        t_upload_end = time.perf_counter() # Ends timer for total client-side upload duration.\n","        self.upload_timings[\"client_total_time_s\"] = t_upload_end - t_upload_start # Records total client-side upload time.\n","\n","        try:\n","            # Refreshes the index to make newly ingested documents searchable.\n","            # This is critical after `refresh=False` was used during bulk ingestion.\n","            self.client.indices.refresh(index=self.index_name)\n","            print(f\"âœ… Elasticsearch index '{self.index_name}' refreshed, documents are now searchable.\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Error refreshing Elasticsearch index: {e}\")\n","\n","        print(f\"\\nâ±ï¸  Elasticsearch Upload Timing Summary:\")\n","        print(f\"    Index Creation Time: {self.upload_timings['index_creation_s']:.4f}s\")\n","        print(f\"    Bulk Upload (SERVER processing time): {self.upload_timings['server_bulk_time_ms']/1000:.4f}s (Total time Elasticsearch spent processing all bulk requests)\")\n","        print(f\"    Bulk Upload (CLIENT total request time): {self.upload_timings['client_total_time_s']:.4f}s (Total time spent by client making bulk requests)\")\n","        print(f\"    Total Upload Time (Client-side, incl. index creation): {self.upload_timings['index_creation_s'] + self.upload_timings['client_total_time_s']:.4f}s\")\n","\n","        return total_uploaded # Returns the number of documents successfully uploaded.\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a k-Nearest Neighbor (kNN) search with a binarized query vector against Elasticsearch.\n","        It leverages Elasticsearch's `knn` query type to find the most similar documents.\n","        \"\"\"\n","        query_embedding = self.query_embeddings[query_idx] # Retrieves the specific query embedding.\n","\n","        # Constructs the kNN search query body for Elasticsearch.\n","        # 'k': The number of results to return per shard.\n","        # 'num_candidates': The number of candidates to consider from each shard (higher = more accurate but slower).\n","        query_body = {\n","            \"knn\": {\n","                \"field\": \"embedding\", # The `dense_vector` field containing document embeddings.\n","                \"query_vector\": query_embedding, # The binarized query vector for similarity search.\n","                \"k\": top_k, # Number of results to return from the total kNN search.\n","                \"num_candidates\": min(top_k * 10, 10000) # **User Customizable**: Adjust `num_candidates` for speed/accuracy trade-off.\n","            },\n","            \"size\": top_k, # Ensures only the top_k results are returned in the overall response.\n","            \"_source\": [\"doc_id\"] # Only retrieves the 'doc_id' field to minimize data transfer.\n","        }\n","\n","        try:\n","            t_start = time.perf_counter() # Starts client-side timer for the search request.\n","            response = self.client.search(index=self.index_name, body=query_body) # Executes the Elasticsearch search.\n","            t_end = time.perf_counter() # Ends client-side timer.\n","            client_time_s = t_end - t_start # Calculates client-side search time.\n","\n","            server_took_ms = response.get(\"took\", 0) # Extracts server-side processing time in milliseconds.\n","            server_took_s = server_took_ms / 1000.0 # Converts server time to seconds.\n","\n","            self.search_timings.append({\n","                \"server_time_ms\": server_took_ms,\n","                \"server_time_s\": server_took_s,\n","                \"client_time_s\": client_time_s,\n","                \"client_time_ms\": client_time_s * 1000\n","            }) # Records detailed timings for this search.\n","\n","            hits = response.get(\"hits\", {}).get(\"hits\", []) # Extracts the actual search hits.\n","            results = {}\n","            for i, hit in enumerate(hits):\n","                doc_id = hit.get(\"_id\") # Document ID is available in the `_id` field in ES.\n","                score = hit.get(\"_score\", 1.0 / (1.0 + i)) # Retrieval score; falls back to a decreasing score if `_score` is missing.\n","                results[str(doc_id)] = score # Stores results in {doc_id: score} format.\n","\n","            return results\n","        except Exception as e:\n","            print(f\"\\nâŒ Elasticsearch search error: {e}\")\n","            # Logs zero timings for failed searches to avoid breaking statistics calculations.\n","            self.search_timings.append({\n","                \"server_time_ms\": 0,\n","                \"server_time_s\": 0.0,\n","                \"client_time_s\": 0.0,\n","                \"client_time_ms\": 0.0\n","            })\n","            return {} # Returns empty results on error.\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for Elasticsearch (both server-side and client-side).\n","        This provides a detailed breakdown of where time is spent during search operations.\n","        \"\"\"\n","        server_times_s = [t[\"server_time_s\"] for t in self.search_timings] # Server times in seconds.\n","        server_times_ms = [t[\"server_time_ms\"] for t in self.search_timings] # Server times in milliseconds.\n","        client_times_s = [t[\"client_time_s\"] for t in self.search_timings] # Client times in seconds.\n","        client_times_ms = [t[\"client_time_ms\"] for t in self.search_timings] # Client times in milliseconds.\n","        return {\n","            \"server_s\": calculate_timing_stats(server_times_s),\n","            \"server_ms\": calculate_timing_stats(server_times_ms),\n","            \"client_s\": calculate_timing_stats(client_times_s),\n","            \"client_ms\": calculate_timing_stats(client_times_ms)\n","        }\n","\n","    def cleanup(self):\n","        \"\"\"Deletes the Elasticsearch index created for the benchmark.\n","        This is crucial for managing cloud resources and ensuring a clean state for future runs.\n","        \"\"\"\n","        try:\n","            self.client.indices.delete(index=self.index_name, ignore=[400, 404]) # Deletes index; ignores errors if it's already gone.\n","            print(f\"ðŸ§¹ Deleted Elasticsearch index: {self.index_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete Elasticsearch index '{self.index_name}': {e}. You may need to delete it manually via Kibana or ES API if it persists.\")\n","\n","\n","# -------------------- 10. Main Benchmark Orchestration Function --------------------\n","# The `run_binary_benchmark` function orchestrates the entire benchmarking workflow\n","# for a given dataset and selected providers. It encompasses data loading, embedding\n","# generation, binarization, uploading to various vector databases, performing searches,\n","# evaluating retrieval quality, and saving detailed results.\n","\n","def run_binary_benchmark(dataset_name, provider_names, source=\"beir\"):\n","    \"\"\"Runs a full benchmark with BINARY embeddings for specified providers and a chosen dataset.\n","    This function coordinates data loading, embedding, binarization, upload, search, and evaluation.\n","    \"\"\"\n","    print(f\"\\n{'='*70}\")\n","    print(f\"ðŸš€ Starting Benchmark for Dataset: {dataset_name} (Source: {source.upper()})\")\n","    print(f\"ðŸ“Š Using: BINARY Embeddings (Sign-based Binarization)\")\n","    print(f\"Providers to test: {', '.join(provider_names).capitalize()}\")\n","    print(f\"{'='*70})\")\n","\n","    # Loads dataset based on the selected source (BEIR or MAIR).\n","    if source == \"beir\":\n","        dataset_path = f\"{DATA_ROOT}/{dataset_name}\"\n","\n","        # Downloads BEIR dataset if it's not already present locally.\n","        if not os.path.exists(dataset_path):\n","            print(f\"ðŸ“¦ Downloading BEIR dataset: {dataset_name}. This may take some time.\")\n","            url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n","            util.download_and_unzip(url, DATA_ROOT) # Uses BEIR's utility for download and extraction.\n","\n","        # Loads BEIR corpus, queries, and qrels using their `GenericDataLoader`.\n","        corpus, queries, qrels = GenericDataLoader(dataset_path).load(split=\"test\")\n","        print(f\"âœ… Loaded BEIR dataset: {len(corpus)} documents, {len(queries)} queries, {len(qrels)} qrels.\")\n","    else:  # Logic for MAIR dataset loading.\n","        dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","        docs_path = os.path.join(dataset_path, 'docs')\n","        queries_path = os.path.join(dataset_path, 'queries')\n","\n","        # Checks if MAIR dataset files exist; if not, skips this dataset and logs a warning.\n","        if not os.path.exists(docs_path) or not os.path.exists(queries_path):\n","            print(f\"âš ï¸ MAIR dataset '{dataset_name}' not found at {dataset_path}. Skipping this dataset.\")\n","            return []\n","\n","        print(f\"\\nðŸ“¥ Loading MAIR dataset: {dataset_name} from {dataset_path}\")\n","        corpus = {}\n","        # MAIR datasets can have multiple .jsonl files for documents; all are loaded.\n","        docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","        for file in docs_files:\n","            corpus.update(load_jsonl(os.path.join(docs_path, file))) # Uses helper to load JSONL.\n","\n","        queries = {}\n","        # Similarly, all query .jsonl files are loaded.\n","        query_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","        for file in query_files:\n","            queries.update(load_jsonl(os.path.join(queries_path, file)))\n","\n","        # Extracts qrels (relevance judgments) from MAIR query data using the helper function.\n","        qrels = extract_qrels_from_queries(queries)\n","        print(f\"âœ… Loaded MAIR dataset: {len(corpus)} documents, {len(queries)} queries, {len(qrels)} qrels.\")\n","\n","    # Early exit if any critical component (corpus, queries, or qrels) is missing or empty.\n","    if not corpus or not queries or not qrels:\n","        print(f\"âš ï¸ Essential data (corpus, queries, or qrels) is empty for {dataset_name}. Skipping benchmark.\")\n","        return []\n","\n","    # ========== STEP 1: Generate Float Embeddings with Cohere (Once per dataset) ==========\n","    # This step uses the Cohere API to generate dense float embeddings for both the corpus\n","    # documents and the queries. These float embeddings are then binarized in the next step.\n","    print(f\"\\nðŸ§  Step 1: Generating float embeddings with Cohere {EMBEDDING_MODEL} (Dimension: {VECTOR_DIMENSION}D)...\")\n","\n","    # Limits the number of documents to embed and upload, based on `MAX_UPLOAD_DOCS`.\n","    # This is a user-configurable parameter to manage benchmark scale and cost.\n","    docs = list(corpus.items())[:MAX_UPLOAD_DOCS]\n","    print(f\"  Processing up to {len(docs)} documents for embedding (limited by MAX_UPLOAD_DOCS={MAX_UPLOAD_DOCS}).\")\n","\n","    # Extracts text content from corpus documents for embedding. It handles various common\n","    # field names (`text`, `contents`, `body`, etc.) and falls back to string conversion.\n","    texts = []\n","    corpus_texts = {} # Stores original texts, which can be useful for potential reranking or debugging.\n","    for doc_id, doc_content in docs:\n","        text = None\n","        if isinstance(doc_content, dict):\n","            # Prioritizes common fields for text content in documents.\n","            for field in ['text', 'contents', 'content', 'body', 'passage', 'document', 'title', 'abstract']:\n","                if field in doc_content and doc_content[field]:\n","                    val = doc_content[field]\n","                    if isinstance(val, str) and val.strip(): # Ensures it's a non-empty string.\n","                        text = val\n","                        break\n","            # Fallback to combining title and text if both are present.\n","            if not text and 'title' in doc_content and 'text' in doc_content:\n","                text = f\"{doc_content['title']}. {doc_content['text']}\".strip()\n","            # Last resort: converts the entire dictionary to a string if no specific text field is found.\n","            # This might not be ideal for embedding but prevents errors.\n","            if not text:\n","                text = str(doc_content)\n","        else:\n","            text = str(doc_content) # For non-dict corpus entries (e.g., if corpus directly contains strings).\n","\n","        final_text = text if text else \"document\" # Provides a default if text extraction still yields empty.\n","        texts.append(final_text)\n","        corpus_texts[str(doc_id)] = final_text # Stores the processed text associated with its ID.\n","\n","    doc_ids = [str(d[0]) for d in docs] # Ensures all document IDs are strings for consistency.\n","\n","    print(f\"  ðŸ“„ Generating corpus embeddings for {len(texts)} documents using Cohere {EMBEDDING_MODEL}...\")\n","    t_embed_start = time.perf_counter() # Starts timer for corpus embedding generation.\n","\n","    corpus_embeddings_float = []\n","    # Processes corpus embeddings in batches to optimize API calls to Cohere and manage memory.\n","    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding corpus documents\"):\n","        batch_texts = texts[i:i+BATCH_SIZE]\n","        response = cohere_client.embed(\n","            texts=batch_texts,\n","            model=EMBEDDING_MODEL,\n","            input_type=INPUT_TYPE_CORPUS # Specifies input type for optimal embedding generation.\n","        )\n","        corpus_embeddings_float.extend(response.embeddings) # Collects generated embeddings.\n","\n","    t_embed_end = time.perf_counter() # Ends timer for corpus embedding generation.\n","    embedding_time = t_embed_end - t_embed_start # Calculates total corpus embedding time.\n","\n","    # Generates query embeddings in a similar batched manner.\n","    print(f\"  ðŸ” Generating query embeddings for {len(queries)} queries using Cohere {EMBEDDING_MODEL}...\")\n","    query_ids = list(queries.keys())\n","    query_texts = []\n","\n","    for qid in query_ids:\n","        q = queries[qid]\n","        text = None\n","        if isinstance(q, dict):\n","            # Prioritizes common query text fields.\n","            text = (q.get('text') or q.get('query') or q.get('instruction') or\n","                   q.get('question') or q.get('query_text') or str(q)) # Falls back to string conversion.\n","        else:\n","            text = str(q) # For non-dict query entries.\n","        query_texts.append(text if text else \"query\") # Provides a default if query text is empty.\n","\n","    t_query_start = time.perf_counter() # Starts timer for query embedding generation.\n","    query_embeddings_float = []\n","    for i in tqdm(range(0, len(query_texts), BATCH_SIZE), desc=\"Embedding queries\"):\n","        batch_texts = [str(t) if t else \"query\" for t in query_texts[i:i+BATCH_SIZE]] # Ensures texts are strings.\n","        response = cohere_client.embed(\n","            texts=batch_texts,\n","            model=EMBEDDING_MODEL,\n","            input_type=INPUT_TYPE_QUERY # Specifies input type for optimal embedding generation.\n","        )\n","        query_embeddings_float.extend(response.embeddings) # Collects generated embeddings.\n","\n","    t_query_end = time.perf_counter() # Ends timer for query embedding generation.\n","    query_embedding_time = t_query_end - t_query_start # Calculates total query embedding time.\n","\n","    print(f\"\\nâ±ï¸  Embedding Generation Time Summary:\")\n","    print(f\"    Corpus embeddings ({len(corpus_embeddings_float)} documents): {embedding_time:.4f}s\")\n","    print(f\"    Query embeddings ({len(query_embeddings_float)} queries): {query_embedding_time:.4f}s\")\n","\n","    # ========== STEP 2: Binarize Embeddings (Sign-based) ==========\n","    # This is the core step for binary embedding benchmarking. It converts the\n","    # dense float embeddings generated by Cohere into a binary representation\n","    # (0s and 1s) based on the sign of each component.\n","    print(f\"\\nðŸ”„ Step 2: Binarizing embeddings (sign-based method: values >= 0 -> 1, values < 0 -> 0)...\")\n","\n","    t_binarize_start = time.perf_counter() # Starts timer for binarization.\n","\n","    # Binarizes corpus and query embeddings. The `binarize_embeddings` function\n","    # converts them to float32 (0.0 or 1.0) to maintain compatibility with vector databases.\n","    corpus_embeddings_binary = binarize_embeddings(corpus_embeddings_float)\n","    query_embeddings_binary = binarize_embeddings(query_embeddings_float)\n","    t_binarize_end = time.perf_counter() # Ends timer for binarization.\n","    binarization_time = t_binarize_end - t_binarize_start # Calculates total binarization time.\n","\n","    # Calculates space savings achieved by binarization. This provides an estimate\n","    # of memory reduction, assuming the binary data is stored as float32 for compatibility.\n","    # Actual savings can be higher if the database supports native bit storage.\n","    float_size_mb = (len(corpus_embeddings_float) * VECTOR_DIMENSION * 4) / (1024 * 1024) # Size of original float embeddings (float32 = 4 bytes).\n","    binary_size_mb = (corpus_embeddings_binary.nbytes) / (1024 * 1024) # Size of binarized embeddings in memory (still float32).\n","    space_savings = (1 - (binary_size_mb / float_size_mb)) * 100 # Percentage of space saved.\n","\n","    print(f\"\\nðŸ’¾ Space and Performance Efficiency After Binarization:\")\n","    print(f\"    Original Float Embeddings Size: {float_size_mb:.2f} MB (if stored as float32)\")\n","    print(f\"    Binarized Embeddings Size (in memory as float32): {binary_size_mb:.2f} MB\")\n","    print(f\"    Achieved Space Savings: {space_savings:.1f}% (effectively {float_size_mb/binary_size_mb:.1f}x compression when stored as float32 representing binary)\")\n","    print(f\"    Binarization processing time: {binarization_time:.4f}s\")\n","    print(f\"    Note: Actual storage savings can be much higher (up to 32x) if the vector database supports native bit/binary types.\")\n","\n","    # ========== STEP 3: Prepare Vectors for Upload to Vector Databases ==========\n","    # This step formats the binarized vectors into a list of dictionaries, which is\n","    # the standard format expected by most vector database APIs for bulk uploads.\n","    print(f\"\\nðŸ“¦ Step 3: Preparing vectors for upload to selected vector databases...\")\n","\n","    # Creates a list of dictionaries, each containing an 'id' and a 'vector' (list of float 0.0s or 1.0s).\n","    binary_vectors_for_upload = []\n","    for doc_id, vec in zip(doc_ids, corpus_embeddings_binary):\n","        binary_vectors_for_upload.append({\n","            \"id\": doc_id,\n","            \"vector\": vec.tolist() if isinstance(vec, np.ndarray) else list(vec), # Ensures vector is a standard Python list.\n","        })\n","\n","    # Converts query embeddings to a list of lists format for consistency with API calls.\n","    query_embeddings_list_for_search = [\n","        q.tolist() if isinstance(q, np.ndarray) else list(q) for q in query_embeddings_binary\n","    ]\n","\n","    # ========== OPTIONAL: Save Binarized Embeddings to Disk ==========\n","    # This section provides an interactive option for the user to save the generated\n","    # binary embeddings locally. This can be useful for inspection, debugging, or\n","    # for using the embeddings with other tools outside of this notebook.\n","    download_choice = input(f\"\\nðŸ’¾ Do you want to download the binarized embeddings (saved in multiple formats to {DRIVE_PATH})? (y/n): \").strip().lower()\n","    if download_choice in ['y', 'yes']:\n","        # Defines a subdirectory within DRIVE_PATH to store the embeddings.\n","        embeddings_output_dir = os.path.join(DRIVE_PATH, f\"{source}_{dataset_name}_binary_embeddings\")\n","        save_binary_embeddings(doc_ids, corpus_embeddings_binary, query_ids, query_embeddings_binary,\n","                              dataset_name, embeddings_output_dir) # Uses the helper function to save.\n","    else:\n","        print(f\"â­ï¸  Skipping binary embedding download.\")\n","\n","    results_all = [] # A list to collect benchmark results from all provider runs for this dataset.\n","\n","    # ========== STEP 4: Benchmark Each Selected Provider with Binary Vectors ==========\n","    # This is the core loop of the benchmark. For each selected vector database provider,\n","    # it initializes the provider, uploads the binarized data, performs searches using\n","    # binarized queries, evaluates the retrieval quality, and records performance metrics.\n","    for provider_name in provider_names:\n","        print(f\"\\n{'â”€'*70}\")\n","        print(f\"ðŸ”§ Running benchmark for Provider: {provider_name.upper()} | Using Binarized Vectors\")\n","        print(f\"{'â”€'*70}\")\n","\n","        try:\n","            provider = None\n","            # Initializes the appropriate provider class based on the `provider_name`.\n","            # This ensures that provider-specific API calls and logic are used.\n","            if provider_name == 'moorcheh':\n","                if 'moorcheh' not in clients: # Skips if Moorcheh client was not initialized (e.g., missing API key).\n","                    print(f\"âš ï¸ Skipping {provider_name}: Moorcheh client not initialized (API key missing?).\")\n","                    continue\n","                safe_name = dataset_name.replace('/', '-').replace('.', '_') # Creates a URL-safe namespace name.\n","                namespace_name = f\"{source}-{safe_name}-binary-v4\"[:63] # Moorcheh namespace names max 63 chars.\n","                provider = MoorchehBinaryProvider(\n","                    clients['moorcheh'], namespace_name,\n","                    binary_vectors_for_upload, query_embeddings_list_for_search\n","                )\n","            elif provider_name == 'pinecone':\n","                if 'pinecone' not in clients: # Skips if Pinecone client was not initialized.\n","                    print(f\"âš ï¸ Skipping {provider_name}: Pinecone client not initialized (API key missing?).\")\n","                    continue\n","                safe_name = dataset_name.replace('/', '-').replace('_', '-').lower() # Pinecone index names have specific rules.\n","                index_name = f\"{source}-{safe_name}-binary\"[:45] # Pinecone index names max 45 chars.\n","                provider = PineconeBinaryProvider(\n","                    clients['pinecone'], index_name,\n","                    binary_vectors_for_upload, query_embeddings_list_for_search\n","                )\n","            elif provider_name == 'elasticsearch':\n","                if es_client is None: # Skips if ES client failed to connect.\n","                    print(f\"âš ï¸ Skipping {provider_name}: Elasticsearch client not connected or configured.\")\n","                    continue\n","                safe_name = dataset_name.replace('/', '-').replace('_', '-').lower() # ES index names are typically lowercase.\n","                index_name = f\"{source}-{safe_name}-binary\" # ES index names can be longer.\n","                provider = ElasticsearchBinaryProvider(\n","                    es_client, index_name,\n","                    binary_vectors_for_upload, query_embeddings_list_for_search\n","                )\n","\n","            if provider is None: # Catches cases where no provider object was created due to configuration issues.\n","                print(f\"âŒ Could not initialize provider: {provider_name}. Check configuration and API keys.\")\n","                continue\n","\n","            # Uploads binarized data to the current provider's vector database.\n","            print(f\"\\nðŸ“¤ Uploading binary vectors to {provider_name}...\")\n","            num_uploaded = provider.upload()\n","            if num_uploaded == 0: # If no documents were uploaded, skips search and evaluation for this provider.\n","                print(f\"âš ï¸ No documents uploaded to {provider_name}. Skipping search and evaluation for this provider.\")\n","                del provider # Cleans up the provider object.\n","                clean_memory() # Forces garbage collection.\n","                continue\n","\n","            # Performs searches for all queries against the current provider.\n","            print(f\"\\nðŸ” Performing searches with binary queries on {provider_name} (top_k={TOP_K_SEARCH})...\")\n","            results_per_provider = {} # Stores search results for this specific provider.\n","\n","            for i, qid in enumerate(tqdm(query_ids, desc=f\"Searching {provider_name} for {dataset_name}\")):\n","                try:\n","                    results_per_provider[qid] = provider.search(i, top_k=TOP_K_SEARCH)\n","                except Exception as e:\n","                    print(f\"\\nâŒ Query {i+1} ('{query_ids[i]}') failed for {provider_name}: {e}\")\n","                    results_per_provider[qid] = {} # Logs empty results for failed queries to prevent further errors.\n","\n","            # Retrieves and prints search timing statistics from the provider object.\n","            search_stats = provider.get_search_stats()\n","\n","            if provider_name == 'moorcheh':\n","                print(f\"\\nâ±ï¸  {provider_name.capitalize()} Search Timing Summary (Server-Side):\")\n","                overall = search_stats.get('overall', {}) # Retrieves overall timing statistics.\n","                print(f\"    Overall Mean Query Time: {overall.get('mean', 0)*1000:.2f}ms\")\n","                print(f\"    Overall Median Query Time: {overall.get('median', 0)*1000:.2f}ms\")\n","                print(f\"    Min: {overall.get('min', 0)*1000:.2f}ms, Max: {overall.get('max', 0)*1000:.2f}ms\")\n","                # Optionally prints detailed component timings for Moorcheh if available.\n","                if len(search_stats) > 1: # Checks if detailed components exist beyond 'overall'.\n","                    print(f\"    Detailed Components (Mean):\")\n","                    for key, stats_comp in search_stats.items():\n","                        if key != 'overall':\n","                            print(f\"      - {key.replace('moorcheh_', '').replace('_s', '')}: {stats_comp.get('mean',0)*1000:.2f}ms\")\n","            elif provider_name == 'pinecone':\n","                print(f\"\\nâ±ï¸  {provider_name.capitalize()} Search Timing Summary (Client-Side):\")\n","                query_stats = search_stats.get('query_time_ms', {}) # Retrieves query timing statistics in milliseconds.\n","                print(f\"    Mean Query Time: {query_stats.get('mean', 0):.2f}ms\")\n","                print(f\"    Median Query Time: {query_stats.get('median', 0):.2f}ms\")\n","                print(f\"    Min: {query_stats.get('min', 0):.2f}ms, Max: {query_stats.get('max', 0):.2f}ms\")\n","                print(f\"    Std Dev: {query_stats.get('std', 0):.2f}ms\")\n","            elif provider_name == 'elasticsearch':\n","                print(f\"\\nâ±ï¸  {provider_name.capitalize()} Search Timing Summary:\")\n","                # Displays both server-side and client-side timings for Elasticsearch.\n","                server_stats_ms = search_stats.get('server_ms', {})\n","                client_stats_ms = search_stats.get('client_ms', {})\n","                print(f\"    Server-side Mean Query Time: {server_stats_ms.get('mean', 0):.2f}ms\")\n","                print(f\"    Client-side Mean Query Time: {client_stats_ms.get('mean', 0):.2f}ms\")\n","\n","            # Evaluates retrieval quality using BEIR's evaluation suite or a custom equivalent.\n","            print(f\"\\nðŸ“Š Evaluating retrieval quality for {provider_name} (metrics at K={K_VALUES})...\")\n","            # The `EvaluateRetrieval` class from BEIR is a robust tool for calculating standard IR metrics.\n","            evaluator = EvaluateRetrieval()\n","            ndcg, _map, recall, precision = evaluator.evaluate(qrels, results_per_provider, k_values=K_VALUES)\n","            format_and_print_metrics(ndcg, _map, recall, precision, ks=K_VALUES)\n","\n","            # Stores all metrics and timing data into a result dictionary for CSV output.\n","            metrics = extract_all_metrics(ndcg, _map, recall, precision, ks=K_VALUES)\n","            result_entry = {\n","                \"Dataset\": dataset_name,\n","                \"Source\": source.upper(),\n","                \"Provider\": provider_name,\n","                \"Num_Corpus\": len(corpus), # Original number of corpus documents.\n","                \"Num_Uploaded\": num_uploaded, # Actual number of documents successfully uploaded to the provider.\n","                \"Num_Queries\": len(queries),\n","                \"Vector_Dimension\": VECTOR_DIMENSION,\n","                \"Embedding_Model\": EMBEDDING_MODEL,\n","                \"Binarization_Method\": \"sign-based\",\n","                \"Embedding_Time_s\": round(embedding_time, 4), # Time to generate initial float embeddings.\n","                \"Query_Embedding_Time_s\": round(query_embedding_time, 4), # Time to generate query float embeddings.\n","                \"Binarization_Time_s\": round(binarization_time, 4), # Time to convert to binary.\n","                \"Float_Size_MB\": round(float_size_mb, 2), # Estimated size of original float embeddings.\n","                \"Binary_Size_MB\": round(binary_size_mb, 2), # Estimated size of binarized embeddings (as float32).\n","                \"Space_Savings_Pct\": round(space_savings, 1), # Percentage of space saved.\n","                \"Compression_Ratio\": round(float_size_mb / binary_size_mb, 1), # Compression ratio.\n","            }\n","\n","            # Adds provider-specific timing details to the result dictionary.\n","            if provider_name == 'moorcheh':\n","                overall_stats = search_stats.get('overall', {}) # Retrieves overall search statistics.\n","                result_entry[\"Upload_Time_s\"] = round(provider.upload_timings[\"server_upload_time_s\"], 4)\n","                result_entry[\"Search_Server_Total_s\"] = round(overall_stats.get('total', 0), 4)\n","                result_entry[\"Search_Server_Mean_s\"] = round(overall_stats.get('mean', 0), 4)\n","                result_entry[\"Search_Server_Median_s\"] = round(overall_stats.get('median', 0), 4)\n","                result_entry[\"Search_Server_Min_s\"] = round(overall_stats.get('min', 0), 4)\n","                result_entry[\"Search_Server_Max_s\"] = round(overall_stats.get('max', 0), 4)\n","                result_entry[\"Search_Server_Std_s\"] = round(overall_stats.get('std', 0), 4)\n","                result_entry[\"Search_Server_Mean_ms\"] = round(overall_stats.get('mean', 0) * 1000, 2)\n","\n","                # Adds detailed Moorcheh timing breakdowns in milliseconds (if available).\n","                for key, stats_comp in search_stats.items():\n","                    if key != 'overall' and isinstance(stats_comp, dict):\n","                        clean_key = key.replace('moorcheh_', '').replace('_s', '') # Cleans key name for CSV column.\n","                        result_entry[f\"Moorcheh_{clean_key}_mean_s\"] = round(stats_comp.get('mean', 0), 4)\n","                        result_entry[f\"Moorcheh_{clean_key}_mean_ms\"] = round(stats_comp.get('mean', 0) * 1000, 4)\n","\n","            elif provider_name == 'pinecone':\n","                # Pinecone upload timing includes index creation and upsert time.\n","                result_entry[\"Upload_Index_Creation_s\"] = round(provider.upload_timings[\"index_creation_s\"], 4)\n","                result_entry[\"Upload_Upsert_s\"] = round(provider.upload_timings[\"upsert_time_s\"], 4)\n","                result_entry[\"Upload_Total_s\"] = round(\n","                    provider.upload_timings[\"index_creation_s\"] + provider.upload_timings[\"upsert_time_s\"], 4\n","                )\n","                # Pinecone search timing (client-side).\n","                query_stats_s = search_stats.get('query_time_s', {})\n","                query_stats_ms = search_stats.get('query_time_ms', {})\n","\n","                result_entry[\"Search_Total_s\"] = round(query_stats_s.get('total', 0), 4)\n","                result_entry[\"Search_Mean_s\"] = round(query_stats_s.get('mean', 0), 4)\n","                result_entry[\"Search_Median_s\"] = round(query_stats_s.get('median', 0), 4)\n","                result_entry[\"Search_Min_s\"] = round(query_stats_s.get('min', 0), 4)\n","                result_entry[\"Search_Max_s\"] = round(query_stats_s.get('max', 0), 4)\n","                result_entry[\"Search_Std_s\"] = round(query_stats_s.get('std', 0), 4)\n","\n","                result_entry[\"Search_Mean_ms\"] = round(query_stats_ms.get('mean', 0), 2)\n","                result_entry[\"Search_Median_ms\"] = round(query_stats_ms.get('median', 0), 2)\n","                result_entry[\"Search_Min_ms\"] = round(query_stats_ms.get('min', 0), 2)\n","                result_entry[\"Search_Max_ms\"] = round(query_stats_ms.get('max', 0), 2)\n","                result_entry[\"Search_Std_ms\"] = round(query_stats_ms.get('std', 0), 2)\n","\n","            elif provider_name == 'elasticsearch':\n","                # Elasticsearch upload timing, differentiating server and client times.\n","                result_entry[\"Upload_Index_Creation_s\"] = round(provider.upload_timings[\"index_creation_s\"], 4)\n","                result_entry[\"Upload_Bulk_Server_s\"] = round(provider.upload_timings[\"server_bulk_time_ms\"] / 1000, 4)\n","                result_entry[\"Upload_Bulk_Client_s\"] = round(provider.upload_timings[\"client_total_time_s\"], 4)\n","                result_entry[\"Upload_Total_s\"] = round(\n","                    provider.upload_timings[\"index_creation_s\"] + provider.upload_timings[\"client_total_time_s\"], 4\n","                )\n","                # Elasticsearch search timing, differentiating server and client times.\n","                server_stats_s = search_stats.get('server_s', {})\n","                server_stats_ms = search_stats.get('server_ms', {})\n","                client_stats_s = search_stats.get('client_s', {})\n","                client_stats_ms = search_stats.get('client_ms', {})\n","\n","                result_entry[\"Search_Server_Total_s\"] = round(server_stats_s.get('total', 0), 4)\n","                result_entry[\"Search_Server_Mean_s\"] = round(server_stats_s.get('mean', 0), 4)\n","                result_entry[\"Search_Server_Median_s\"] = round(server_stats_s.get('median', 0), 4)\n","                result_entry[\"Search_Server_Min_s\"] = round(server_stats_s.get('min', 0), 4)\n","                result_entry[\"Search_Server_Max_s\"] = round(server_stats_s.get('max', 0), 4)\n","                result_entry[\"Search_Server_Std_s\"] = round(server_stats_s.get('std', 0), 4)\n","\n","                result_entry[\"Search_Server_Mean_ms\"] = round(server_stats_ms.get('mean', 0), 2)\n","                result_entry[\"Search_Server_Median_ms\"] = round(server_stats_ms.get('median', 0), 2)\n","                result_entry[\"Search_Server_Min_ms\"] = round(server_stats_ms.get('min', 0), 2)\n","                result_entry[\"Search_Server_Max_ms\"] = round(server_stats_ms.get('max', 0), 2)\n","                result_entry[\"Search_Server_Std_ms\"] = round(server_stats_ms.get('std', 0), 2)\n","\n","                result_entry[\"Search_Client_Total_s\"] = round(client_stats_s.get('total', 0), 4)\n","                result_entry[\"Search_Client_Mean_s\"] = round(client_stats_s.get('mean', 0), 4)\n","                result_entry[\"Search_Client_Median_s\"] = round(client_stats_s.get('median', 0), 4)\n","                result_entry[\"Search_Client_Min_s\"] = round(client_stats_s.get('min', 0), 4)\n","                result_entry[\"Search_Client_Max_s\"] = round(client_stats_s.get('max', 0), 4)\n","                result_entry[\"Search_Client_Std_s\"] = round(client_stats_s.get('std', 0), 4)\n","\n","                result_entry[\"Search_Client_Mean_ms\"] = round(client_stats_ms.get('mean', 0), 2)\n","                result_entry[\"Search_Client_Median_ms\"] = round(client_stats_ms.get('median', 0), 2)\n","                result_entry[\"Search_Client_Min_ms\"] = round(client_stats_ms.get('min', 0), 2)\n","                result_entry[\"Search_Client_Max_ms\"] = round(client_stats_ms.get('max', 0), 2)\n","                result_entry[\"Search_Client_Std_ms\"] = round(client_stats_ms.get('std', 0), 2)\n","\n","            result_entry.update(metrics) # Adds retrieval quality metrics to the result entry.\n","\n","            save_results_to_csv(result_entry, CSV_PATH) # Saves results to CSV after each provider/dataset run.\n","            results_all.append(result_entry) # Appends to the list of all results for final summary.\n","\n","            # Cleanup: Asks the user whether to delete the created index/namespace to manage cloud resources.\n","            should_delete = should_cleanup_namespace(provider_name, dataset_name)\n","            if should_delete:\n","                provider.cleanup() # Calls the provider's cleanup method.\n","            else:\n","                print(f\"ðŸ’¾ Keeping {provider_name} resources for {dataset_name}. Remember to delete them manually if no longer needed to avoid charges.\")\n","\n","            del provider # Explicitly deletes the provider object to free up resources.\n","            del results_per_provider # Clears search results for the next iteration.\n","            clean_memory() # Forces garbage collection.\n","\n","        except Exception as e:\n","            print(f\"âŒ An unexpected error occurred during {provider_name} benchmark for {dataset_name}: {e}\")\n","            import traceback\n","            traceback.print_exc() # Prints full traceback for debugging.\n","            clean_memory()\n","\n","    # Clears large embedding arrays from memory after all providers for a dataset have been processed.\n","    del corpus_embeddings_float\n","    del query_embeddings_float\n","    del corpus_embeddings_binary\n","    del query_embeddings_binary\n","    del binary_vectors_for_upload\n","    del query_embeddings_list_for_search\n","    clean_memory()\n","\n","    return results_all\n","\n","\n","# -------------------- 11. Interactive Dataset Selection Loop --------------------\n","# This section allows the user to interactively select which datasets to run the\n","# benchmark on. The benchmark will execute for each selected dataset and provider combination.\n","\n","if DATASET_SOURCE == \"beir\":\n","    print(\"\\nðŸ“Š Available BEIR datasets for benchmarking:\")\n","    for ds in BEIR_DATASETS_SORTED_DISPLAY:\n","        print(ds) # Displays user-friendly list of BEIR datasets.\n","    available_datasets = BEIR_DATASETS\n","    dataset_map = {i+1: ds for i, ds in enumerate(BEIR_DATASETS)} # Maps display number to actual dataset name.\n","else:  # Logic for MAIR dataset selection.\n","    print(\"\\nðŸ“Š Discovering MAIR datasets in your combined path...\")\n","    # Retrieves available MAIR datasets using the helper function, categorized and with sizes.\n","    mair_datasets, datasets_by_category, dataset_sizes = get_mair_datasets()\n","\n","    if not mair_datasets: # If no MAIR datasets are found, prompts the user and exits.\n","        print(\"âŒ No MAIR datasets found! Please ensure you have run the previous step to download and combine them into the specified MAIR_COMBINED_PATH.\")\n","        exit(1) # Exits if no datasets to benchmark.\n","\n","    print(f\"\\nðŸ“‚ Available MAIR datasets by category ({len(mair_datasets)} total found):\")\n","    dataset_index = 1\n","    dataset_map = {}\n","    # Displays MAIR datasets grouped by category, sorted by document count (largest first).\n","    for category in sorted(datasets_by_category.keys()):\n","        datasets_in_category = datasets_by_category[category]\n","        if datasets_in_category:\n","            # Sorts datasets within each category by document count in descending order.\n","            sorted_datasets_in_category = sorted(datasets_in_category, key=lambda d: dataset_sizes.get(d, 0), reverse=True)\n","            print(f\"\\n  ðŸ“ {category} ({len(sorted_datasets_in_category)} datasets):\")\n","            for dataset in sorted_datasets_in_category:\n","                # Formats the document count for readability (e.g., K for thousands, M for millions).\n","                size_str = f\"{dataset_sizes.get(dataset, 0):,}\" if dataset_sizes.get(dataset, 0) < 1000000 else f\"{dataset_sizes.get(dataset, 0)/1000000:.1f}M\"\n","                print(f\"     {dataset_index}. {dataset} ({size_str} docs)\")\n","                dataset_map[dataset_index] = dataset # Maps the display index to the actual dataset name.\n","                dataset_index += 1\n","    available_datasets = mair_datasets # Keeps a list of all discovered MAIR datasets.\n","\n","print(f\"\\nðŸ’¡ Binary Embedding Benchmark Overview (Key Configuration):\")\n","print(f\"  â€¢ Embeddings will be generated once per dataset using Cohere {EMBEDDING_MODEL}\")\n","print(f\"  â€¢ Embeddings are then binarized with a sign-based method (values >= 0 become 1, values < 0 become 0)\")\n","print(f\"  â€¢ Benchmarking will be performed on the selected providers: {', '.join(selected_providers).capitalize()} (if configured)\")\n","print(f\"  â€¢ Expected space compression: ~32x (from float32 to binary representation, if database supports native bit types)\")\n","print(f\"  â€¢ Evaluation includes retrieval quality (NDCG, MAP, Recall, Precision) and performance (upload/search times)\")\n","\n","all_results = [] # List to store results from all benchmark runs across all datasets and providers.\n","\n","while True:\n","    # Prompts the user for dataset selection based on the chosen source (BEIR or MAIR).\n","    if DATASET_SOURCE == \"beir\":\n","        choice = input(f\"\\nâž¡ï¸ Enter dataset number (1-{len(BEIR_DATASETS)}) to benchmark, or type 'stop' to finish: \").strip().lower()\n","    else:\n","        choice = input(f\"\\nâž¡ï¸ Enter dataset number (1-{len(dataset_map)}) to benchmark, or type 'stop' to finish: \").strip().lower()\n","\n","    if choice == \"stop\":\n","        print(\"ðŸ Stopping benchmark process as requested.\")\n","        break\n","\n","    try:\n","        idx = int(choice) # Converts user input to an integer index.\n","        dataset_name = None\n","        if DATASET_SOURCE == \"beir\":\n","            if 1 <= idx <= len(BEIR_DATASETS):\n","                dataset_name = BEIR_DATASETS[idx - 1] # Retrieves BEIR dataset name from the list.\n","            else:\n","                print(f\"âš ï¸ Invalid number. Please enter a number between 1 and {len(BEIR_DATASETS)} for BEIR datasets.\")\n","                continue # Asks for input again.\n","        else: # MAIR dataset selection.\n","            if idx in dataset_map:\n","                dataset_name = dataset_map[idx] # Retrieves MAIR dataset name from the mapped dictionary.\n","            else:\n","                print(f\"âš ï¸ Invalid number. Please enter a valid number from the MAIR list above.\")\n","                continue # Asks for input again.\n","    except ValueError:\n","        print(\"âš ï¸ Invalid input. Please enter a number corresponding to a dataset, or type 'stop'.\")\n","        continue # Asks for input again.\n","\n","    # Runs the benchmark for the selected dataset and the chosen providers.\n","    # The results from this single dataset run are appended to the overall results list.\n","    results_for_current_dataset = run_binary_benchmark(dataset_name, selected_providers, source=DATASET_SOURCE)\n","    all_results.extend(results_for_current_dataset)\n","\n","# -------------------- 12. Final Benchmark Summary and Analysis --------------------\n","# After all selected datasets are processed, this section provides an overall summary\n","# of the benchmark results. It includes tables of key metrics, average performance\n","# comparisons by provider, and insights into space efficiency and timing.\n","\n","if all_results: # Checks if any results were generated from the benchmark runs.\n","    print(f\"\\n{'='*70}\")\n","    print(\"ðŸ BENCHMARK COMPLETE - BINARY EMBEDDINGS (Overall Summary)\")\n","    print(f\"{'='*70}\")\n","\n","    df = pd.DataFrame(all_results) # Converts the list of result dictionaries into a Pandas DataFrame for easy analysis.\n","\n","    print(f\"\\nðŸ’¾ All detailed results have been saved to: {CSV_PATH}\")\n","\n","    # Summary table for key metrics, providing a concise overview of each benchmark run.\n","    print(\"\\nðŸ“Š Overall Results Summary (Key Metrics per Run):\")\n","    print(\"â”€\" * 70)\n","    # Defines key columns for a concise summary display in the console.\n","    summary_cols = ['Dataset', 'Source', 'Provider', 'NDCG@10', 'MAP@10', 'Recall@100', 'Search_Mean_ms']\n","    available_cols = [col for col in summary_cols if col in df.columns] # Filters for columns that actually exist in the DataFrame.\n","    if available_cols:\n","        # Displays a concise table of results using Pandas' `to_string` for better console formatting.\n","        print(df[available_cols].to_string(index=False))\n","\n","    # Calculates and displays average performance by provider across all tested datasets.\n","    print(\"\\nðŸ“ˆ Average Performance by Provider (across all tested datasets and runs):\")\n","    print(\"â”€\" * 70)\n","    for provider in df['Provider'].unique(): # Iterates through each unique provider that was benchmarked.\n","        provider_data = df[df['Provider'] == provider] # Filters DataFrame for the current provider.\n","        print(f\"\\n{provider.upper()}:\")\n","        print(f\"  Average NDCG@10:     {provider_data['NDCG@10'].mean():.4f}\")\n","        print(f\"  Average MAP@10:      {provider_data['MAP@10'].mean():.4f}\")\n","        print(f\"  Average Recall@100:  {provider_data['Recall@100'].mean():.4f}\")\n","        if 'Search_Mean_ms' in provider_data.columns: # General search time (e.g., Pinecone client-side total).\n","            print(f\"  Average Search Mean: {provider_data['Search_Mean_ms'].mean():.2f}ms\")\n","        if 'Search_Server_Mean_ms' in provider_data.columns: # Server-side search time (e.g., for ES, Moorcheh).\n","            print(f\"  Average Search Server Mean: {provider_data['Search_Server_Mean_ms'].mean():.2f}ms\")\n","        if 'Search_Client_Mean_ms' in provider_data.columns: # Client-side search time (e.g., for ES).\n","            print(f\"  Average Search Client Mean: {provider_data['Search_Client_Mean_ms'].mean():.2f}ms\")\n","\n","    # Compares speed (average search time per query) across providers.\n","    print(\"\\nâš¡ Speed Comparison (Average Search Time per Query across providers):\")\n","    print(\"â”€\" * 70)\n","    for provider in df['Provider'].unique():\n","        provider_data = df[df['Provider'] == provider]\n","        if 'Search_Mean_ms' in provider_data.columns: # Uses general mean search time if available.\n","            avg_time = provider_data['Search_Mean_ms'].mean()\n","            print(f\"  {provider.upper():15} {avg_time:8.2f}ms\")\n","        elif 'Search_Server_Mean_ms' in provider_data.columns: # Fallback to server time for providers like ES/Moorcheh.\n","            avg_server = provider_data['Search_Server_Mean_ms'].mean()\n","            avg_client = provider_data.get('Search_Client_Mean_ms', pd.Series([0.0])).mean() # Retrieves client time if it exists.\n","            print(f\"  {provider.upper():15} Server: {avg_server:8.2f}ms | Client: {avg_client:8.2f}ms\")\n","\n","    # Compares upload times (total average upload duration) across providers.\n","    print(\"\\nðŸ“¤ Upload Time Comparison (Average across providers and datasets):\")\n","    print(\"â”€\" * 70)\n","    for provider in df['Provider'].unique():\n","        provider_data = df[df['Provider'] == provider]\n","        if provider == 'moorcheh' and 'Upload_Time_s' in provider_data.columns: # Moorcheh has a specific upload time key.\n","            avg_upload = provider_data['Upload_Time_s'].mean()\n","            print(f\"  {provider.upper():15} {avg_upload:8.2f}s (server-side total upload)\")\n","        elif provider == 'pinecone' and 'Upload_Total_s' in provider_data.columns: # Pinecone has a total upload key.\n","            total_upload = provider_data['Upload_Total_s'].mean()\n","            index_creation = provider_data['Upload_Index_Creation_s'].mean()\n","            upsert = provider_data['Upload_Upsert_s'].mean()\n","            print(f\"  {provider.upper():15}\")\n","            print(f\"    Index Creation: {index_creation:8.2f}s\")\n","            print(f\"    Upsert:         {upsert:8.2f}s\")\n","            print(f\"    Total:          {total_upload:8.2f}s (Index Creation + Upsert)\")\n","        elif provider == 'elasticsearch' and 'Upload_Total_s' in provider_data.columns: # Elasticsearch has a total upload key.\n","            total_upload = provider_data['Upload_Total_s'].mean()\n","            index_creation = provider_data['Upload_Index_Creation_s'].mean()\n","            bulk_client = provider_data['Upload_Bulk_Client_s'].mean()\n","            bulk_server = provider_data['Upload_Bulk_Server_s'].mean()\n","            print(f\"  {provider.upper():15}\")\n","            print(f\"    Index Creation:  {index_creation:8.8f}s\") # Increased precision for small times.\n","            print(f\"    Bulk (Server):   {bulk_server:8.8f}s\")\n","            print(f\"    Bulk (Client):   {bulk_client:8.8f}s\")\n","            print(f\"    Total:           {total_upload:8.8f}s (Index Creation + Client Bulk)\")\n","\n","    # Summarizes space efficiency for binary embeddings.\n","    print(\"\\nðŸ’¾ Space Efficiency (Binary Embeddings, Average across all runs):\")\n","    print(\"â”€\" * 70)\n","    if 'Float_Size_MB' in df.columns: # Checks if space efficiency metrics exist.\n","        avg_float = df['Float_Size_MB'].mean()\n","        avg_binary = df['Binary_Size_MB'].mean()\n","        avg_savings = df['Space_Savings_Pct'].mean()\n","        avg_compression = df['Compression_Ratio'].mean()\n","        print(f\"  Average Original Float Size (per dataset): {avg_float:.2f} MB\")\n","        print(f\"  Average Binarized Size (per dataset):      {avg_binary:.2f} MB\")\n","        print(f\"  Average Space Savings:                     {avg_savings:.1f}%\")\n","        print(f\"  Average Compression Ratio:                 {avg_compression:.1f}x\")\n","\n","    # Provides a timing breakdown for embedding generation and binarization.\n","    print(\"\\nâ±ï¸  Embedding Generation & Binarization Time (Average per dataset):\")\n","    print(\"â”€\" * 70)\n","    avg_embed_time = df['Embedding_Time_s'].mean()\n","    avg_query_time = df['Query_Embedding_Time_s'].mean()\n","    avg_binarize_time = df['Binarization_Time_s'].mean()\n","    total_prep_time = avg_embed_time + avg_query_time + avg_binarize_time\n","    print(f\"  Corpus embedding generation:     {avg_embed_time:.4f}s\")\n","    print(f\"  Query embedding generation:      {avg_query_time:.4f}s\")\n","    print(f\"  Binarization process:            {avg_binarize_time:.4f}s\")\n","    print(f\"  Total Embedding Prep Time:       {total_prep_time:.4f}s\")\n","\n","    # Details quality metrics for all K values tested.\n","    print(\"\\nðŸŽ¯ Detailed Quality Metrics (Average across datasets, all K values):\")\n","    print(\"â”€\" * 70)\n","    for provider in df['Provider'].unique():\n","        provider_data = df[df['Provider'] == provider]\n","        print(f\"\\n{provider.upper()}:\")\n","        for k in K_VALUES:\n","            ndcg_col = f'NDCG@{k}'\n","            map_col = f'MAP@{k}'\n","            recall_col = f'Recall@{k}'\n","            precision_col = f'P@{k}'\n","            if ndcg_col in df.columns: # Checks if metric column exists.\n","                print(f\"  @{k:3d} - NDCG: {provider_data[ndcg_col].mean():.4f} | \"\n","                      f\"MAP: {provider_data[map_col].mean():.4f} | \"\n","                      f\"Recall: {provider_data[recall_col].mean():.4f} | \"\n","                      f\"Precision: {provider_data[precision_col].mean():.4f}\")\n","\n","    # Provides a per-dataset breakdown for quick comparison of key metrics.\n","    print(\"\\nðŸ“‹ Per-Dataset Breakdown (Summary of key metrics for each run):\")\n","    print(\"â”€\" * 70)\n","    for dataset in df['Dataset'].unique():\n","        dataset_data = df[df['Dataset'] == dataset]\n","        source = dataset_data['Source'].iloc[0] if 'Source' in dataset_data.columns else 'UNKNOWN'\n","        print(f\"\\nDataset: {dataset.upper()} (Source: {source}):\")\n","        for provider in dataset_data['Provider'].unique():\n","            provider_dataset_data = dataset_data[dataset_data['Provider'] == provider]\n","            row = provider_dataset_data.iloc[0] # Assumes one entry per dataset-provider combination.\n","\n","            search_time_str = \"N/A\"\n","            if 'Search_Mean_ms' in row: # Prioritizes general search mean.\n","                search_time_str = f\"{row['Search_Mean_ms']:.2f}ms\"\n","            elif 'Search_Server_Mean_ms' in row: # Falls back to server search mean.\n","                search_time_str = f\"{row['Search_Server_Mean_ms']:.2f}ms (server)\"\n","            elif 'Search_Client_Mean_ms' in row: # Falls back to client search mean.\n","                 search_time_str = f\"{row['Search_Client_Mean_ms']:.2f}ms (client)\"\n","\n","            upload_time_str = \"N/A\"\n","            if 'Upload_Time_s' in row: # Moorcheh upload.\n","                upload_time_str = f\"{row['Upload_Time_s']:.2f}s\"\n","            elif 'Upload_Total_s' in row: # Pinecone/Elasticsearch total upload.\n","                upload_time_str = f\"{row['Upload_Total_s']:.2f}s\"\n","\n","            print(f\"  {provider.capitalize():15} | \"\n","                  f\"NDCG@10: {row['NDCG@10']:.4f} | \"\n","                  f\"Search Time: {search_time_str:15} | \"\n","                  f\"Upload Time: {upload_time_str}\")\n","\n","    # Identifies top performers for key metrics (e.g., best NDCG, fastest search).\n","    print(\"\\nðŸ† Performance Leaders (Across all runs for averaged metrics):\")\n","    print(\"â”€\" * 70)\n","\n","    # Best NDCG.\n","    if 'NDCG@10' in df.columns:\n","        best_ndcg_provider_avg = df.groupby('Provider')['NDCG@10'].mean().idxmax()\n","        best_ndcg_score_avg = df.groupby('Provider')['NDCG@10'].mean().max()\n","        print(f\"  Best Average NDCG@10: {best_ndcg_provider_avg.upper()} - {best_ndcg_score_avg:.4f}\")\n","        # Identifies the single best run for NDCG@10 (may involve a different provider/dataset).\n","        best_ndcg_idx = df['NDCG@10'].idxmax()\n","        best_ndcg_row = df.loc[best_ndcg_idx]\n","        print(f\"  Highest Single NDCG@10: {best_ndcg_row['Provider'].upper()} - {best_ndcg_row['NDCG@10']:.4f} on {best_ndcg_row['Dataset']} ({best_ndcg_row['Source']})\")\n","\n","    # Best MAP.\n","    if 'MAP@10' in df.columns:\n","        best_map_provider_avg = df.groupby('Provider')['MAP@10'].mean().idxmax()\n","        best_map_score_avg = df.groupby('Provider')['MAP@10'].mean().max()\n","        print(f\"  Best Average MAP@10: {best_map_provider_avg.upper()} - {best_map_score_avg:.4f}\")\n","        best_map_idx = df['MAP@10'].idxmax()\n","        best_map_row = df.loc[best_map_idx]\n","        print(f\"  Highest Single MAP@10: {best_map_row['Provider'].upper()} - {best_map_row['MAP@10']:.4f} on {best_map_row['Dataset']} ({best_map_row['Source']})\")\n","\n","    # Best Recall.\n","    if 'Recall@100' in df.columns:\n","        best_recall_provider_avg = df.groupby('Provider')['Recall@100'].mean().idxmax()\n","        best_recall_score_avg = df.groupby('Provider')['Recall@100'].mean().max()\n","        print(f\"  Best Average Recall@100: {best_recall_provider_avg.upper()} - {best_recall_score_avg:.4f}\")\n","        best_recall_idx = df['Recall@100'].idxmax()\n","        best_recall_row = df.loc[best_recall_idx]\n","        print(f\"  Highest Single Recall@100: {best_recall_row['Provider'].upper()} - {best_recall_row['Recall@100']:.4f} on {best_recall_row['Dataset']} ({best_recall_row['Source']})\")\n","\n","    # Fastest search (using the most relevant mean search time metric).\n","    if 'Search_Mean_ms' in df.columns: # Prioritizes general search mean.\n","        fastest_provider_avg = df.groupby('Provider')['Search_Mean_ms'].mean().idxmin()\n","        fastest_time_avg = df.groupby('Provider')['Search_Mean_ms'].mean().min()\n","        print(f\"  Fastest Average Search: {fastest_provider_avg.upper()} - {fastest_time_avg:.2f}ms\")\n","        fastest_idx = df['Search_Mean_ms'].idxmin()\n","        fastest_row = df.loc[fastest_idx]\n","        print(f\"  Fastest Single Search: {fastest_row['Provider'].upper()} - {fastest_row['Search_Mean_ms']:.2f}ms on {fastest_row['Dataset']} ({fastest_row['Source']})\")\n","    elif 'Search_Server_Mean_ms' in df.columns: # Falls back to server search mean for providers like ES/Moorcheh.\n","        fastest_provider_avg = df.groupby('Provider')['Search_Server_Mean_ms'].mean().idxmin()\n","        fastest_time_avg = df.groupby('Provider')['Search_Server_Mean_ms'].mean().min()\n","        print(f\"  Fastest Average Search (Server-side): {fastest_provider_avg.upper()} - {fastest_time_avg:.2f}ms\")\n","        fastest_idx = df['Search_Server_Mean_ms'].idxmin()\n","        fastest_row = df.loc[fastest_idx]\n","        print(f\"  Fastest Single Search (Server-side): {fastest_row['Provider'].upper()} - {fastest_row['Search_Server_Mean_ms']:.2f}ms on {fastest_row['Dataset']} ({fastest_row['Source']})\")\n","\n","    # Fastest upload (using the most relevant total upload time metric).\n","    if 'Upload_Time_s' in df.columns or 'Upload_Total_s' in df.columns: # Checks for either upload column.\n","        upload_col = 'Upload_Time_s' if 'Upload_Time_s' in df.columns else 'Upload_Total_s'\n","        fastest_upload_provider_avg = df.groupby('Provider')[upload_col].mean().idxmin()\n","        fastest_upload_time_avg = df.groupby('Provider')[upload_col].mean().min()\n","        print(f\"  Fastest Average Upload: {fastest_upload_provider_avg.upper()} - {fastest_upload_time_avg:.2f}s\")\n","        fastest_upload_idx = df[upload_col].idxmin()\n","        fastest_upload_row = df.loc[fastest_upload_idx]\n","        print(f\"  Fastest Single Upload: {fastest_upload_row['Provider'].upper()} - {fastest_upload_row[upload_col]:.2f}s on {fastest_upload_row['Dataset']} ({fastest_upload_row['Source']})\")\n","\n","    # Provides key insights and overall conclusions derived from the benchmark results.\n","    print(\"\\nðŸ’¡ Key Insights from the Benchmark:\")\n","    print(\"â”€\" * 70)\n","    print(f\"  â€¢ Dataset source(s) tested: {', '.join(df['Source'].unique())} (This includes both BEIR and MAIR data if selected)\")\n","    print(f\"  â€¢ Total unique datasets benchmarked: {len(df['Dataset'].unique())}\")\n","    print(f\"  â€¢ Total queries processed across all runs: {df['Num_Queries'].sum():,}\")\n","    print(f\"  â€¢ Total documents indexed across all successful runs: {df['Num_Uploaded'].sum():,}\")\n","    if 'Space_Savings_Pct' in df.columns:\n","        print(f\"  â€¢ Average space compression achieved by binarization (when stored as float32): {df['Space_Savings_Pct'].mean():.1f}%\")\n","    print(f\"  â€¢ Binarization method used: {df['Binarization_Method'].iloc[0]} (Sign-based is standard for this type of binary embedding)\")\n","    print(f\"  â€¢ All initial embeddings generated using Cohere's {df['Embedding_Model'].iloc[0]} model for consistency.\")\n","    print(f\"  â€¢ This benchmark provides valuable insights into the quality, speed, and space trade-offs when using binary embeddings with different vector databases.\")\n","    print(f\"  â€¢ Remember that actual performance can vary based on specific dataset characteristics, cloud region, network latency, and vector database configuration.\")\n","\n","    # Presents a more detailed provider comparison table if multiple providers were tested.\n","    if len(df['Provider'].unique()) > 1:\n","        print(\"\\nðŸ“Š Provider Comparison Table (Averages across all datasets for key metrics):\")\n","        print(\"â”€\" * 70)\n","\n","        providers = sorted(df['Provider'].unique())\n","        metrics_to_compare = ['NDCG@10', 'MAP@10', 'Recall@100', 'P@10']\n","\n","        # Constructs the header for the comparison table dynamically based on tested providers.\n","        header = f\"{'Metric':<20}\"\n","        for provider in providers:\n","            header += f\" {provider.capitalize():<15}\"\n","        print(header)\n","        print(\"â”€\" * (20 + 15 * len(providers)))\n","\n","        # Populates the metrics rows for the comparison table.\n","        for metric in metrics_to_compare:\n","            if metric in df.columns:\n","                row = f\"{metric:<20}\"\n","                for provider in providers:\n","                    provider_data = df[df['Provider'] == provider]\n","                    val = provider_data[metric].mean() # Average metric value for the provider.\n","                    row += f\" {val:<15.4f}\" # Formats to 4 decimal places.\n","                print(row)\n","\n","        # Adds a row for Search Time (using a consolidated mean search time).\n","        row = f\"{'Search Time (ms)':<20}\"\n","        for provider in providers:\n","            provider_data = df[df['Provider'] == provider]\n","            if 'Search_Mean_ms' in provider_data.columns: # Prioritizes general search mean.\n","                val = provider_data['Search_Mean_ms'].mean()\n","                row += f\" {val:<15.2f}\"\n","            elif 'Search_Server_Mean_ms' in provider_data.columns: # Fallback to server time.\n","                val = provider_data['Search_Server_Mean_ms'].mean()\n","                row += f\" {val:<15.2f}\"\n","            else:\n","                row += f\" {'N/A':<15}\" # Indicates if no search time is available.\n","        print(row)\n","\n","        # Adds a row for Upload Time (using a consolidated total upload time).\n","        row = f\"{'Upload Time (s)':<20}\"\n","        for provider in providers:\n","            provider_data = df[df['Provider'] == provider]\n","            if 'Upload_Time_s' in provider_data.columns: # Moorcheh upload key.\n","                val = provider_data['Upload_Time_s'].mean()\n","                row += f\" {val:<15.2f}\"\n","            elif 'Upload_Total_s' in provider_data.columns: # Pinecone/ES total upload key.\n","                val = provider_data['Upload_Total_s'].mean()\n","                row += f\" {val:<15.2f}\"\n","            else:\n","                row += f\" {'N/A':<15}\"\n","        print(row)\n","\n","    print(\"\\nâœ… Benchmark Conclusion and Key Takeaways:\")\n","    print(\"  â€¢ This benchmark provides a comparative analysis of retrieval quality and performance for various vector databases using binary embeddings.\")\n","    print(\"  â€¢ The results highlight the potential for significant space savings through binarization, along with the trade-offs in retrieval quality and search speed across different vector database implementations.\")\n","    print(f\"  â€¢ The complete, raw results in CSV format can be found at: {CSV_PATH}. This file contains all detailed metrics for further analysis.\")\n","    print(\"  â€¢ Consider your specific application needs (e.g., latency, storage costs, retrieval accuracy) when choosing a vector database and embedding strategy.\")\n","\n","else:\n","    print(\"\\nâš ï¸ No benchmark results were generated. Please ensure at least one dataset and one provider were successfully processed. Check for API key issues or errors during execution.\")"],"metadata":{"id":"qh8UZs36MKny"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vector (Binary) Search with PGVector and PostgreSQL in Google Colab"],"metadata":{"id":"8zeLDZi_MoCt"}},{"cell_type":"code","source":["# ============================================================\n","# MAIR Benchmark - PGVector Binary Edition (Colab Optimized)\n","# This notebook section focuses on benchmarking binary embeddings (1-bit)\n","# using PostgreSQL with the pgvector extension for vector search.\n","# ============================================================\n","\n","# -------------------- STEP 1: Install PostgreSQL & pgvector --------------------\n","print(\"ðŸ”§ Installing PostgreSQL and pgvector in Google Colab environment...\")\n","print(\"This process involves installing system packages and compiling pgvector from source. It will take approximately 2-3 minutes on the first run.\")\n","print(\"Subsequent runs in the same Colab session will be faster as packages might be cached.\")\n","\n","import os\n","import subprocess\n","import time\n","\n","# Install PostgreSQL server and client utilities.\n","# `apt-get update -qq`: Updates package lists quietly.\n","# `apt-get install -y postgresql postgresql-contrib`: Installs PostgreSQL server and extensions.\n","print(\"ðŸ“¦ Installing PostgreSQL server...\")\n","os.system('apt-get update -qq > /dev/null 2>&1')\n","os.system('apt-get install -y postgresql postgresql-contrib > /dev/null 2>&1')\n","\n","# Install build dependencies for pgvector (needed to compile from source).\n","# `build-essential`: Provides compilers (gcc, g++).\n","# `git`: To clone the pgvector repository.\n","# `postgresql-server-dev-14`: Development headers for PostgreSQL 14 (adjust version if needed).\n","print(\"ðŸ“¦ Installing build tools and PostgreSQL development headers for pgvector compilation...\")\n","os.system('apt-get install -y build-essential git postgresql-server-dev-14 > /dev/null 2>&1')\n","\n","# Install pgvector extension by cloning its repository and compiling from source.\n","# This ensures we get the latest features, including BIT type support for binary vectors.\n","print(\"ðŸ“¦ Cloning and installing pgvector extension from GitHub...\")\n","os.system('cd /tmp && rm -rf pgvector && git clone --quiet https://github.com/pgvector/pgvector.git')\n","os.system('cd /tmp/pgvector && make > /dev/null 2>&1 && make install > /dev/null 2>&1')\n","\n","# Start PostgreSQL service.\n","print(\"ðŸš€ Starting PostgreSQL service...\")\n","os.system('service postgresql start > /dev/null 2>&1')\n","time.sleep(2) # Give the service a moment to fully start up.\n","\n","# Configure the PostgreSQL database.\n","# `sudo -u postgres`: Executes commands as the 'postgres' user, which has administrative privileges.\n","# `DROP DATABASE IF EXISTS vectordb`: Ensures a clean slate for the 'vectordb' database.\n","# `CREATE DATABASE vectordb`: Creates a new database named 'vectordb' for our vector data.\n","# `ALTER USER postgres PASSWORD 'postgres'`: Sets a simple password for the default 'postgres' user.\n","# `CREATE EXTENSION IF NOT EXISTS vector`: Enables the pgvector extension within the 'vectordb'.\n","print(\"ðŸ”§ Configuring 'vectordb' database and enabling pgvector extension...\")\n","os.system('sudo -u postgres psql -c \"DROP DATABASE IF EXISTS vectordb;\" > /dev/null 2>&1')\n","os.system('sudo -u postgres psql -c \"CREATE DATABASE vectordb;\" > /dev/null 2>&1')\n","os.system('sudo -u postgres psql -c \"ALTER USER postgres PASSWORD \\'postgres\\';\" > /dev/null 2>&1')\n","os.system('sudo -u postgres psql -d vectordb -c \"CREATE EXTENSION IF NOT EXISTS vector;\" > /dev/null 2>&1')\n","\n","print(\"âœ… PostgreSQL with pgvector is ready for use!\n","\")\n","\n","# -------------------- STEP 2: Install Python Dependencies --------------------\n","# Install Python packages required for connecting to PostgreSQL, generating embeddings, and data analysis.\n","# `psycopg2-binary`: PostgreSQL adapter for Python.\n","# `pgvector`: Python client for pgvector.\n","# `cohere`: For generating embeddings.\n","# `pandas`, `numpy`, `tqdm`: Standard data science libraries.\n","print(\"ðŸ“¦ Installing Python packages: psycopg2-binary, pgvector, cohere, pandas, numpy, tqdm...\")\n","os.system('pip install -q psycopg2-binary pgvector cohere pandas numpy tqdm')\n","print(\"âœ… Python packages installed!\n","\")\n","\n","# -------------------- STEP 3: Import Libraries --------------------\n","# Import all necessary Python libraries for the benchmark script.\n","import gc # For garbage collection to manage memory.\n","import json # For handling JSONL dataset files.\n","import statistics # For calculating performance statistics.\n","import numpy as np # For numerical operations, especially with embeddings.\n","import pandas as pd # For data manipulation and CSV output.\n","from tqdm import tqdm # For displaying progress bars during long operations.\n","import psycopg2 # The Python PostgreSQL adapter.\n","import psycopg2.extras # For advanced psycopg2 features like execute_values.\n","from pgvector.psycopg2 import register_vector # Utility to register vector type with psycopg2.\n","\n","# -------------------- Environment Setup --------------------\n","# DRIVE_PATH: This variable determines where your benchmark results will be saved.\n","# MAIR_COMBINED_PATH: Path to the combined MAIR datasets in your Google Drive.\n","DRIVE_PATH = \".\"\n","MAIR_COMBINED_PATH = \"/content/drive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Combined\"\n","\n","try:\n","    # Attempt to import Colab specific modules to check if running in Google Colab.\n","    from google.colab import drive, userdata as colab_userdata\n","\n","    try:\n","        # Mount Google Drive to allow Colab to access your files.\n","        drive.mount('/content/drive', force_remount=True)\n","        print(\"âœ… Google Drive mounted successfully.\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Drive mount warning: {e}. Continuing without Drive mount, results will be saved locally.\")\n","\n","    # Set the DRIVE_PATH for results within Google Drive.\n","    # You can customize this path to organize your benchmark results.\n","    DRIVE_PATH = '/content/drive/MyDrive/Moorcheh/Benchmark_Results/MAIR.PGVector.Binary'\n","    os.makedirs(DRIVE_PATH, exist_ok=True) # Ensure the results directory exists.\n","    print(f\"âœ… Running in Colab. Benchmark results will be saved to: {DRIVE_PATH}\")\n","\n","    # Retrieve COHERE_API_KEY from Colab secrets.\n","    # To use this, you need to add your API key to Colab's \"Secrets\" feature (look for the key icon ðŸ”‘ on the left sidebar).\n","    # Give it the name `COHERE_API_KEY`.\n","    COHERE_API_KEY = colab_userdata.get('COHERE_API_KEY')\n","\n","except ImportError:\n","    # This block runs if not in Google Colab (e.g., local environment).\n","    DRIVE_PATH = \".\"\n","    print(\"âš ï¸ Not running in Google Colab. Results will be saved locally. Ensure COHERE_API_KEY is set as an environment variable.\")\n","    # Retrieve COHERE_API_KEY from environment variables.\n","    COHERE_API_KEY = os.environ.get('COHERE_API_KEY')\n","\n","# PostgreSQL connection parameters.\n","# These define how the script connects to the local PostgreSQL instance.\n","# 'host', 'port', 'database', 'user', 'password' are set based on the setup in Step 1.\n","PG_CONN_PARAMS = {\n","    'host': 'localhost',\n","    'port': '5432',\n","    'database': 'vectordb',\n","    'user': 'postgres',\n","    'password': 'postgres'\n","}\n","\n","# Configuration parameters. These can be adjusted by the user.\n","# TOP_K_SEARCH: The number of top results to retrieve from pgvector for each query.\n","TOP_K_SEARCH = 100\n","# K_VALUES: A list of 'k' values for which retrieval metrics (NDCG, MAP, Recall, Precision) will be calculated.\n","K_VALUES = [1, 3, 5, 10, 100]\n","# MAX_UPLOAD_DOCS: Limits the number of documents uploaded to pgvector.\n","# Useful for testing with very large datasets to manage execution time.\n","MAX_UPLOAD_DOCS = 700000\n","# BATCH_SIZE: Number of embeddings to process or upload in a single request/batch.\n","# Adjusting this can impact performance and memory usage.\n","BATCH_SIZE = 100\n","# EMBEDDING_MODEL: The Cohere model used to generate embeddings.\n","EMBEDDING_MODEL = \"embed-v4.0\"\n","# INPUT_TYPE_CORPUS: Input type for corpus documents when generating Cohere embeddings.\n","INPUT_TYPE_CORPUS = \"search_document\"\n","# INPUT_TYPE_QUERY: Input type for queries when generating Cohere embeddings.\n","INPUT_TYPE_QUERY = \"search_query\"\n","# VECTOR_DIMENSION: The dimensionality of the embeddings generated by Cohere.\n","# This defines the length of the `BIT` string in PostgreSQL.\n","VECTOR_DIMENSION = 1536\n","# CSV_PATH: The path where the final benchmark results will be saved in CSV format.\n","CSV_PATH = os.path.join(DRIVE_PATH, \"MAIR.PGVector.Binary.Cohere.V4.csv\")\n","\n","# Cleanup Policy for PostgreSQL tables.\n","# Users can choose how the benchmark handles the created pgvector tables after each dataset run.\n","print(\"\\nðŸ§¹ PGVector table cleanup policy options:\")\n","print(\"  1) ask_each_time  -> Prompt after each dataset (default behaviour)\")\n","print(\"  2) always_delete  -> Automatically delete tables after benchmarking each dataset\")\n","print(\"  3) always_keep    -> Never delete tables (keep for future searches or debugging)\")\n","\n","cleanup_choice = input(\"âž¡ï¸ Choose a cleanup policy [1/2/3] (default is 1): \").strip() or \"1\"\n","CLEANUP_POLICY_MAP = {\"1\": \"ask_each_time\", \"2\": \"always_delete\", \"3\": \"always_keep\"}\n","CLEANUP_POLICY = CLEANUP_POLICY_MAP.get(cleanup_choice, \"ask_each_time\")\n","print(f\"âœ… Selected cleanup policy: {CLEANUP_POLICY}\")\n","\n","# -------------------- Helper Functions --------------------\n","# These functions facilitate data loading, processing, and metric calculation.\n","\n","def load_jsonl(filepath):\n","    \"\"\"Load JSONL file into a dictionary, handling various ID keys (e.g., _id, id, query_id, doc_id).\"\"\"\n","    data = {}\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip(): # Skip empty lines\n","                    item = json.loads(line)\n","                    item_id = item.get('_id') or item.get('id') or item.get('query_id') or item.get('doc_id')\n","                    if item_id:\n","                        data[str(item_id)] = item\n","                    else:\n","                        data[str(len(data))] = item # Fallback if no ID is found\n","        print(f\"   Loaded {len(data)} items from {os.path.basename(filepath)}\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Error loading {filepath}: {e}\")\n","    return data\n","\n","def extract_qrels_from_queries(queries):\n","    \"\"\"Extract qrels (relevance judgments) from the 'labels' field in queries, supporting various formats.\"\"\"\n","    qrels = {}\n","    for qid, q in queries.items():\n","        if isinstance(q, dict):\n","            labels = q.get('labels') or q.get('relevance') or q.get('qrels') # Check common qrels field names\n","            if labels:\n","                qrels[str(qid)] = {} # Initialize qrels for the current query\n","                if isinstance(labels, list):\n","                    for label_item in labels:\n","                        if isinstance(label_item, dict):\n","                            doc_id = label_item.get('id') or label_item.get('doc_id')\n","                            score = label_item.get('score', 1) # Default score to 1 if not provided\n","                            if doc_id:\n","                                qrels[str(qid)][str(doc_id)] = score\n","                        else:\n","                            qrels[str(qid)][str(label_item)] = 1 # Assume score of 1 if just a list of doc_ids\n","                elif isinstance(labels, dict):\n","                    for doc_id, score in labels.items():\n","                        qrels[str(qid)][str(doc_id)] = score\n","    return qrels\n","\n","def binarize_embeddings(embeddings):\n","    \"\"\"\n","    Binarize float embeddings using a sign-based method (>=0 -> 1, <0 -> 0).\n","    This function specifically converts the resulting binary array into a list of strings\n","    (e.g., '10100...') suitable for PostgreSQL's `BIT` data type in pgvector.\n","    Returns: A tuple containing (list of binary strings, original float embeddings size in MB).\n","    \"\"\"\n","    float_embeddings = np.array(embeddings, dtype=np.float32) # Convert input to numpy array of float32\n","    # Calculate size of original float embeddings for space efficiency comparison.\n","    float_size_mb = (float_embeddings.nbytes) / (1024*1024)\n","\n","    # Convert to boolean array (True for >=0, False for <0).\n","    binary_bool = (float_embeddings >= 0)\n","\n","    # Efficiently convert each row (vector) of the boolean array into a string of '1's and '0's.\n","    # This format is required for the `BIT` column type in PostgreSQL.\n","    binary_strings = []\n","    for row in binary_bool:\n","        binary_strings.append(\"\".join(row.astype(int).astype(str))) # Convert bool to int (0/1), then to string, then join.\n","\n","    return binary_strings, float_size_mb\n","\n","# Predefined categories for MAIR datasets for structured display.\n","DATASET_CATEGORIES = {\n","    \"Legal & Regulatory\": [\"ACORDAR\", \"AILA2019-Case\", \"AILA2019-Statutes\", \"CUAD\", \"LeCaRDv2\", \"LegalQuAD\", \"REGIR-EU2UK\", \"REGIR-UK2EU\"],\n","    \"Medical & Clinical\": [\"CliniDS-2014\", \"CliniDS-2015\", \"CliniDS-2016\", \"ClinicalTrials-2021\", \"ClinicalTrials-2022\", \"ClinicalTrials-2023\", \"NFCorpus\", \"PrecisionMedicine\", \"Genomics-AdHoc\"],\n","    \"Code & Programming\": [\"APPS\", \"CodeEditSearch\", \"CodeSearchNet\", \"Conala\", \"HumanEval-X\", \"LeetCode\", \"MBPP\", \"RepoBench\", \"SWE-Bench-Lite\"],\n","    \"Financial\": [\"ConvFinQA\", \"FiQA\", \"FinQA\", \"FinanceBench\", \"HC3Finance\"],\n","    \"Academic & Scientific\": [\"ArguAna\", \"LitSearch\", \"ProofWiki-Proof\", \"ProofWiki-Reference\", \"Competition-Math\"],\n","    \"Conversational & Dialog\": [\"CAsT-2019\", \"CAsT-2020\", \"CAsT-2021\", \"CAsT-2022\", \"ProCIS-Dialog\", \"ProCIS-Turn\", \"SParC\", \"Quora\"],\n","    \"News & Social Media\": [\"ChroniclingAmericaQA\", \"Microblog-2011\", \"Microblog-2012\", \"Microblog-2013\", \"Microblog-2014\", \"News21\"],\n","    \"API Documentation\": [\"Apple\", \"FoodAPI\", \"HuggingfaceAPI\", \"PytorchAPI\"],\n","    \"Others\": [\"BSARD\", \"BillSum\", \"CARE\", \"CPCD\", \"CQADupStack\", \"DD\", \"ELI5\", \"ExcluIR\", \"FairRanking\", \"Fever\", \"GerDaLIR\", \"IFEval\", \"InstructIR\", \"MISeD\", \"Monant\", \"NTCIR\", \"NeuCLIR\", \"NevIR\", \"PointRec\", \"ProductSearch_2023\", \"QuanTemp\", \"Robust04\"]\n","}\n","\n","def get_dataset_category(dataset_name):\n","    \"\"\"Assigns a given dataset name to its predefined category.\"\"\"\n","    for category, datasets in DATASET_CATEGORIES.items():\n","        if dataset_name in datasets:\n","            return category\n","    return \"Others\" # Default category if not found in specific lists\n","\n","def get_dataset_size(dataset_name):\n","    \"\"\"Counts the number of documents in a MAIR dataset by reading its JSONL files.\"\"\"\n","    dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","    docs_path = os.path.join(dataset_path, 'docs')\n","    if not os.path.exists(docs_path):\n","        return 0\n","    doc_count = 0\n","    try:\n","        docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","        for file in docs_files:\n","            file_path = os.path.join(docs_path, file)\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                for line in f: # Iterate through lines to count documents\n","                    if line.strip(): # Count non-empty lines as documents\n","                        doc_count += 1\n","    except Exception as e:\n","        print(f\"âš ï¸ Error counting documents in {dataset_name}: {e}\")\n","    return doc_count\n","\n","def format_size(num):\n","    \"\"\"Formats a number (document count) with K (thousands) or M (millions) suffix for readability.\"\"\"\n","    if num >= 1_000_000:\n","        return f\"{num/1_000_000:.1f}M\"\n","    elif num >= 1_000:\n","        return f\"{num/1_000:.1f}K\"\n","    return str(num)\n","\n","def get_mair_datasets():\n","    \"\"\"Discovers available MAIR datasets in the combined path, categorizes them, and gets their sizes.\"\"\"\n","    datasets_by_category = {category: [] for category in DATASET_CATEGORIES.keys()}\n","    all_datasets = []\n","    dataset_sizes = {}\n","    if os.path.exists(MAIR_COMBINED_PATH): # Check if the combined MAIR path exists\n","        print(f\"ðŸ” Scanning for MAIR datasets in: {MAIR_COMBINED_PATH}\")\n","        try:\n","            items = os.listdir(MAIR_COMBINED_PATH)\n","            for item in sorted(items): # Iterate through directories, sorted alphabetically\n","                if item.startswith('.') or item.startswith('_'): # Skip hidden/system files\n","                    continue\n","                dataset_path = os.path.join(MAIR_COMBINED_PATH, item)\n","                if not os.path.isdir(dataset_path): # Ensure it's a directory\n","                    continue\n","                docs_path = os.path.join(dataset_path, 'docs')\n","                queries_path = os.path.join(dataset_path, 'queries')\n","                # Verify that both 'docs' and 'queries' subdirectories exist and contain .jsonl files\n","                if os.path.exists(docs_path) and os.path.exists(queries_path):\n","                    docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","                    queries_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","                    if docs_files and queries_files:\n","                        all_datasets.append(item)\n","                        category = get_dataset_category(item) # Get assigned category\n","                        datasets_by_category[category].append(item)\n","                        doc_count = get_dataset_size(item) # Get document count\n","                        dataset_sizes[item] = doc_count\n","                        print(f\"   âœ… Found dataset: {item} [Category: {category}] - {format_size(doc_count)} docs\")\n","        except Exception as e:\n","            print(f\"   âŒ Error scanning directory {MAIR_COMBINED_PATH}: {e}\")\n","    return all_datasets, datasets_by_category, dataset_sizes\n","\n","def calculate_timing_stats(timing_list):\n","    \"\"\"Calculates basic statistics (mean, median, min, max, std, total) for a list of numerical timings.\"\"\"\n","    if not timing_list:\n","        return {\"mean\": 0.0, \"median\": 0.0, \"min\": 0.0, \"max\": 0.0, \"std\": 0.0, \"total\": 0.0}\n","    return {\n","        \"mean\": statistics.mean(timing_list),\n","        \"median\": statistics.median(timing_list),\n","        \"min\": min(timing_list),\n","        \"max\": max(timing_list),\n","        \"std\": statistics.stdev(timing_list) if len(timing_list) > 1 else 0.0, # Standard deviation requires at least two samples\n","        \"total\": sum(timing_list)\n","    }\n","\n","def calculate_retrieval_metrics(retrieved_results, qrels, k_values=K_VALUES):\n","    \"\"\"Calculates NDCG, MAP, Recall, and Precision @K for given retrieval results and ground truth qrels.\"\"\"\n","    ndcg_scores = {f\"NDCG@{k}\": [] for k in k_values}\n","    map_scores = {f\"MAP@{k}\": [] for k in k_values}\n","    recall_scores = {f\"Recall@{k}\": [] for k in k_values}\n","    precision_scores = {f\"P@{k}\": [] for k in k_values}\n","\n","    for qid in retrieved_results:\n","        if qid not in qrels: # Skip if no qrels for this query\n","            continue\n","        relevant_docs = set(qrels[qid].keys()) # Set of relevant document IDs for this query\n","        retrieved_docs = list(retrieved_results[qid].keys()) # Ordered list of retrieved document IDs\n","        if len(relevant_docs) == 0: # Cannot calculate metrics if no relevant docs exist\n","            continue\n","\n","        for k in k_values:\n","            top_k = retrieved_docs[:k] # Consider only top K retrieved documents\n","            hits = len(set(top_k) & relevant_docs) # Count relevant documents in top K\n","\n","            # Recall@k: Proportion of relevant documents found in the top K.\n","            recall = hits / len(relevant_docs) if len(relevant_docs) > 0 else 0.0\n","            recall_scores[f\"Recall@{k}\"].append(recall)\n","\n","            # Precision@k: Proportion of retrieved documents that are relevant in the top K.\n","            precision = hits / k if k > 0 else 0.0\n","            precision_scores[f\"P@{k}\"].append(precision)\n","\n","            # Average Precision (AP@k): Sum of (Precision@i * relevance_at_i) / num_relevant.\n","            ap = 0.0\n","            hits_so_far = 0\n","            for i, doc_id in enumerate(top_k, 1):\n","                if doc_id in relevant_docs:\n","                    hits_so_far += 1\n","                    ap += hits_so_far / i\n","            # Normalize AP by the minimum of actual relevant docs or k. Avoid division by zero.\n","            ap = ap / min(len(relevant_docs), k) if len(relevant_docs) > 0 else 0.0\n","            map_scores[f\"MAP@{k}\"].append(ap)\n","\n","            # Normalized Discounted Cumulative Gain (NDCG@k): Measures ranking quality.\n","            dcg = 0.0\n","            for i, doc_id in enumerate(top_k, 1):\n","                if doc_id in relevant_docs:\n","                    dcg += 1.0 / np.log2(i + 1) # Logarithmic discount for lower-ranked documents\n","\n","            # Ideal DCG (IDCG): DCG of a perfect ranking.\n","            # Assumes all relevant docs have score 1.\n","            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), k))) # +2 because index starts at 0, rank at 1\n","            ndcg = dcg / idcg if idcg > 0 else 0.0\n","            ndcg_scores[f\"NDCG@{k}\"].append(ndcg)\n","\n","    # Calculate average metrics across all queries.\n","    ndcg_avg = {k: np.mean(v) if v else 0.0 for k, v in ndcg_scores.items()}\n","    map_avg = {k: np.mean(v) if v else 0.0 for k, v in map_scores.items()}\n","    recall_avg = {k: np.mean(v) if v else 0.0 for k, v in recall_scores.items()}\n","    precision_avg = {k: np.mean(v) if v else 0.0 for k, v in precision_scores.items()}\n","    return ndcg_avg, map_avg, recall_avg, precision_avg\n","\n","def format_and_print_metrics(ndcg, _map, recall, precision, k_values=K_VALUES):\n","    \"\"\"Prints retrieval metrics (NDCG, MAP, Recall, Precision) in a formatted, readable table.\"\"\"\n","    print(\"\\nRetrieval Metrics:\")\n","    print(\"â”€\" * 80)\n","    for k in k_values:\n","        print(f\"NDCG@{k}: {ndcg.get(f'NDCG@{k}', 0.0):.4f} | \"\n","              f\"MAP@{k}: {_map.get(f'MAP@{k}', 0.0):.4f} | \"\n","              f\"Recall@{k}: {recall.get(f'Recall@{k}', 0.0):.4f} | \"\n","              f\"P@{k}: {precision.get(f'P@{k}', 0.0):.4f}\")\n","\n","def extract_all_metrics(ndcg, _map, recall, precision, k_values=K_VALUES):\n","    \"\"\"Extracts all calculated retrieval metrics into a single dictionary for consistent storage (e.g., in CSV).\"\"\"\n","    metrics = {}\n","    for k in k_values:\n","        metrics[f\"NDCG@{k}\"] = float(ndcg.get(f\"NDCG@{k}\", 0.0))\n","        metrics[f\"MAP@{k}\"] = float(_map.get(f\"MAP@{k}\", 0.0))\n","        metrics[f\"Recall@{k}\"] = float(recall.get(f\"Recall@{k}\", 0.0))\n","        metrics[f\"P@{k}\"] = float(precision.get(f\"P@{k}\", 0.0))\n","    return metrics\n","\n","def save_results_to_csv(new_result: dict, csv_path: str):\n","    \"\"\"Appends a new benchmark result entry to a CSV file. Creates the file and header if it doesn't exist.\"\"\"\n","    new_df = pd.DataFrame([new_result]) # Convert the new result dictionary into a Pandas DataFrame row\n","    os.makedirs(os.path.dirname(csv_path), exist_ok=True) # Ensure the directory for the CSV file exists\n","    write_header = not os.path.exists(csv_path) # Check if the CSV file already exists to decide if a header is needed\n","    try:\n","        if write_header:\n","            new_df.to_csv(csv_path, mode='w', header=True, index=False) # Write to a new file with header\n","        else:\n","            existing_df = pd.read_csv(csv_path) # Read existing data\n","            combined_df = pd.concat([existing_df, new_df], ignore_index=True) # Concatenate new data\n","            combined_df.to_csv(csv_path, mode='w', header=True, index=False) # Overwrite with combined data (maintaining header)\n","        print(f\"ðŸ’¾ Results saved to: {csv_path}\")\n","    except Exception as e:\n","        print(f\"âŒ CSV save failed: {e}. Please check file permissions or path validity.\")\n","\n","def clean_memory():\n","    \"\"\"Forces Python's garbage collector to free up unreferenced objects, crucial in Colab to prevent OOM errors.\"\"\"\n","    gc.collect()\n","\n","def should_cleanup_table(table_name, dataset_name):\n","    \"\"\"Determines whether to delete the PGVector table based on the user-selected cleanup policy.\"\"\"\n","    if CLEANUP_POLICY == \"always_delete\":\n","        return True # Automatically delete without asking\n","    elif CLEANUP_POLICY == \"always_keep\":\n","        return False # Never delete, keep the table\n","    else: # CLEANUP_POLICY == \"ask_each_time\"\n","        response = input(f\"\\nâ“ Delete PGVector table '{table_name}' for dataset '{dataset_name}'? (y/n): \").strip().lower()\n","        return response in ['y', 'yes'] # Ask user for confirmation\n","\n","# -------------------- PGVector Provider Class (BINARY) --------------------\n","\n","class PGVectorBinaryProvider:\n","    \"\"\"Manages interaction with PostgreSQL and pgvector for binary embedding benchmarking.\"\"\"\n","    def __init__(self, conn_params, table_name, precomputed_vectors, query_embeddings, vector_dim=1536):\n","        self.conn_params = conn_params # Dictionary of PostgreSQL connection parameters\n","        self.table_name = table_name # Name of the table to create/use in PostgreSQL\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and binary vector strings\n","        self.query_embeddings = query_embeddings # List of query binary vector strings\n","        self.vector_dim = vector_dim # Dimension of the vectors (length of the BIT string)\n","        self.conn = None # Placeholder for the database connection object\n","        self.upload_timings = {\n","            \"table_creation_s\": 0.0, # Time to create the database table\n","            \"insert_time_s\": 0.0, # Total time for inserting vectors\n","            \"index_creation_s\": 0.0, # Time to create the HNSW index\n","            \"batch_details\": [] # Detailed timings for each insert batch\n","        }\n","        self.search_timings = [] # Timings for individual search queries\n","\n","    def connect(self):\n","        \"\"\"Establishes a connection to the PostgreSQL database using the provided parameters.\"\"\"\n","        try:\n","            self.conn = psycopg2.connect(**self.conn_params) # Connect to DB\n","            register_vector(self.conn) # Register the pgvector extension with the psycopg2 connection\n","            print(f\"âœ… Connected to PostgreSQL database: {self.conn_params['database']}\")\n","        except Exception as e:\n","            print(f\"âŒ PostgreSQL connection failed: {e}. Please check PG_CONN_PARAMS and server status.\")\n","            raise # Re-raise to stop execution if connection fails\n","\n","    def upload(self):\n","        \"\"\"Uploads binary vectors to the PGVector table using the `BIT` data type.\"\"\"\n","        if not self.conn: # Ensure connection is established\n","            self.connect()\n","\n","        cur = self.conn.cursor() # Create a cursor object for executing SQL commands\n","\n","        # Drop table if it exists to ensure a clean benchmark run.\n","        try:\n","            cur.execute(f\"DROP TABLE IF EXISTS {self.table_name}\")\n","            self.conn.commit() # Commit the transaction\n","            print(f\"ðŸ—‘ï¸ Dropped existing table: {self.table_name} (if it existed).\")\n","        except Exception as e:\n","            self.conn.rollback() # Rollback on error\n","            print(f\"âš ï¸ Could not drop table {self.table_name}: {e}\")\n","\n","        # Create table with BIT type instead of regular vector type for binary embeddings.\n","        # `bit(N)` specifies a fixed-length bit string of N bits.\n","        t0 = time.time() # Start timer for table creation\n","        try:\n","            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\") # Ensure pgvector extension is enabled\n","            cur.execute(f\"\"\"\n","                CREATE TABLE {self.table_name} (\n","                    id TEXT PRIMARY KEY,                       -- Document ID\n","                    embedding bit({self.vector_dim})           -- Binary embedding stored as a bit string\n","                )\n","            \"\"\")\n","            self.conn.commit() # Commit the table creation\n","            t1 = time.time() # End timer\n","            self.upload_timings[\"table_creation_s\"] = t1 - t0\n","            print(f\"âœ… Created table: {self.table_name} with column 'embedding' of type bit({self.vector_dim})\")\n","        except Exception as e:\n","            print(f\"âŒ Table creation failed: {e}. Check PostgreSQL logs for details.\")\n","            self.conn.rollback() # Rollback on error\n","            raise\n","\n","        # Insert vectors in batches using `execute_values` for efficiency.\n","        batch_num = 0\n","        total_inserted = 0\n","        t_insert_start = time.time() # Start timer for total insert time\n","\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading Binary Vectors to PGVector\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE] # Get a batch of vectors\n","            batch_num += 1\n","\n","            t_batch_start = time.time() # Start timer for current batch\n","            try:\n","                # Prepare data for `execute_values`: list of tuples (id, vector_string).\n","                values = [(v['id'], v['vector']) for v in batch]\n","                # `psycopg2.extras.execute_values` is much faster than individual INSERTs.\n","                # `template=\"(%s, %s::bit varying)\"` specifies how the values are formatted for the SQL query.\n","                # `::bit varying` casts the string to a bit string type, ensuring correct interpretation.\n","                psycopg2.extras.execute_values(\n","                    cur,\n","                    f\"INSERT INTO {self.table_name} (id, embedding) VALUES %s\",\n","                    values,\n","                    template=\"(%s, %s::bit varying)\"\n","                )\n","                self.conn.commit() # Commit the batch insert\n","\n","                t_batch_end = time.time() # End timer for current batch\n","                batch_time = t_batch_end - t_batch_start\n","                total_inserted += len(batch)\n","\n","                self.upload_timings[\"batch_details\"].append({\n","                    \"batch_num\": batch_num,\n","                    \"batch_size\": len(batch),\n","                    \"insert_time_s\": batch_time\n","                })\n","\n","            except Exception as e:\n","                print(f\"\\nâŒ Batch {batch_num} failed to insert to PGVector: {e}\")\n","                self.conn.rollback() # Rollback the current batch on error\n","\n","        t_insert_end = time.time() # End timer for total insert time\n","        self.upload_timings[\"insert_time_s\"] = t_insert_end - t_insert_start\n","\n","        # Create HNSW index using Hamming distance (`bit_hamming_ops`).\n","        # HNSW (Hierarchical Navigable Small World) is a popular approximate nearest neighbor (ANN) algorithm.\n","        # `bit_hamming_ops` tells pgvector to use Hamming distance for this index type.\n","        # `m` and `ef_construction` are HNSW parameters influencing index quality and build time.\n","        print(f\"\\nðŸ”§ Creating HNSW index on 'embedding' column using Hamming distance ('bit_hamming_ops')...\")\n","        t_index_start = time.time() # Start timer for index creation\n","        try:\n","            cur.execute(f\"\"\"\n","                CREATE INDEX ON {self.table_name}\n","                USING hnsw (embedding bit_hamming_ops)\n","                WITH (m = 16, ef_construction = 64)\n","            \"\"\") # SQL command to create HNSW index\n","            self.conn.commit() # Commit the index creation\n","            t_index_end = time.time() # End timer\n","            self.upload_timings[\"index_creation_s\"] = t_index_end - t_index_start\n","            print(f\"âœ… HNSW (Hamming) index created in {self.upload_timings['index_creation_s']:.4f}s\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Index creation failed: {e}. Indexing is important for search performance. Proceeding without index.\")\n","            self.conn.rollback() # Rollback on error\n","\n","        cur.close() # Close the cursor\n","\n","        print(f\"\\nâ±ï¸  PGVector Upload Timing Summary:\")\n","        print(f\"    Table Creation: {self.upload_timings['table_creation_s']:.4f}s\")\n","        print(f\"    Total Data Insert: {self.upload_timings['insert_time_s']:.4f}s\")\n","        print(f\"    Index Creation: {self.upload_timings['index_creation_s']:.4f}s\")\n","        print(f\"    Total Upload Process Time: {sum([self.upload_timings['table_creation_s'], self.upload_timings['insert_time_s'], self.upload_timings['index_creation_s']]):.4f}s\")\n","\n","        return total_inserted # Return count of successfully inserted vectors\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a vector search for similar vectors using Hamming distance in pgvector.\"\"\"\n","        if not self.conn: # Ensure connection is established\n","            self.connect()\n","\n","        query_embedding_str = self.query_embeddings[query_idx] # Get the binary query vector string\n","        cur = self.conn.cursor() # Create a cursor\n","\n","        t_start = time.time() # Start timer for search query\n","        try:\n","            # Use `<~>` operator for Hamming distance comparison in pgvector.\n","            # The query string `bit varying` casts the query string to a bit string for comparison.\n","            # We select `id` and the Hamming distance as `hamming_dist`.\n","            # Results are ordered by `hamming_dist` (lower distance = more similar).\n","            cur.execute(f\"\"\"\n","                SELECT id, (embedding <~> %s) as hamming_dist\n","                FROM {self.table_name}\n","                ORDER BY embedding <~> %s\n","                LIMIT %s\n","            \"\"\", (query_embedding_str, query_embedding_str, top_k))\n","\n","            results = cur.fetchall() # Fetch all results\n","            t_end = time.time() # End timer\n","\n","            search_time = t_end - t_start\n","            self.search_timings.append({\"search_time_s\": search_time}) # Record search time\n","\n","            # Convert Hamming distance to a similarity score [0, 1] for evaluation compatibility.\n","            # A common way is: Score = 1 - (HammingDistance / VectorDimension).\n","            formatted_results = {}\n","            for row in results:\n","                doc_id = str(row[0])\n","                dist = float(row[1])\n","                score = 1.0 - (dist / self.vector_dim) # Calculate similarity score\n","                formatted_results[doc_id] = score\n","\n","            return formatted_results\n","\n","        except Exception as e:\n","            print(f\"\\nâŒ PGVector search error for query {query_idx}: {e}\")\n","            self.search_timings.append({\"search_time_s\": 0.0}) # Log 0 time for failed searches\n","            return {} # Return empty results on error\n","        finally:\n","            cur.close() # Always close the cursor\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for PGVector.\"\"\"\n","        search_times = [t[\"search_time_s\"] for t in self.search_timings] # Extract all search times\n","        return calculate_timing_stats(search_times) # Use helper to calculate stats\n","\n","    def cleanup(self):\n","        \"\"\"Drops the PostgreSQL table created for the benchmark and closes the database connection.\"\"\"\n","        if not self.conn: # If no connection, nothing to clean up\n","            return\n","\n","        try:\n","            cur = self.conn.cursor() # Create cursor\n","            cur.execute(f\"DROP TABLE IF EXISTS {self.table_name}\") # Drop table\n","            self.conn.commit() # Commit transaction\n","            cur.close() # Close cursor\n","            print(f\"ðŸ§¹ Deleted PGVector table: {self.table_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete PGVector table '{self.table_name}': {e}. You may need to drop it manually.\")\n","        finally:\n","            self.conn.close() # Always close the database connection\n","            self.conn = None\n","\n","# -------------------- Main Benchmark Function --------------------\n","# This function orchestrates the entire benchmarking process for a single dataset.\n","\n","def run_benchmark_pgvector(dataset_name, pg_conn_params):\n","    \"\"\"Runs a full benchmark for PGVector with binary embeddings for a specified MAIR dataset.\"\"\"\n","    print(f\"\\n{'='*70}\")\n","    print(f\"ðŸš€ Starting PGVector Binary Benchmark for Dataset: {dataset_name}\")\n","    print(f\"ðŸ“Š Using Cohere {EMBEDDING_MODEL} embeddings binarized for pgvector BIT type.\")\n","    print(f\"{('='*70})\")\n","\n","    # Construct paths to the dataset within the combined MAIR directory.\n","    dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","    docs_path = os.path.join(dataset_path, 'docs')\n","    queries_path = os.path.join(dataset_path, 'queries')\n","\n","    # Verify dataset files exist before proceeding.\n","    if not os.path.exists(docs_path) or not os.path.exists(queries_path):\n","        print(f\"âš ï¸ Dataset '{dataset_name}' not found at {dataset_path}. Skipping this benchmark run.\")\n","        return None\n","\n","    print(f\"\\nðŸ“¥ Loading MAIR dataset: {dataset_name} from {dataset_path}...\")\n","    corpus = {}\n","    docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","    for file in docs_files:\n","        corpus.update(load_jsonl(os.path.join(docs_path, file))) # Load document JSONL files\n","\n","    queries = {}\n","    query_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","    for file in query_files:\n","        queries.update(load_jsonl(os.path.join(queries_path, file))) # Load query JSONL files\n","\n","    qrels = extract_qrels_from_queries(queries) # Extract relevance judgments from queries\n","    print(f\"âœ… Loaded: {len(corpus)} documents, {len(queries)} queries, {len(qrels)} qrels.\")\n","\n","    if not corpus or not queries or not qrels: # Early exit if essential data is missing\n","        print(f\"âš ï¸ Essential data (corpus, queries, or qrels) is empty for {dataset_name}. Skipping benchmark.\")\n","        return None\n","\n","    print(f\"\\nðŸ§  Generating float embeddings with Cohere {EMBEDDING_MODEL} (Dimension: {VECTOR_DIMENSION}D)...\")\n","    # Limit the number of documents to embed and upload, based on MAX_UPLOAD_DOCS configuration.\n","    docs = list(corpus.items())[:MAX_UPLOAD_DOCS]\n","    print(f\"  Processing up to {len(docs)} documents for embedding (limited by MAX_UPLOAD_DOCS={MAX_UPLOAD_DOCS}).\")\n","\n","    # Extract text content from corpus documents for embedding.\n","    texts = []\n","    for doc_id, doc_content in docs:\n","        if isinstance(doc_content, dict):\n","            text = None\n","            for field in ['doc', 'text', 'content', 'body', 'passage', 'document', 'title', 'abstract']:\n","                if field in doc_content and doc_content[field]:\n","                    val = doc_content[field]\n","                    if isinstance(val, str) and val.strip():\n","                        text = val\n","                        break\n","            if not text: # Fallback for text extraction if primary fields are empty\n","                for key, val in doc_content.items():\n","                    if isinstance(val, str) and val.strip() and len(val) > 10:\n","                        text = val\n","                        break\n","            if not text:\n","                text = str(doc_content) # Last resort, convert entire dict to string\n","        else:\n","            text = str(doc_content)\n","\n","        texts.append(text if text else \"document\") # Default text if extraction yields empty string\n","\n","    doc_ids = [str(d[0]) for d in docs] # Ensure all document IDs are strings\n","\n","    print(f\"  ðŸ“„ Generating corpus embeddings for {len(texts)} documents...\")\n","    t_embed_start = time.time() # Start timer for corpus embedding\n","\n","    corpus_embeddings_float = []\n","    # Batch processing for Cohere API calls for corpus embeddings.\n","    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding corpus documents\"):\n","        batch_texts = texts[i:i+BATCH_SIZE]\n","        response = cohere_client.embed(texts=batch_texts, model=EMBEDDING_MODEL, input_type=INPUT_TYPE_CORPUS)\n","        corpus_embeddings_float.extend(response.embeddings)\n","\n","    t_embed_end = time.time() # End timer\n","    embedding_time = t_embed_end - t_embed_start\n","\n","    print(f\"  ðŸ” Generating query embeddings for {len(queries)} queries...\")\n","    query_ids = list(queries.keys())\n","    query_texts = []\n","    for qid in query_ids:\n","        q = queries[qid]\n","        if isinstance(q, dict):\n","            text = (q.get('text') or q.get('query') or q.get('instruction') or q.get('question') or q.get('query_text') or str(q))\n","        else:\n","            text = str(q)\n","        query_texts.append(text if text else \"query\")\n","\n","    t_query_start = time.time() # Start timer for query embedding\n","    query_embeddings_float = []\n","    # Batch processing for Cohere API calls for query embeddings.\n","    for i in tqdm(range(0, len(query_texts), BATCH_SIZE), desc=\"Embedding queries\"):\n","        batch_texts = [str(t) if t else \"query\" for t in query_texts[i:i+BATCH_SIZE]]\n","        response = cohere_client.embed(texts=batch_texts, model=EMBEDDING_MODEL, input_type=INPUT_TYPE_QUERY)\n","        query_embeddings_float.extend(response.embeddings)\n","\n","    t_query_end = time.time() # End timer\n","    query_embedding_time = t_query_end - t_query_start\n","\n","    # ------------------ BINARIZATION ------------------\n","    print(f\"\\nðŸ”„ Binarizing float embeddings (Sign-based method: >=0 -> '1', <0 -> '0')...\")\n","    t_bin_start = time.time() # Start timer for binarization\n","\n","    corpus_binary_strs, float_size_mb = binarize_embeddings(corpus_embeddings_float) # Binarize corpus embeddings into strings\n","    query_binary_strs, _ = binarize_embeddings(query_embeddings_float) # Binarize query embeddings into strings\n","\n","    t_bin_end = time.time() # End timer\n","    binarize_time = t_bin_end - t_bin_start\n","\n","    # Clean up large float embedding arrays from memory as they are no longer needed.\n","    # This is important to free up RAM, especially in Colab.\n","    del corpus_embeddings_float\n","    del query_embeddings_float\n","    gc.collect() # Force garbage collection\n","\n","    print(f\"    Original Float Embeddings Size: {float_size_mb:.2f} MB\")\n","    print(f\"    Binarization to Bit Strings Time: {binarize_time:.4f}s\")\n","    print(f\"    Note: `BIT` type in PostgreSQL natively stores 1 bit per dimension, offering significant storage savings.\")\n","    # --------------------------------------------------\n","\n","    # Prepare precomputed vectors in the format expected by the PGVector provider.\n","    precomputed_vectors_for_upload = []\n","    for doc_id, bin_vec_str in zip(doc_ids, corpus_binary_strs):\n","        precomputed_vectors_for_upload.append({\"id\": doc_id, \"vector\": bin_vec_str})\n","\n","    print(f\"\\nâ±ï¸  Total Embedding & Binarization Preparation Time:\")\n","    print(f\"    Float Embedding Generation: {embedding_time + query_embedding_time:.4f}s\")\n","    print(f\"    Binary String Conversion: {binarize_time:.4f}s\")\n","\n","    # Generate a safe table name for PostgreSQL from the dataset name.\n","    safe_dataset_name = dataset_name.replace('/', '_').replace('-', '_').lower()\n","    table_name = f\"mair_bin_{safe_dataset_name}\"[:63] # PostgreSQL table names have a max length of 63 characters.\n","\n","    try:\n","        # Initialize the PGVector provider class.\n","        provider = PGVectorBinaryProvider(\n","            conn_params=pg_conn_params,\n","            table_name=table_name,\n","            precomputed_vectors=precomputed_vectors_for_upload,\n","            query_embeddings=query_binary_strs,\n","            vector_dim=VECTOR_DIMENSION\n","        )\n","\n","        print(f\"\\nðŸ“¤ Uploading binary vectors to PGVector table: {table_name}...\")\n","        num_uploaded = provider.upload() # Upload vectors to PostgreSQL\n","        if num_uploaded == 0: # If no documents were uploaded, skip search and evaluation.\n","            print(f\"âš ï¸ No documents uploaded to PGVector. Skipping search and evaluation.\")\n","            # Proceed to cleanup, but don't try to search.\n","            should_delete = should_cleanup_table(table_name, dataset_name)\n","            if should_delete:\n","                provider.cleanup()\n","            else:\n","                print(f\"ðŸ’¾ Keeping PGVector table: {table_name}. Remember to delete it manually if no longer needed.\")\n","            del provider\n","            clean_memory()\n","            return None\n","\n","        print(f\"\\nðŸ” Performing binary vector search (Hamming distance) in PGVector...\")\n","        results_per_query = {} # Dictionary to store retrieved results {query_id: {doc_id: score}}\n","        for i, qid in enumerate(tqdm(query_ids, desc=\"Searching PGVector\")):\n","            try:\n","                results_per_query[qid] = provider.search(i, top_k=TOP_K_SEARCH)\n","            except Exception as e:\n","                print(f\"\\nâŒ Search for query {i+1} failed: {e}\")\n","                results_per_query[qid] = {} # Log empty results for failed queries\n","\n","        search_stats = provider.get_search_stats() # Get search timing statistics\n","        print(f\"\\nâ±ï¸  PGVector Search Timing Summary:\")\n","        print(f\"    Total search time across all queries: {search_stats['total']:.4f}s\")\n","        print(f\"    Mean query search time: {search_stats['mean']:.4f}s ({search_stats['mean']*1000:.2f}ms)\")\n","        print(f\"    Median query search time: {search_stats['median']:.4f}s\")\n","        print(f\"    Min query search time: {search_stats['min']:.4f}s, Max query search time: {search_stats['max']:.4f}s\")\n","\n","        print(f\"\\nðŸ“Š Evaluating retrieval quality (NDCG, MAP, Recall, Precision @K={K_VALUES})...\")\n","        ndcg, _map, recall, precision = calculate_retrieval_metrics(results_per_query, qrels, K_VALUES) # Calculate metrics\n","        format_and_print_metrics(ndcg, _map, recall, precision) # Print formatted metrics\n","\n","        metrics = extract_all_metrics(ndcg, _map, recall, precision, K_VALUES) # Extract all metrics into a dictionary\n","        dataset_category = get_dataset_category(dataset_name) # Get dataset category for result entry\n","\n","        result_entry = { # Compile all results and metadata into a dictionary\n","            \"Dataset\": dataset_name,\n","            \"Category\": dataset_category,\n","            \"Provider\": \"pgvector_binary\", # Indicate provider\n","            \"Num_Corpus\": len(corpus),\n","            \"Num_Uploaded\": num_uploaded,\n","            \"Num_Queries\": len(queries),\n","            \"Embedding_Model\": EMBEDDING_MODEL,\n","            \"Vector_Dimension\": VECTOR_DIMENSION,\n","            \"Embedding_Type\": \"Binary (1-bit)\", # Explicitly state embedding type\n","            \"Batch_Size\": BATCH_SIZE,\n","            \"Cleanup_Policy\": CLEANUP_POLICY,\n","            \"Embedding_Generation_Time_s\": round(embedding_time, 4), # Time to generate float embeddings\n","            \"Query_Embedding_Generation_Time_s\": round(query_embedding_time, 4), # Time to generate query float embeddings\n","            \"Binarization_Time_s\": round(binarize_time, 4), # Time to convert to binary strings\n","            \"Upload_Table_Creation_s\": round(provider.upload_timings[\"table_creation_s\"], 4),\n","            \"Upload_Insert_Total_s\": round(provider.upload_timings[\"insert_time_s\"], 4),\n","            \"Upload_Index_Creation_s\": round(provider.upload_timings[\"index_creation_s\"], 4),\n","            \"Upload_Total_s\": round(\n","                provider.upload_timings[\"table_creation_s\"] +\n","                provider.upload_timings[\"insert_time_s\"] +\n","                provider.upload_timings[\"index_creation_s\"], 4\n","            ),\n","            \"Search_Total_s\": round(search_stats['total'], 4),\n","            \"Search_Mean_s\": round(search_stats['mean'], 4),\n","            \"Search_Median_s\": round(search_stats['median'], 4),\n","            \"Search_Min_s\": round(search_stats['min'], 4),\n","            \"Search_Max_s\": round(search_stats['max'], 4),\n","            \"Search_Std_s\": round(search_stats['std'], 4),\n","            \"Search_Mean_ms\": round(search_stats['mean'] * 1000, 2), # Mean search time in milliseconds\n","            \"Index_Type\": \"HNSW (bit_hamming_ops)\", # Type of index used\n","            \"Similarity_Metric\": \"Hamming\" # Similarity metric used for search\n","        }\n","\n","        result_entry.update(metrics) # Add the retrieval metrics to the result dictionary\n","        save_results_to_csv(result_entry, CSV_PATH) # Save this run's results to the CSV file\n","\n","        should_delete = should_cleanup_table(table_name, dataset_name) # Determine cleanup action based on policy\n","        if should_delete:\n","            provider.cleanup() # Delete table and close connection\n","        else:\n","            print(f\"ðŸ’¾ Keeping PGVector table: {table_name}. Remember to delete it manually if no longer needed to avoid resource consumption.\")\n","\n","        del provider # Explicitly delete provider object\n","        del results_per_query # Clear search results\n","        clean_memory() # Force garbage collection\n","\n","        print(f\"\\nâœ… PGVector Binary benchmark completed successfully for dataset: {dataset_name}\")\n","        return result_entry # Return the result entry for this dataset\n","\n","    except Exception as e:\n","        print(f\"âŒ An error occurred during the PGVector benchmark for dataset {dataset_name}: {e}\")\n","        import traceback\n","        traceback.print_exc() # Print full traceback for debugging\n","        clean_memory()\n","        return None # Return None on error\n","    finally:\n","        # Clean up large objects from memory regardless of success or failure.\n","        del precomputed_vectors_for_upload\n","        del corpus\n","        clean_memory()\n","\n","\n","# -------------------- MAIN EXECUTION BLOCK --------------------\n","# This block handles the overall flow of the benchmark, including API key checks,\n","# dataset discovery, and the interactive loop for running benchmarks.\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ðŸŽ¯ STARTING MAIR BENCHMARK - PGVECTOR BINARY EDITION\")\n","print(\"=\"*70)\n","\n","print(\"\\nðŸ”‘ Checking API Keys and Database Connection Status...\")\n","\n","# Check Cohere API key availability.\n","if not COHERE_API_KEY:\n","    print(\"\\nâŒ Cohere API key required for embedding generation!\")\n","    print(\"   Please add your API key to Colab Secrets (look for the key icon ðŸ”‘ on the left sidebar)\")\n","    print(\"   and name it 'COHERE_API_KEY'. Alternatively, set it as an environment variable.\")\n","    exit(1) # Stop execution if key is missing\n","\n","import cohere\n","cohere_client = cohere.Client(COHERE_API_KEY) # Initialize Cohere client.\n","print(f\"âœ… Cohere client initialized for embedding generation (model: {EMBEDDING_MODEL}).\")\n","\n","# Test PostgreSQL connection.\n","try:\n","    test_conn = psycopg2.connect(**PG_CONN_PARAMS) # Attempt to connect to DB\n","    register_vector(test_conn) # Register vector type\n","    cur = test_conn.cursor() # Create cursor\n","    cur.execute(\"SELECT version();\") # Get PostgreSQL version\n","    pg_version = cur.fetchone()[0]\n","    cur.execute(\"SELECT extversion FROM pg_extension WHERE extname = 'vector';\") # Get pgvector version\n","    pgvector_version = cur.fetchone()\n","    cur.close() # Close cursor\n","    test_conn.close() # Close connection\n","    print(f\"âœ… PostgreSQL connection successful to '{PG_CONN_PARAMS['database']}'.\")\n","    print(f\"   PostgreSQL version: {pg_version.split(',')[0]}\")\n","    if pgvector_version:\n","        print(f\"   pgvector extension version: v{pgvector_version[0]}\")\n","    else:\n","        print(f\"   âš ï¸ pgvector extension not found in 'vectordb'. Please check installation in Step 1.\")\n","except Exception as e:\n","    print(f\"âŒ PostgreSQL connection failed: {e}. Please ensure PostgreSQL is running and configured correctly.\")\n","    exit(1) # Stop execution if DB connection fails\n","\n","# Discover available MAIR datasets.\n","print(\"\\nðŸ“Š Discovering MAIR datasets from the combined path...\")\n","mair_datasets, datasets_by_category, dataset_sizes = get_mair_datasets() # Use helper function\n","\n","if not mair_datasets: # If no datasets are found, inform the user and exit.\n","    print(\"âŒ No MAIR datasets found in the combined path!\")\n","    print(f\"   Please ensure datasets are downloaded and combined into: {MAIR_COMBINED_PATH}\")\n","    print(\"   Run the previous notebook sections to prepare the datasets.\")\n","    exit(1)\n","\n","print(f\"\\nðŸ“‚ Available MAIR datasets for benchmarking ({len(mair_datasets)} total):\")\n","dataset_index = 1\n","dataset_map = {} # Map user-friendly index to actual dataset name\n","\n","# Display datasets grouped by category and sorted by size for easy selection.\n","for category in sorted(datasets_by_category.keys()):\n","    datasets_in_category = datasets_by_category[category]\n","    if datasets_in_category:\n","        # Sort datasets within each category by document count (descending).\n","        sorted_datasets_in_category = sorted(datasets_in_category, key=lambda d: dataset_sizes.get(d, 0), reverse=True)\n","        print(f\"\\n  ðŸ“ {category} ({len(sorted_datasets_in_category)} datasets):\")\n","        for dataset in sorted_datasets_in_category:\n","            size_str = format_size(dataset_sizes.get(dataset, 0)) # Format document count for display\n","            print(f\"     {dataset_index}. {dataset} ({size_str} docs)\")\n","            dataset_map[dataset_index] = dataset\n","            dataset_index += 1\n","\n","all_results = [] # List to store all benchmark results.\n","\n","# Main interactive benchmarking loop.\n","print(\"\\n\" + \"=\"*70)\n","print(\"ðŸš€ READY TO START PGVECTOR BINARY EMBEDDINGS BENCHMARK!\")\n","print(\"   Choose datasets to test. Results will be saved to: {CSV_PATH}\")\n","print(\"=\"*70)\n","\n","while True:\n","    choice = input(f\"\\nâž¡ï¸ Enter dataset number (1-{len(dataset_map)}) to benchmark, or type 'stop' to finish: \").strip().lower()\n","    if choice == \"stop\":\n","        break # Exit loop if user types 'stop'\n","\n","    try:\n","        idx = int(choice)\n","        if idx in dataset_map:\n","            dataset_name = dataset_map[idx] # Get the selected dataset name\n","        else:\n","            print(f\"âš ï¸ Invalid number '{choice}'. Please choose a number between 1 and {len(dataset_map)} from the list above.\")\n","            continue # Ask for input again\n","    except ValueError:\n","        print(\"âš ï¸ Invalid input. Please enter a number or 'stop'.\")\n","        continue # Ask for input again\n","\n","    # Run the benchmark for the selected dataset.\n","    result = run_benchmark_pgvector(dataset_name, PG_CONN_PARAMS)\n","    if result: # If the benchmark run was successful, add its result.\n","        all_results.append(result)\n","\n","# Final Summary after all chosen datasets are processed.\n","if all_results:\n","    print(f\"\\n{'='*70}\")\n","    print(\"ðŸ PGVECTOR BINARY BENCHMARK COMPLETE! (Overall Summary)\")\n","    print(f\"{('='*70}\")\n","\n","    df = pd.DataFrame(all_results) # Create a DataFrame from collected results.\n","    print(f\"\\nðŸ’¾ All detailed results have been saved to: {CSV_PATH}\")\n","\n","    print(\"\\nðŸ“Š Summary Table of Key Metrics (per dataset):\")\n","    # Display key retrieval metrics and performance times.\n","    summary_cols = ['Dataset', 'Category', 'NDCG@10', 'MAP@10', 'Recall@100', 'Search_Mean_ms', 'Upload_Total_s']\n","    available_cols = [col for col in summary_cols if col in df.columns] # Ensure columns exist\n","    print(df[available_cols].to_string(index=False))\n","\n","    print(\"\\nðŸ“ˆ Average Performance Across All Benchmarked Datasets:\")\n","    # Calculate averages for numeric metrics.\n","    numeric_cols = ['NDCG@10', 'MAP@10', 'Recall@100', 'Search_Mean_ms', 'Upload_Total_s']\n","    valid_cols = [col for col in numeric_cols if col in df.columns]\n","    if valid_cols:\n","        avg_vals = df[valid_cols].mean()\n","        print(\"\\n  Retrieval Quality (Averages):\")\n","        for col in ['NDCG@10', 'MAP@10', 'Recall@100']:\n","            if col in valid_cols:\n","                print(f\"    {col}: {avg_vals[col]:.4f}\")\n","        print(\"\\n  Performance (Averages):\")\n","        if 'Search_Mean_ms' in valid_cols:\n","            print(f\"    Average Search Time per Query: {avg_vals['Search_Mean_ms']:.2f}ms\")\n","        if 'Upload_Total_s' in valid_cols:\n","            print(f\"    Average Total Upload Time: {avg_vals['Upload_Total_s']:.2f}s\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"âœ¨ PGVector Binary Benchmark Concluded Successfully!\")\n","    print(\"   Detailed results are available in the CSV file for further analysis.\")\n","    print(\"   Remember to manage your cloud resources if you chose to keep tables.\")\n","    print(\"=\"*70)\n","else:\n","    print(\"\\nâš ï¸ No results to display. Please ensure you successfully ran at least one benchmark for a dataset.\")"],"metadata":{"id":"sikvccXfN8Ze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vector (Non-Binary) Benchmarking Based on MAIR Dataset"],"metadata":{"id":"0efooQnSOOUZ"}},{"cell_type":"markdown","source":["## Vector (Non-Binary) Search in Moorcheh + Pinecone (with Cohere) + Elasticsearch"],"metadata":{"id":"eFBB-XAMOfW0"}},{"cell_type":"code","source":["# ============================================================\n","# MAIR Benchmark - COMPLETE: Moorcheh vs Pinecone vs Elasticsearch\n","# Apple-to-Apple Comparison: Embed ONCE, Test ALL providers\n","# ============================================================\n","\n","# 1ï¸âƒ£ Install Necessary Libraries\n","# This section ensures all required Python packages are installed in the Colab environment.\n","# The installation might take a few moments, especially on the first run.\n","!pip install moorcheh-sdk cohere pinecone elasticsearch datasets\n","\n","import os\n","import gc\n","import json\n","import time\n","import statistics\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","# 2ï¸âƒ£ Environment Setup (Colab / Local Compatibility)\n","# This section configures paths and retrieves API keys, adapting to either Google Colab or a local environment.\n","\n","# DRIVE_PATH: Defines where benchmark results will be saved. Default is the current directory.\n","# If running in Google Colab, it will be automatically updated to a path in your mounted Google Drive.\n","DRIVE_PATH = \".\"\n","\n","# MAIR_COMBINED_PATH: **User Customizable Path**\n","# This is the path to your combined MAIR datasets in Google Drive. It should match the SAVE_PATH from the\n","# 'Combine MAIR Docs and Queries' step. Ensure this path is correct for your setup.\n","MAIR_COMBINED_PATH = \"/content/gdrive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Combined\"\n","\n","try:\n","    # This block attempts to configure for Google Colab, leveraging its `drive` and `userdata` (Secrets) features.\n","    from google.colab import drive, userdata as colab_userdata\n","\n","    try:\n","        # Mount Google Drive to allow Colab to access your files. If you've mounted it recently,\n","        # Colab might remember the authentication. `force_remount=True` ensures a fresh mount if needed.\n","        drive.mount('/content/gdrive')\n","        print(\"âœ… Google Drive mounted successfully\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Drive mount warning: {e}\")\n","        print(\"Continuing without Drive mount...\")\n","\n","    # DRIVE_PATH: **User Customizable Path**\n","    # If running in Colab, results will be saved here within your Google Drive.\n","    # You can customize this path (e.g., '/content/gdrive/MyDrive/MyProject/BenchmarkResults')\n","    # to organize your benchmark outputs effectively.\n","    DRIVE_PATH = '/content/gdrive/MyDrive/Moorcheh/Benchmark_Results/MAIR.Moorcheh.vs.Elasticsearch.Results'\n","    os.makedirs(DRIVE_PATH, exist_ok=True) # Creates the directory if it doesn't exist.\n","    print(f\"âœ… Running in Colab. Results will be saved to: {DRIVE_PATH}\")\n","\n","    # Retrieve API keys from Colab secrets. **User Action Required**:\n","    # To use this feature, you MUST add your API keys to Colab's \"Secrets\" panel.\n","    # Look for the key icon (ðŸ”‘) on the left sidebar of your Colab notebook.\n","    # Name your secrets EXACTLY as follows:\n","    # - `MOORCHEH_API_KEY` for Moorcheh\n","    # - `COHERE_API_KEY` for Cohere (essential for embedding generation)\n","    # - `PINECONE_API_KEY` for Pinecone\n","    # - `ELASTIC_URL` for Elasticsearch endpoint (e.g., 'https://your-es-cluster.es.io:9243')\n","    # - `ELASTIC_API_KEY` for Elasticsearch API Key (preferred) OR\n","    # - `ELASTIC_USERNAME` and `ELASTIC_PASSWORD` for Elasticsearch Basic Auth.\n","    api_keys = {\n","        'moorcheh': colab_userdata.get('MOORCHEH_API_KEY'),\n","        'cohere': colab_userdata.get('COHERE_API_KEY'),\n","        'pinecone': colab_userdata.get('PINECONE_API_KEY'),\n","        'elasticsearch': {\n","            'url': colab_userdata.get('ELASTIC_URL'),\n","            'api_key': colab_userdata.get('ELASTIC_API_KEY'),\n","            'username': colab_userdata.get('ELASTIC_USERNAME'),\n","            'password': colab_userdata.get('ELASTIC_PASSWORD')\n","        }\n","    }\n","    # If any Elasticsearch credential is null, set the entire elasticsearch dict to None to indicate missing config.\n","    # This simplifies checks later for whether Elasticsearch is configured.\n","    if not any(api_keys['elasticsearch'].values()):\n","        api_keys['elasticsearch'] = None\n","\n","except ImportError:\n","    # This block executes if not in Google Colab (e.g., a local Python environment).\n","    DRIVE_PATH = \".\" # Results will be saved in the current working directory.\n","    # In a local environment, API keys are typically read from environment variables.\n","    # **User Action Required**: Set these environment variables before running the script.\n","    # - `MOORCHEH_API_KEY`\n","    # - `COHERE_API_KEY`\n","    # - `PINECONE_API_KEY`\n","    # - `ELASTIC_URL` (e.g., \"http://localhost:9200\" for local ES)\n","    # - `ELASTIC_API_KEY` OR `ELASTIC_USERNAME`, `ELASTIC_PASSWORD`\n","    api_keys = {\n","        'moorcheh': os.environ.get('MOORCHEH_API_KEY'),\n","        'cohere': os.environ.get('COHERE_API_KEY'),\n","        'pinecone': os.environ.get('PINECONE_API_KEY'),\n","        'elasticsearch': {\n","            'url': os.environ.get('ELASTIC_URL') or \"http://localhost:9200\", # Defaults to localhost for local setup\n","            'api_key': os.environ.get('ELASTIC_API_KEY'),\n","            'username': os.environ.get('ELASTIC_USERNAME') or \"elastic\",\n","            'password': os.environ.get('ELASTIC_PASSWORD')\n","        }\n","    }\n","    # Similar to Colab, if ES config is incomplete, mark it as None.\n","    if not any(api_keys['elasticsearch'].values()):\n","        api_keys['elasticsearch'] = None\n","    print(\"âš ï¸ Not running in Google Colab. Saving results locally. Ensure environment variables are set.\")\n","\n","# 3ï¸âƒ£ Benchmark Configuration\n","# These parameters control various aspects of the benchmark. Users can adjust these values\n","# to customize the benchmark's behavior, performance, and resource usage.\n","\n","# TOP_K_SEARCH: **User Customizable Value**\n","# The number of top-ranked results to retrieve from the vector database for each query.\n","# A higher value might improve recall but generally increases search latency and resource usage.\n","TOP_K_SEARCH = 100\n","\n","# K_VALUES: **User Customizable Value**\n","# A list of 'k' values at which retrieval metrics (NDCG, MAP, Recall, Precision) will be calculated.\n","# These values define the cut-off points for evaluation (e.g., NDCG@1, MAP@10, Recall@100).\n","# You can add or remove values based on your evaluation needs.\n","K_VALUES = [1, 3, 5, 10, 100]\n","\n","# MAX_UPLOAD_DOCS: **User Customizable Value**\n","# Limits the number of documents uploaded to vector databases. This is crucial for managing costs\n","# and execution time, especially with very large datasets. Set to a lower number (e.g., 10000)\n","# for quick tests or a very high number (e.g., 700000) for comprehensive runs. Set to `None`\n","# or a number greater than your dataset size to upload all documents.\n","MAX_UPLOAD_DOCS = 700000\n","\n","# BATCH_SIZE: **User Customizable Value**\n","# The number of embeddings to process or upload in a single API request/batch.\n","# Adjusting this value can significantly impact performance, memory usage, and API rate limits.\n","# Larger batches are generally faster due to reduced overhead but consume more RAM.\n","BATCH_SIZE = 100\n","\n","# EMBEDDING_MODEL: **User Customizable Value**\n","# The Cohere model used to generate the initial dense float embeddings.\n","# 'embed-v4.0' is recommended for its performance and higher dimensionality. Other options include\n","# 'embed-english-v3.0', 'embed-multilingual-v3.0', etc. Changing this will require adjusting VECTOR_DIMENSION.\n","EMBEDDING_MODEL = \"embed-v4.0\"\n","\n","# RERANK_MODEL: **User Customizable Value (Pinecone only)**\n","# The Cohere reranking model used to re-order the initial search results from Pinecone.\n","# 'rerank-english-v3.0' or 'rerank-multilingual-v3.0' can be used. This significantly improves\n","# retrieval quality but adds latency. Moorcheh and Elasticsearch utilize built-in scoring.\n","RERANK_MODEL = \"rerank-english-v3.0\" # You can change this to 'rerank-english-v3.0' for faster reranking.\n","\n","# INPUT_TYPE_CORPUS: Specifies the input type for corpus documents to Cohere's embedding model.\n","# This helps Cohere optimize embedding generation for different content types (e.g., 'search_document').\n","INPUT_TYPE_CORPUS = \"search_document\"\n","\n","# INPUT_TYPE_QUERY: Specifies the input type for queries to Cohere's embedding model.\n","# Similar to corpus input type, this optimizes query embedding generation (e.g., 'search_query').\n","INPUT_TYPE_QUERY = \"search_query\"\n","\n","# VECTOR_DIMENSION: **User Customizable Value (Must match EMBEDDING_MODEL)**\n","# The dimensionality of the generated embeddings. This value is critical and MUST match the output\n","# dimension of the `EMBEDDING_MODEL` you choose. Cohere's 'embed-v4.0' has 1536 dimensions;\n","# 'embed-v3.0' has 1024 dimensions. Incorrect dimension will lead to errors in vector databases.\n","VECTOR_DIMENSION = 1536\n","\n","# CSV_PATH: **User Customizable Path**\n","# The full path where the final benchmark results will be saved in CSV format.\n","# This file will be created or appended to in your Google Drive (or local directory).\n","CSV_PATH = os.path.join(DRIVE_PATH, \"MAIR.Moorcheh.vs.Elasticsearch.Cohere.V4.csv\")\n","\n","# 4ï¸âƒ£ Cleanup Policy\n","# This section allows the user to define how vector database resources (namespaces/indexes)\n","# are handled after each benchmark run. This helps manage cloud costs and maintain a clean environment.\n","print(\"\\nðŸ§¹ Namespace cleanup policy options:\")\n","print(\"  1) ask_each_time  -> Prompt after each dataset (default)\")\n","print(\"  2) always_delete  -> Automatically delete after benchmarking\")\n","print(\"  3) always_keep    -> Never delete (keep for future searches)\")\n","\n","cleanup_choice = input(\"Choose policy [1/2/3] (default 1): \").strip() or \"1\"\n","CLEANUP_POLICY_MAP = {\"1\": \"ask_each_time\", \"2\": \"always_delete\", \"3\": \"always_keep\"}\n","CLEANUP_POLICY = CLEANUP_POLICY_MAP.get(cleanup_choice, \"ask_each_time\")\n","print(f\"âœ… Selected cleanup policy: {CLEANUP_POLICY}\")\n","\n","# 5ï¸âƒ£ Helper Functions\n","# These functions are designed to assist in loading and processing MAIR datasets,\n","# extracting qrels, calculating retrieval metrics, and managing benchmark results.\n","\n","def load_jsonl(filepath):\n","    \"\"\"Loads data from a JSONL (JSON Lines) file into a dictionary.\n","    It intelligently identifies various common ID field names to create a mapping\n","    from item ID to its content. This is flexible for MAIR's varied formats.\n","    \"\"\"\n","    data = {}\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip():\n","                    item = json.loads(line)\n","                    item_id = item.get('_id') or item.get('id') or item.get('query_id') or item.get('doc_id')\n","                    if item_id:\n","                        data[str(item_id)] = item\n","                    else:\n","                        data[str(len(data))] = item\n","        print(f\"   Loaded {len(data)} items from {os.path.basename(filepath)}\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Error loading {filepath}: {e}\")\n","    return data\n","\n","def extract_qrels_from_queries(queries):\n","    \"\"\"Extracts Qrels (query-relevance judgments) from query data structures.\n","    This function is tailored for MAIR datasets where qrels might be embedded\n","    within query files, often in fields like 'labels', 'relevance', or 'qrels'.\n","    It supports different formats for these fields (list of dicts, list of IDs, or dict).\n","    \"\"\"\n","    qrels = {}\n","    for qid, q in queries.items():\n","        if isinstance(q, dict):\n","            labels = q.get('labels') or q.get('relevance') or q.get('qrels')\n","            if labels:\n","                qrels[str(qid)] = {}\n","                if isinstance(labels, list):\n","                    for label_item in labels:\n","                        if isinstance(label_item, dict):\n","                            doc_id = label_item.get('id') or label_item.get('doc_id')\n","                            score = label_item.get('score', 1)\n","                            if doc_id:\n","                                qrels[str(qid)][str(doc_id)] = score\n","                        else:\n","                            qrels[str(qid)][str(label_item)] = 1\n","                elif isinstance(labels, dict):\n","                    for doc_id, score in labels.items():\n","                        qrels[str(qid)][str(doc_id)] = score\n","    return qrels\n","\n","DATASET_CATEGORIES = {\n","    \"Legal & Regulatory\": [\"ACORDAR\", \"AILA2019-Case\", \"AILA2019-Statutes\", \"CUAD\", \"LeCaRDv2\", \"LegalQuAD\", \"REGIR-EU2UK\", \"REGIR-UK2EU\"],\n","    \"Medical & Clinical\": [\"CliniDS-2014\", \"CliniDS-2015\", \"CliniDS-2016\", \"ClinicalTrials-2021\", \"ClinicalTrials-2022\", \"ClinicalTrials-2023\", \"NFCorpus\", \"PrecisionMedicine\", \"Genomics-AdHoc\"],\n","    \"Code & Programming\": [\"APPS\", \"CodeEditSearch\", \"CodeSearchNet\", \"Conala\", \"HumanEval-X\", \"LeetCode\", \"MBPP\", \"RepoBench\", \"SWE-Bench-Lite\"],\n","    \"Financial\": [\"ConvFinQA\", \"FiQA\", \"FinQA\", \"FinanceBench\", \"HC3Finance\"],\n","    \"Academic & Scientific\": [\"ArguAna\", \"LitSearch\", \"ProofWiki-Proof\", \"ProofWiki-Reference\", \"Competition-Math\"],\n","    \"Conversational & Dialog\": [\"CAsT-2019\", \"CAsT-2020\", \"CAsT-2021\", \"CAsT-2022\", \"ProCIS-Dialog\", \"ProCIS-Turn\", \"SParC\", \"Quora\"],\n","    \"News & Social Media\": [\"ChroniclingAmericaQA\", \"Microblog-2011\", \"Microblog-2012\", \"Microblog-2013\", \"Microblog-2014\", \"News21\"],\n","    \"API Documentation\": [\"Apple\", \"FoodAPI\", \"HuggingfaceAPI\", \"PytorchAPI\"],\n","    \"Others\": [\"BSARD\", \"BillSum\", \"CARE\", \"CPCD\", \"CQADupStack\", \"DD\", \"ELI5\", \"ExcluIR\", \"FairRanking\", \"Fever\", \"GerDaLIR\", \"IFEval\", \"InstructIR\", \"MISeD\", \"Monant\", \"NTCIR\", \"NeuCLIR\", \"NevIR\", \"PointRec\", \"ProductSearch_2023\", \"QuanTemp\", \"Robust04\"]\n","}\n","\n","def get_dataset_category(dataset_name):\n","    \"\"\"Assigns a given dataset name to its predefined category for organized display.\"\"\"\n","    for category, datasets in DATASET_CATEGORIES.items():\n","        if dataset_name in datasets:\n","            return category\n","    return \"Others\"\n","\n","def get_dataset_size(dataset_name):\n","    \"\"\"Counts the number of documents in a MAIR dataset by reading its JSONL files.\"\"\"\n","    dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","    docs_path = os.path.join(dataset_path, 'docs')\n","    if not os.path.exists(docs_path):\n","        return 0\n","    doc_count = 0\n","    try:\n","        docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","        for file in docs_files:\n","            file_path = os.path.join(docs_path, file)\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                for line in f:\n","                    if line.strip():\n","                        doc_count += 1\n","    except Exception as e:\n","        print(f\"âš ï¸ Error counting docs in {dataset_name}: {e}\")\n","    return doc_count\n","\n","def format_size(num):\n","    \"\"\"Formats a number (document count) with K (thousands) or M (millions) suffix for readability.\"\"\"\n","    if num >= 1_000_000:\n","        return f\"{num/1_000_000:.1f}M\"\n","    elif num >= 1_000:\n","        return f\"{num/1_000:.1f}K\"\n","    return str(num)\n","\n","def get_mair_datasets():\n","    \"\"\"Discovers available MAIR datasets in the combined path, categorizes them, and gets their sizes.\"\"\"\n","    datasets_by_category = {category: [] for category in DATASET_CATEGORIES.keys()}\n","    all_datasets = []\n","    dataset_sizes = {}\n","    if os.path.exists(MAIR_COMBINED_PATH):\n","        print(f\"ðŸ” Scanning: {MAIR_COMBINED_PATH}\")\n","        try:\n","            items = os.listdir(MAIR_COMBINED_PATH)\n","            for item in sorted(items):\n","                if item.startswith('.') or item.startswith('_'):\n","                    continue\n","                dataset_path = os.path.join(MAIR_COMBINED_PATH, item)\n","                if not os.path.isdir(dataset_path):\n","                    continue\n","                docs_path = os.path.join(dataset_path, 'docs')\n","                queries_path = os.path.join(dataset_path, 'queries')\n","                if os.path.exists(docs_path) and os.path.exists(queries_path):\n","                    docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","                    queries_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","                    if docs_files and queries_files:\n","                        all_datasets.append(item)\n","                        category = get_dataset_category(item)\n","                        datasets_by_category[category].append(item)\n","                        doc_count = get_dataset_size(item)\n","                        dataset_sizes[item] = doc_count\n","                        print(f\"   âœ… {item} [Category: {category}] - {format_size(doc_count)} docs\")\n","        except Exception as e:\n","            print(f\"   âŒ Error scanning directory: {e}\")\n","    return all_datasets, datasets_by_category, dataset_sizes\n","\n","def calculate_timing_stats(timing_list):\n","    \"\"\"Calculates basic descriptive statistics (mean, median, min, max, std dev, total sum)\n","    for a given list of numerical timings. This is used to summarize performance metrics\n","    like search times, upload durations, etc., providing a quick overview of performance.\n","    \"\"\"\n","    if not timing_list:\n","        return {\"mean\": 0.0, \"median\": 0.0, \"min\": 0.0, \"max\": 0.0, \"std\": 0.0, \"total\": 0.0}\n","    return {\n","        \"mean\": statistics.mean(timing_list),\n","        \"median\": statistics.median(timing_list),\n","        \"min\": min(timing_list),\n","        \"max\": max(timing_list),\n","        \"std\": statistics.stdev(timing_list) if len(timing_list) > 1 else 0.0,\n","        \"total\": sum(timing_list)\n","    }\n","\n","def calculate_retrieval_metrics(retrieved_results, qrels, k_values=K_VALUES):\n","    \"\"\"Calculates NDCG, MAP, Recall, and Precision @K for given retrieval results and ground truth qrels.\"\"\"\n","    ndcg_scores = {f\"NDCG@{k}\": [] for k in k_values}\n","    map_scores = {f\"MAP@{k}\": [] for k in k_values}\n","    recall_scores = {f\"Recall@{k}\": [] for k in k_values}\n","    precision_scores = {f\"P@{k}\": [] for k in k_values}\n","\n","    for qid in retrieved_results:\n","        if qid not in qrels:\n","            continue\n","        relevant_docs = set(qrels[qid].keys())\n","        retrieved_docs = list(retrieved_results[qid].keys())\n","        if len(relevant_docs) == 0:\n","            continue\n","\n","        for k in k_values:\n","            top_k = retrieved_docs[:k]\n","            hits = len(set(top_k) & relevant_docs)\n","            recall = hits / len(relevant_docs) if len(relevant_docs) > 0 else 0.0\n","            recall_scores[f\"Recall@{k}\"].append(recall)\n","            precision = hits / k if k > 0 else 0.0\n","            precision_scores[f\"P@{k}\"].append(precision)\n","\n","            ap = 0.0\n","            hits_so_far = 0\n","            for i, doc_id in enumerate(top_k, 1):\n","                if doc_id in relevant_docs:\n","                    hits_so_far += 1\n","                    ap += hits_so_far / i\n","            ap = ap / min(len(relevant_docs), k) if len(relevant_docs) > 0 else 0.0\n","            map_scores[f\"MAP@{k}\"].append(ap)\n","\n","            dcg = 0.0\n","            for i, doc_id in enumerate(top_k, 1):\n","                if doc_id in relevant_docs:\n","                    dcg += 1.0 / np.log2(i + 1)\n","            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), k)))\n","            ndcg = dcg / idcg if idcg > 0 else 0.0\n","            ndcg_scores[f\"NDCG@{k}\"].append(ndcg)\n","\n","    ndcg_avg = {k: np.mean(v) if v else 0.0 for k, v in ndcg_scores.items()}\n","    map_avg = {k: np.mean(v) if v else 0.0 for k, v in map_scores.items()}\n","    recall_avg = {k: np.mean(v) if v else 0.0 for k, v in recall_scores.items()}\n","    precision_avg = {k: np.mean(v) if v else 0.0 for k, v in precision_scores.items()}\n","    return ndcg_avg, map_avg, recall_avg, precision_avg\n","\n","def format_and_print_metrics(ndcg, _map, recall, precision, k_values=K_VALUES):\n","    \"\"\"Formats and prints retrieval metrics (NDCG, MAP, Recall, Precision) in a clean,\n","    tabular format for specified K values. This provides an immediate, human-readable\n","    summary of the retrieval quality for a benchmark run.\n","    \"\"\"\n","    print(\"\\nRetrieval Metrics:\")\n","    print(\"â”€\" * 80)\n","    for k in k_values:\n","        print(f\"NDCG@{k}: {ndcg.get(f'NDCG@{k}', 0.0):.4f} | MAP@{k}: {_map.get(f'MAP@{k}', 0.0):.4f} | Recall@{k}: {recall.get(f'Recall@{k}', 0.0):.4f} | P@{k}: {precision.get(f'P@{k}', 0.0):.4f}\")\n","\n","def extract_all_metrics(ndcg, _map, recall, precision, k_values=K_VALUES):\n","    \"\"\"Extracts all relevant retrieval metrics into a single dictionary.\n","    This structured format is ideal for storage, particularly for CSV output,\n","    ensuring all evaluation results are consistently captured.\n","    \"\"\"\n","    metrics = {}\n","    for k in k_values:\n","        metrics[f\"NDCG@{k}\"] = float(ndcg.get(f\"NDCG@{k}\", 0.0)) if ndcg.get(f\"NDCG@{k}\") is not None else 0.0\n","        metrics[f\"MAP@{k}\"] = float(_map.get(f\"MAP@{k}\", 0.0))\n","        metrics[f\"Recall@{k}\"] = float(recall.get(f\"Recall@{k}\", 0.0))\n","        metrics[f\"P@{k}\"] = float(precision.get(f\"P@{k}\", 0.0))\n","    return metrics\n","\n","def save_results_to_csv(new_result: dict, csv_path: str):\n","    \"\"\"Appends a new benchmark result entry to a CSV file. If the file doesn't exist,\n","    it creates it along with the header. Otherwise, it appends the new data,\n","    ensuring data integrity and continuity of results over multiple runs.\n","    \"\"\"\n","    new_df = pd.DataFrame([new_result])\n","    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n","    write_header = not os.path.exists(csv_path)\n","    try:\n","        if write_header:\n","            new_df.to_csv(csv_path, mode='w', header=True, index=False)\n","        else:\n","            existing_df = pd.read_csv(csv_path)\n","            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n","            combined_df.to_csv(csv_path, mode='w', header=True, index=False)\n","        print(f\"ðŸ’¾ Saved to: {csv_path}\")\n","    except Exception as e:\n","        print(f\"âŒ CSV save failed: {e}\")\n","\n","def clean_memory():\n","    \"\"\"Forces Python's garbage collector to release memory.\n","    This is particularly important in resource-constrained environments like Colab,\n","    especially when processing large datasets, to prevent out-of-memory errors.\n","    \"\"\"\n","    gc.collect()\n","\n","def should_cleanup_namespace(provider_name, dataset_name):\n","    \"\"\"Interactively prompts the user whether to delete the created vector database\n","    index/namespace after benchmarking. This gives the user control over resource\n","    management, helping to prevent unintended cloud costs, based on the `CLEANUP_POLICY`.\n","    \"\"\"\n","    if CLEANUP_POLICY == \"always_delete\":\n","        return True\n","    elif CLEANUP_POLICY == \"always_keep\":\n","        return False\n","    else:\n","        response = input(f\"\\nâ“ Delete {provider_name} namespace for {dataset_name}? (y/n): \").strip().lower()\n","        return response in ['y', 'yes']\n","\n","# 6ï¸âƒ£ Provider Classes\n","# These classes encapsulate the specific logic for interacting with each vector\n","# database provider (Moorcheh, Pinecone, Elasticsearch). Each class handles\n","# operations such as vector upload, search queries, and resource cleanup, tailored\n","# to the provider's API.\n","\n","class MoorchehProvider:\n","    \"\"\"Manages interaction with the Moorcheh vector database for benchmarking.\"\"\"\n","    def __init__(self, client, namespace_name, precomputed_vectors, query_embeddings):\n","        self.client = client # Moorcheh API client instance.\n","        self.namespace_name = namespace_name # Unique name for the Moorcheh namespace.\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and vectors.\n","        self.query_embeddings = query_embeddings # List of query embeddings.\n","        self.upload_timings = {\"server_upload_time_s\": 0.0, \"batch_details\": []}\n","        self.search_timings = []\n","\n","    def upload(self):\n","        \"\"\"Uploads vectors to a Moorcheh vector namespace.\"\"\"\n","        from moorcheh_sdk import ConflictError # Imports specific Moorcheh exception for existing namespaces.\n","        try:\n","            # Attempts to create a new namespace. Moorcheh organizes vectors into namespaces.\n","            self.client.create_namespace(namespace_name=self.namespace_name, type=\"vector\", vector_dimension=VECTOR_DIMENSION)\n","            print(f\"âœ… Created VECTOR namespace: {self.namespace_name} (dim: {VECTOR_DIMENSION})\")\n","        except ConflictError:\n","            print(f\"âš ï¸ Namespace exists, using: {self.namespace_name}\")\n","        except Exception as e:\n","            print(f\"âŒ Error creating Moorcheh namespace: {e}\")\n","            raise\n","\n","        batch_num = 0\n","        # Iterates through the precomputed vectors in batches, displaying a progress bar.\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading to Moorcheh\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE] # Gets a slice of vectors for the current batch.\n","            batch_num += 1\n","            try:\n","                # Calls the Moorcheh SDK to upload the batch of vectors.\n","                response = self.client.upload_vectors(namespace_name=self.namespace_name, vectors=batch)\n","                server_time = 0.0\n","                if isinstance(response, dict):\n","                    server_time = response.get(\"execution_time\", 0.0)\n","                    if \"timings\" in response:\n","                        server_time = response[\"timings\"].get(\"total\", server_time)\n","                self.upload_timings[\"server_upload_time_s\"] += server_time # Accumulates total server upload time.\n","                self.upload_timings[\"batch_details\"].append({\"batch_num\": batch_num, \"batch_size\": len(batch), \"server_time_s\": server_time})\n","            except Exception as e:\n","                print(f\"\\nâŒ Batch {batch_num} failed: {e}\")\n","\n","        print(f\"\\nâ±ï¸  Moorcheh Upload Timing (SERVER-SIDE): {self.upload_timings['server_upload_time_s']:.4f}s\")\n","        return len(self.precomputed_vectors)\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a vector search with a query vector against Moorcheh.\"\"\"\n","        query_embedding = self.query_embeddings[query_idx] # Retrieves the specific query embedding.\n","        resp = self.client.search(namespaces=[self.namespace_name], query=query_embedding, top_k=top_k)\n","        server_time = resp.get(\"execution_time\", 0.0) if isinstance(resp, dict) else 0.0\n","        timings = resp.get(\"timings\", {}) if isinstance(resp, dict) else {}\n","        self.search_timings.append({\"server_time_s\": server_time, \"timings\": timings}) # Records timings for this search operation.\n","        hits = resp.get(\"results\", []) if isinstance(resp, dict) else []\n","        return {r[\"id\"]: r[\"score\"] for r in hits}\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for Moorcheh, including detailed component breakdowns.\"\"\"\n","        server_times = [t[\"server_time_s\"] for t in self.search_timings] # Extracts overall server times.\n","        stats = calculate_timing_stats(server_times) # Calculates stats for overall server time.\n","        component_totals = {}\n","        for timing in self.search_timings:\n","            for component, time_val in timing.get(\"timings\", {}).items():\n","                if component not in component_totals:\n","                    component_totals[component] = []\n","                component_totals[component].append(time_val)\n","        component_stats = {component: calculate_timing_stats(times) for component, times in component_totals.items()}\n","        return stats, component_stats\n","\n","    def cleanup(self):\n","        \"\"\"Deletes the Moorcheh namespace created for the benchmark.\"\"\"\n","        try:\n","            self.client.delete_namespace(self.namespace_name)\n","            print(f\"ðŸ§¹ Deleted namespace: {self.namespace_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete: {e}\")\n","\n","\n","class PineconeProvider:\n","    \"\"\"Manages interaction with the Pinecone vector database for benchmarking.\"\"\"\n","    def __init__(self, client, index_name, precomputed_vectors, query_embeddings, corpus_texts, cohere_client):\n","        self.client = client # Pinecone API client instance.\n","        self.index_name = index_name # Unique name for the Pinecone index.\n","        self.index = None # Will store the Pinecone Index object after creation.\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and vectors.\n","        self.query_embeddings = query_embeddings # List of query embeddings.\n","        self.corpus_texts = corpus_texts # Original texts of the corpus for reranking.\n","        self.cohere_client = cohere_client # Cohere client for reranking.\n","        self.upload_timings = {\"index_creation_s\": 0.0, \"upsert_time_s\": 0.0, \"batch_details\": []}\n","        self.search_timings = []\n","        self.rerank_timings = []\n","\n","    def upload(self):\n","        \"\"\"Uploads vectors to a Pinecone index.\"\"\"\n","        from pinecone import ServerlessSpec # Imports Pinecone's specification for index creation.\n","\n","        t0 = time.perf_counter()\n","        # Checks if an index with the same name already exists and deletes it.\n","        try:\n","            if self.index_name in [idx.name for idx in self.client.list_indexes()]:\n","                self.client.delete_index(self.index_name)\n","                time.sleep(5) # Pauses to allow Pinecone to complete the index deletion.\n","        except:\n","            pass\n","\n","        # Creates a new Pinecone index.\n","        self.client.create_index(name=self.index_name, dimension=VECTOR_DIMENSION, metric='cosine', spec=ServerlessSpec(cloud='aws', region='us-east-1')) # **User Customizable**: Adjust cloud/region as needed for `ServerlessSpec`.\n","        # Waits for the newly created index to be ready.\n","        while not self.client.describe_index(self.index_name).status['ready']:\n","            time.sleep(1)\n","\n","        t1 = time.perf_counter()\n","        self.upload_timings[\"index_creation_s\"] = t1 - t0 # Records index creation time.\n","        self.index = self.client.Index(self.index_name) # Gets the Pinecone Index object.\n","\n","        batch_num = 0\n","        # Upserts vectors in batches to Pinecone.\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading to Pinecone\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE]\n","            vectors = [(v['id'], v['vector']) for v in batch]\n","            batch_num += 1\n","            t_batch_start = time.perf_counter()\n","            self.index.upsert(vectors=vectors)\n","            t_batch_end = time.perf_counter()\n","            batch_time = t_batch_end - t_batch_start\n","            self.upload_timings[\"upsert_time_s\"] += batch_time # Accumulates total upsert time.\n","            self.upload_timings[\"batch_details\"].append({\"batch_num\": batch_num, \"batch_size\": len(batch), \"upsert_time_s\": batch_time})\n","\n","        print(f\"\\nâ±ï¸  Pinecone Upload Timing: Index Creation: {self.upload_timings['index_creation_s']:.4f}s, Upsert Total: {self.upload_timings['upsert_time_s']:.4f}s\")\n","        return len(self.precomputed_vectors)\n","\n","    def search(self, query_idx, query_text, top_k=100):\n","        \"\"\"Performs a vector search and then reranks results using Cohere for Pinecone.\"\"\"\n","        query_embedding = self.query_embeddings[query_idx] # Retrieves the specific query embedding.\n","        t0 = time.perf_counter()\n","        results = self.index.query(vector=query_embedding, top_k=top_k) # Executes the Pinecone query.\n","        t1 = time.perf_counter()\n","        search_time = t1 - t0\n","\n","        doc_ids = [match['id'] for match in results['matches']] # Extracts document IDs from initial search.\n","        doc_texts = [self.corpus_texts.get(doc_id, \"\") for doc_id in doc_ids] # Retrieves original texts for reranking.\n","\n","        t_rerank_start = time.perf_counter()\n","        try:\n","            # Reranks the top_k results using Cohere's rerank model.\n","            rerank_response = self.cohere_client.rerank(model=RERANK_MODEL, query=query_text, documents=doc_texts, top_n=top_k, return_documents=False)\n","            reranked_results = {doc_ids[result.index]: result.relevance_score for result in rerank_response.results}\n","            t_rerank_end = time.perf_counter()\n","            rerank_time = t_rerank_end - t_rerank_start\n","        except Exception as e:\n","            print(f\"\\nâš ï¸ Reranking failed: {e}, using original results\")\n","            reranked_results = {match['id']: match['score'] for match in results['matches']}\n","            rerank_time = 0.0\n","\n","        total_time = search_time + rerank_time # Total time including initial search and reranking.\n","        self.search_timings.append({\"query_time_s\": search_time, \"total_time_s\": total_time}) # Records timings.\n","        self.rerank_timings.append({\"rerank_time_s\": rerank_time})\n","        return reranked_results\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for Pinecone (client-side durations).\"\"\"\n","        query_times = [t[\"query_time_s\"] for t in self.search_timings]\n","        total_times = [t[\"total_time_s\"] for t in self.search_timings]\n","        rerank_times = [t[\"rerank_time_s\"] for t in self.rerank_timings]\n","        return {\"query\": calculate_timing_stats(query_times), \"total\": calculate_timing_stats(total_times), \"rerank\": calculate_timing_stats(rerank_times)}\n","\n","    def cleanup(self):\n","        \"\"\"Deletes the Pinecone index created for the benchmark.\"\"\"\n","        try:\n","            self.client.delete_index(self.index_name)\n","            print(f\"ðŸ§¹ Deleted Pinecone index: {self.index_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete: {e}\")\n","\n","\n","class ElasticsearchProvider:\n","    \"\"\"Manages interaction with Elasticsearch for benchmarking, using its `dense_vector` field type.\"\"\"\n","    def __init__(self, client, index_name, precomputed_vectors, query_embeddings):\n","        self.client = client # Elasticsearch API client instance.\n","        self.index_name = index_name # Unique name for the Elasticsearch index.\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and vectors.\n","        self.query_embeddings = query_embeddings # List of query embeddings.\n","        self.upload_timings = {\n","            \"index_creation_s\": 0.0,\n","            \"server_bulk_time_ms\": 0.0,\n","            \"client_total_time_s\": 0.0,\n","            \"batch_details\": []\n","        }\n","        self.search_timings = []\n","\n","    def upload(self):\n","        \"\"\"Uploads vectors to an Elasticsearch index using a `dense_vector` field.\"\"\"\n","        try:\n","            # Deletes an existing index if it's found, ensuring a fresh start for the benchmark.\n","            self.client.indices.delete(index=self.index_name)\n","            time.sleep(2)\n","        except:\n","            pass\n","\n","        t0 = time.perf_counter()\n","        # Defines the index mapping with a `dense_vector` field for storing embeddings.\n","        index_config = {\n","            \"mappings\": {\"properties\": {\"doc_id\": {\"type\": \"keyword\"}, \"embedding\": {\"type\": \"dense_vector\", \"dims\": VECTOR_DIMENSION, \"index\": True, \"similarity\": \"cosine\"}}},\n","            \"settings\": {\"number_of_shards\": 1, \"number_of_replicas\": 0} # **User Customizable**: Adjust shard/replica settings.\n","        }\n","\n","        try:\n","            self.client.indices.create(index=self.index_name, body=index_config)\n","            print(f\"âœ… Created Elasticsearch index: {self.index_name} (dim: {VECTOR_DIMENSION})\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Index creation error: {e}\")\n","            raise\n","\n","        t1 = time.perf_counter()\n","        self.upload_timings[\"index_creation_s\"] = t1 - t0 # Records index creation time.\n","\n","        print(f\"\\nðŸ“¤ Uploading vectors to Elasticsearch...\")\n","        batch_num = 0\n","        total_uploaded = 0\n","        t_upload_start = time.perf_counter()\n","\n","        # Uses Elasticsearch's bulk API for efficient ingestion of many documents.\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading to Elasticsearch\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE]\n","            batch_num += 1\n","\n","            bulk_body = []\n","            for vec in batch:\n","                bulk_body.append(json.dumps({\"index\": {\"_index\": self.index_name, \"_id\": vec[\"id\"]}}))\n","                bulk_body.append(json.dumps({\"doc_id\": vec[\"id\"], \"embedding\": vec[\"vector\"]}))\n","\n","            bulk_data = \"\\n\".join(bulk_body) + \"\\n\"\n","\n","            try:\n","                t_batch_start = time.perf_counter()\n","                response = self.client.bulk(body=bulk_data, refresh=False) # `refresh=False` improves ingestion performance.\n","                t_batch_end = time.perf_counter()\n","\n","                batch_client_time = t_batch_end - t_batch_start\n","                server_took_ms = response.get(\"took\", 0)\n","                self.upload_timings[\"server_bulk_time_ms\"] += server_took_ms # Accumulates server-side bulk time.\n","\n","                batch_success = 0\n","                if \"items\" in response:\n","                    for item in response[\"items\"]:\n","                        if \"index\" in item and item[\"index\"].get(\"status\") in [200, 201]:\n","                            batch_success += 1\n","\n","                total_uploaded += batch_success\n","                self.upload_timings[\"batch_details\"].append({\n","                    \"batch_num\": batch_num,\n","                    \"batch_size\": len(batch),\n","                    \"server_time_ms\": server_took_ms,\n","                    \"client_time_s\": batch_client_time\n","                })\n","            except Exception as e:\n","                print(f\"\\nâŒ Batch {batch_num} failed: {e}\")\n","\n","        t_upload_end = time.perf_counter()\n","        self.upload_timings[\"client_total_time_s\"] = t_upload_end - t_upload_start # Records client-side total upload time.\n","\n","        try:\n","            self.client.indices.refresh(index=self.index_name) # Refreshes the index to make documents searchable.\n","        except Exception as e:\n","            print(f\"âš ï¸ Refresh error: {e}\")\n","\n","        print(f\"\\nâ±ï¸  Elasticsearch Upload Timing:\")\n","        print(f\"    Index Creation: {self.upload_timings['index_creation_s']:.4f}s\")\n","        print(f\"    Bulk Upload (SERVER): {self.upload_timings['server_bulk_time_ms']/1000:.4f}s\")\n","        print(f\"    Bulk Upload (CLIENT): {self.upload_timings['client_total_time_s']:.4f}s\")\n","        return total_uploaded\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a k-Nearest Neighbor (kNN) search against Elasticsearch.\"\"\"\n","        query_embedding = self.query_embeddings[query_idx] # Retrieves the specific query embedding.\n","        query_body = {\n","            \"knn\": {\n","                \"field\": \"embedding\",\n","                \"query_vector\": query_embedding,\n","                \"k\": top_k,\n","                \"num_candidates\": min(top_k * 10, 10000) # **User Customizable**: Adjust `num_candidates` for speed/accuracy trade-off.\n","            },\n","            \"size\": top_k,\n","            \"_source\": [\"doc_id\"]\n","        }\n","\n","        try:\n","            t_start = time.perf_counter()\n","            response = self.client.search(index=self.index_name, body=query_body)\n","            t_end = time.perf_counter()\n","\n","            client_time_s = t_end - t_start\n","            server_took_ms = response.get(\"took\", 0)\n","            server_took_s = server_took_ms / 1000.0\n","\n","            self.search_timings.append({\n","                \"server_time_ms\": server_took_ms,\n","                \"server_time_s\": server_took_s,\n","                \"client_time_s\": client_time_s\n","            }) # Records detailed timings for this search.\n","\n","            hits = response.get(\"hits\", {}).get(\"hits\", [])\n","            results = {}\n","            for i, hit in enumerate(hits):\n","                doc_id = hit.get(\"_id\")\n","                score = hit.get(\"_score\", 1.0 / (1.0 + i)) # Retrieval score; falls back to a decreasing score if `_score` is missing.\n","                results[str(doc_id)] = score\n","            return results\n","\n","        except Exception as e:\n","            print(f\"\\nâŒ Search error: {e}\")\n","            self.search_timings.append({\n","                \"server_time_ms\": 0,\n","                \"server_time_s\": 0.0,\n","                \"client_time_s\": 0.0\n","            })\n","            return {}\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for Elasticsearch (both server-side and client-side).\"\"\"\n","        server_times_s = [t[\"server_time_s\"] for t in self.search_timings]\n","        client_times_s = [t[\"client_time_s\"] for t in self.search_timings]\n","\n","        server_stats = calculate_timing_stats(server_times_s)\n","        client_stats = calculate_timing_stats(client_times_s)\n","\n","        return {\n","            \"server\": server_stats,\n","            \"client\": client_stats\n","        }\n","\n","    def cleanup(self):\n","        \"\"\"Deletes the Elasticsearch index created for the benchmark.\"\"\"\n","        try:\n","            self.client.indices.delete(index=self.index_name)\n","            print(f\"ðŸ§¹ Deleted Elasticsearch index: {self.index_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete index: {e}\")\n","\n","\n","# 7ï¸âƒ£ Main Benchmark Orchestration Function\n","# The `run_benchmark_all_providers` function orchestrates the entire benchmarking workflow\n","# for a given dataset and selected providers. It encompasses data loading, embedding\n","# generation, uploading to various vector databases, performing searches,\n","# evaluating retrieval quality, and saving detailed results.\n","\n","def run_benchmark_all_providers(dataset_name, provider_names, es_client=None):\n","    \"\"\"Runs a full benchmark for specified providers and a chosen dataset.\"\"\"\n","    print(f\"\\n{'='*70}\")\n","    print(f\"ðŸš€ Dataset: {dataset_name}\")\n","    print(f\"ðŸ“Š Testing providers: {', '.join([p.upper() for p in provider_names])}\")\n","    print(f\"{'='*70}\")\n","\n","    dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","    docs_path = os.path.join(dataset_path, 'docs')\n","    queries_path = os.path.join(dataset_path, 'queries')\n","\n","    if not os.path.exists(docs_path) or not os.path.exists(queries_path):\n","        print(f\"âš ï¸ Dataset not found\")\n","        return []\n","\n","    print(f\"\\nðŸ“¥ Loading dataset...\")\n","    corpus = {}\n","    docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","    for file in docs_files:\n","        corpus.update(load_jsonl(os.path.join(docs_path, file)))\n","\n","    queries = {}\n","    query_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","    for file in query_files:\n","        queries.update(load_jsonl(os.path.join(queries_path, file)))\n","\n","    qrels = extract_qrels_from_queries(queries)\n","    print(f\"âœ… Loaded: {len(corpus)} docs, {len(queries)} queries, {len(qrels)} qrels\")\n","\n","    if not corpus or not queries or not qrels:\n","        print(f\"âš ï¸ Empty dataset\")\n","        return []\n","\n","    # Generate Float Embeddings with Cohere (Once per dataset for all providers)\n","    print(f\"\\nðŸ§  Step 1: Generating embeddings ONCE with Cohere {EMBEDDING_MODEL}...\")\n","    docs = list(corpus.items())[:MAX_UPLOAD_DOCS] # Limit documents based on MAX_UPLOAD_DOCS.\n","\n","    texts = []\n","    corpus_texts = {}\n","    for doc_id, doc_content in docs:\n","        if isinstance(doc_content, dict):\n","            text = None\n","            for field in ['doc', 'text', 'content', 'body', 'passage', 'document', 'title', 'abstract']:\n","                if field in doc_content and doc_content[field]:\n","                    val = doc_content[field]\n","                    if isinstance(val, str) and val.strip():\n","                        text = val\n","                        break\n","            if not text:\n","                for key, val in doc_content.items():\n","                    if isinstance(val, str) and val.strip() and len(val) > 10:\n","                        text = val\n","                        break\n","            if not text:\n","                text = str(doc_content)\n","        else:\n","            text = str(doc_content)\n","\n","        final_text = text if text else \"document\"\n","        texts.append(final_text)\n","        corpus_texts[str(doc_id)] = final_text\n","\n","    doc_ids = [str(d[0]) for d in docs]\n","\n","    print(f\"  ðŸ“„ Generating corpus embeddings for {len(texts)} documents...\")\n","    t_embed_start = time.perf_counter()\n","\n","    corpus_embeddings = []\n","    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding corpus\"):\n","        batch_texts = texts[i:i+BATCH_SIZE]\n","        response = cohere_client.embed(texts=batch_texts, model=EMBEDDING_MODEL, input_type=INPUT_TYPE_CORPUS)\n","        corpus_embeddings.extend(response.embeddings)\n","\n","    t_embed_end = time.perf_counter()\n","    embedding_time = t_embed_end - t_embed_start\n","\n","    precomputed_vectors = []\n","    for doc_id, text, embedding in zip(doc_ids, texts, corpus_embeddings):\n","        precomputed_vectors.append({\"id\": doc_id, \"vector\": embedding})\n","\n","    print(f\"  ðŸ” Generating query embeddings for {len(queries)} queries...\")\n","    query_ids = list(queries.keys())\n","    query_texts = []\n","    for qid in query_ids:\n","        q = queries[qid]\n","        if isinstance(q, dict):\n","            text = (q.get('text') or q.get('query') or q.get('instruction') or q.get('question') or q.get('query_text') or str(q))\n","        else:\n","            text = str(q)\n","        query_texts.append(text if text else \"query\")\n","\n","    t_query_start = time.perf_counter()\n","    query_embeddings = []\n","    for i in tqdm(range(0, len(query_texts), BATCH_SIZE), desc=\"Embedding queries\"):\n","        batch_texts = [str(t) if t else \"query\" for t in query_texts[i:i+BATCH_SIZE]]\n","        response = cohere_client.embed(texts=batch_texts, model=EMBEDDING_MODEL, input_type=INPUT_TYPE_QUERY)\n","        query_embeddings.extend(response.embeddings)\n","\n","    t_query_end = time.perf_counter()\n","    query_embedding_time = t_query_end - t_query_start\n","\n","    print(f\"\\nâ±ï¸  Embedding Generation Time (SHARED):\")\n","    print(f\"    Corpus: {embedding_time:.4f}s, Queries: {query_embedding_time:.4f}s, Total: {embedding_time + query_embedding_time:.4f}s\")\n","\n","    results_all_providers = [] # A list to collect benchmark results from all provider runs for this dataset.\n","\n","    # Benchmark Each Selected Provider\n","    for provider_name in provider_names:\n","        print(f\"\\n{'â”€'*70}\")\n","        print(f\"ðŸ”§ Testing Provider: {provider_name.upper()}\")\n","        print(f\"{'â”€'*70}\")\n","\n","        try:\n","            provider = None\n","            # Initializes the appropriate provider class based on the `provider_name`.\n","            if provider_name == 'moorcheh':\n","                if 'moorcheh' not in clients: # Skips if Moorcheh client was not initialized.\n","                    print(f\"âš ï¸ Skipping {provider_name}\")\n","                    continue\n","                namespace_name = f\"mair-{dataset_name.replace('/', '-')}-cohere-v4\"[:63]\n","                provider = MoorchehProvider(clients['moorcheh'], namespace_name, precomputed_vectors, query_embeddings)\n","            elif provider_name == 'pinecone':\n","                if 'pinecone' not in clients: # Skips if Pinecone client was not initialized.\n","                    print(f\"âš ï¸ Skipping {provider_name}\")\n","                    continue\n","                safe_dataset_name = dataset_name.replace('/', '-').replace('_', '-').lower()\n","                index_name = f\"mair-{safe_dataset_name}-v4\"[:45]\n","                provider = PineconeProvider(clients['pinecone'], index_name, precomputed_vectors, query_embeddings, corpus_texts, cohere_client)\n","            elif provider_name == 'elasticsearch':\n","                if es_client is None: # Skips if ES client failed to connect.\n","                    print(f\"âš ï¸ Skipping {provider_name}\")\n","                    continue\n","                safe_dataset_name = dataset_name.replace('/', '-').replace('_', '-').lower()\n","                index_name = f\"mair-{safe_dataset_name}-v4\"\n","                provider = ElasticsearchProvider(es_client, index_name, precomputed_vectors, query_embeddings)\n","\n","            if provider is None: # Catches cases where no provider object was created.\n","                print(f\"âŒ Could not initialize provider: {provider_name}. Check configuration and API keys.\")\n","                continue\n","\n","            print(f\"\\nðŸ“¤ Uploading pre-computed vectors to {provider_name}...\")\n","            num_uploaded = provider.upload() # Uploads data to the current provider.\n","\n","            print(f\"\\nðŸ” Searching with pre-computed query vectors...\")\n","\n","            results = {} # Stores search results for this specific provider.\n","            for i, qid in enumerate(tqdm(query_ids, desc=f\"Searching {provider_name}\")):\n","                try:\n","                    if provider_name == 'pinecone':\n","                        results[qid] = provider.search(i, query_texts[i], top_k=TOP_K_SEARCH)\n","                    else:\n","                        results[qid] = provider.search(i, top_k=TOP_K_SEARCH)\n","                except Exception as e:\n","                    print(f\"\\nâŒ Query {i+1} failed: {e}\")\n","                    results[qid] = {} # Logs empty results for failed queries.\n","\n","            # Retrieves and prints search timing statistics from the provider object.\n","            if provider_name == 'moorcheh':\n","                search_stats, component_stats = provider.get_search_stats()\n","                print(f\"\\nâ±ï¸  Moorcheh Search Timing (SERVER-SIDE): Total: {search_stats['total']:.4f}s, Mean: {search_stats['mean']:.4f}s\")\n","            elif provider_name == 'pinecone':\n","                search_stats = provider.get_search_stats()\n","                print(f\"\\nâ±ï¸  Pinecone Search Timing: Total: {search_stats['total']['total']:.4f}s, Mean: {search_stats['total']['mean']:.4f}s\")\n","            elif provider_name == 'elasticsearch':\n","                search_stats = provider.get_search_stats()\n","                print(f\"\\nâ±ï¸  Elasticsearch Search Timing:\")\n","                print(f\"    SERVER - Total: {search_stats['server']['total']:.4f}s, Mean: {search_stats['server']['mean']:.4f}s\")\n","                print(f\"    CLIENT - Total: {search_stats['client']['total']:.4f}s, Mean: {search_stats['client']['mean']:.4f}s\")\n","\n","            print(f\"\\nðŸ“Š Evaluating retrieval quality for {provider_name}...\")\n","            ndcg, _map, recall, precision = calculate_retrieval_metrics(results, qrels, K_VALUES) # Evaluate retrieval quality.\n","            format_and_print_metrics(ndcg, _map, recall, precision) # Print formatted metrics.\n","\n","            metrics = extract_all_metrics(ndcg, _map, recall, precision) # Stores all metrics.\n","            dataset_category = get_dataset_category(dataset_name) # Gets dataset category.\n","\n","            result = { # Compiles all results and metadata into a dictionary.\n","                \"Dataset\": dataset_name,\n","                \"Category\": dataset_category,\n","                \"Provider\": provider_name,\n","                \"Num_Corpus\": len(corpus),\n","                \"Num_Uploaded\": num_uploaded,\n","                \"Num_Queries\": len(queries),\n","                \"Embedding_Model\": EMBEDDING_MODEL,\n","                \"Vector_Dimension\": VECTOR_DIMENSION,\n","                \"Batch_Size\": BATCH_SIZE,\n","                \"Cleanup_Policy\": CLEANUP_POLICY,\n","                \"Embedding_Generation_Time_s\": round(embedding_time, 4),\n","                \"Query_Embedding_Time_s\": round(query_embedding_time, 4),\n","                \"Total_Embedding_Time_s\": round(embedding_time + query_embedding_time, 4),\n","            }\n","\n","            if provider_name == 'moorcheh':\n","                search_stats, component_stats = provider.get_search_stats()\n","                result.update({\n","                    \"Rerank_Model\": \"built-in\", # Moorcheh has built-in scoring/reranking.\n","                    \"Upload_Server_Total_s\": round(provider.upload_timings[\"server_upload_time_s\"], 4),\n","                    \"Upload_Total_s\": round(provider.upload_timings[\"server_upload_time_s\"], 4),\n","                    \"Search_Server_Total_s\": round(search_stats['total'], 4),\n","                    \"Search_Server_Mean_s\": round(search_stats['mean'], 4),\n","                    \"Search_Server_Median_s\": round(search_stats['median'], 4),\n","                    \"Search_Server_Mean_ms\": round(search_stats['mean'] * 1000, 2),\n","                })\n","\n","            elif provider_name == 'pinecone':\n","                search_stats = provider.get_search_stats()\n","                result.update({\n","                    \"Rerank_Model\": RERANK_MODEL,\n","                    \"Upload_Index_Creation_s\": round(provider.upload_timings[\"index_creation_s\"], 4),\n","                    \"Upload_Upsert_Total_s\": round(provider.upload_timings[\"upsert_time_s\"], 4),\n","                    \"Upload_Total_s\": round(provider.upload_timings[\"index_creation_s\"] + provider.upload_timings[\"upsert_time_s\"], 4),\n","                    \"Search_Total_s\": round(search_stats['total']['total'], 4),\n","                    \"Search_Mean_s\": round(search_stats['total']['mean'], 4),\n","                    \"Search_Mean_ms\": round(search_stats['total']['mean'] * 1000, 2),\n","                    \"Rerank_Mean_ms\": round(search_stats['rerank']['mean'] * 1000, 2),\n","                })\n","\n","            elif provider_name == 'elasticsearch':\n","                search_stats = provider.get_search_stats()\n","                result.update({\n","                    \"Rerank_Model\": \"built-in (Cosine KNN)\", # ES uses KNN with cosine similarity.\n","                    \"Upload_Index_Creation_s\": round(provider.upload_timings[\"index_creation_s\"], 4),\n","                    \"Upload_Server_Bulk_s\": round(provider.upload_timings[\"server_bulk_time_ms\"] / 1000, 4),\n","                    \"Upload_Client_Total_s\": round(provider.upload_timings[\"client_total_time_s\"], 4),\n","                    \"Upload_Total_s\": round(provider.upload_timings[\"index_creation_s\"] + provider.upload_timings[\"client_total_time_s\"], 4),\n","                    \"Search_Server_Total_s\": round(search_stats['server']['total'], 4),\n","                    \"Search_Server_Mean_s\": round(search_stats['server']['mean'], 4),\n","                    \"Search_Server_Median_s\": round(search_stats['server']['median'], 4),\n","                    \"Search_Server_Min_s\": round(search_stats['server']['min'], 4),\n","                    \"Search_Server_Max_s\": round(search_stats['server']['max'], 4),\n","                    \"Search_Server_Std_s\": round(search_stats['server']['std'], 4),\n","                    \"Search_Server_Mean_ms\": round(search_stats['server']['mean'] * 1000, 2),\n","                    \"Search_Client_Total_s\": round(search_stats['client']['total'], 4),\n","                    \"Search_Client_Mean_s\": round(search_stats['client']['mean'], 4),\n","                    \"Search_Client_Median_s\": round(search_stats['client']['median'], 4),\n","                    \"Search_Client_Mean_ms\": round(search_stats['client']['mean'] * 1000, 2),\n","                })\n","\n","            result.update(metrics) # Adds retrieval quality metrics to the result entry.\n","            save_results_to_csv(result, CSV_PATH) # Saves results to CSV after each provider/dataset run.\n","            results_all_providers.append(result) # Appends to the list of all results for final summary.\n","\n","            # Cleanup: Asks the user whether to delete the created index/namespace to manage cloud resources.\n","            should_delete = should_cleanup_namespace(provider_name, dataset_name)\n","            if should_delete:\n","                provider.cleanup() # Calls the provider's cleanup method.\n","            else:\n","                print(f\"ðŸ’¾ Keeping {provider_name} namespace/index\")\n","\n","            del provider # Explicitly deletes the provider object to free up resources.\n","            del results # Clears search results for the next iteration.\n","            clean_memory() # Forces garbage collection.\n","\n","            print(f\"\\nâœ… {provider_name.upper()} completed for {dataset_name}\")\n","\n","        except Exception as e:\n","            print(f\"âŒ Error with {provider_name}: {e}\")\n","            import traceback\n","            traceback.print_exc() # Prints full traceback for debugging.\n","            clean_memory()\n","\n","    # Clears large embedding arrays from memory after all providers for a dataset have been processed.\n","    del precomputed_vectors\n","    del query_embeddings\n","    del corpus_embeddings\n","    del corpus_texts\n","    clean_memory()\n","\n","    return results_all_providers\n","\n","\n","# 8ï¸âƒ£ Main Execution Block\n","# This block handles the overall flow of the benchmark, including API key checks,\n","# client initialization, provider selection, dataset discovery, and the interactive\n","# loop for running benchmarks.\n","\n","print(\"ðŸ”‘ Checking API Keys...\")\n","for provider in ['moorcheh', 'cohere', 'pinecone']:\n","    key = api_keys.get(provider)\n","    status = \"âœ…\" if key else \"âŒ\"\n","    print(f\"  {status} {provider.capitalize()}\")\n","\n","clients = {}\n","es_client = None\n","\n","# Initializes Cohere client, which is essential for generating embeddings for ALL providers.\n","# The script will exit if the Cohere API key is not found.\n","if api_keys['cohere']:\n","    import cohere\n","    cohere_client = cohere.Client(api_keys['cohere'])\n","    print(f\"âœ… Cohere client initialized\")\n","else:\n","    print(\"\\nâŒ Cohere API key required!\")\n","    exit(1)\n","\n","# Initializes Moorcheh client if it was selected and its API key is present.\n","if 'moorcheh' in api_keys and api_keys['moorcheh']:\n","    from moorcheh_sdk import MoorchehClient, ConflictError\n","    clients['moorcheh'] = MoorchehClient(api_key=api_keys['moorcheh'])\n","    print(f\"âœ… Moorcheh client initialized\")\n","\n","# Initializes Pinecone client if it was selected and its API key is present.\n","if 'pinecone' in api_keys and api_keys['pinecone']:\n","    from pinecone import Pinecone, ServerlessSpec\n","    clients['pinecone'] = Pinecone(api_key=api_keys['pinecone'])\n","    print(f\"âœ… Pinecone client initialized\")\n","\n","# Initializes Elasticsearch client if it was selected and properly configured.\n","try:\n","    from elasticsearch import Elasticsearch\n","    es_config = api_keys['elasticsearch']\n","\n","    if es_config.get('api_key'):\n","        try:\n","            es_client = Elasticsearch(es_config['url'], api_key=es_config['api_key'], request_timeout=60)\n","            if es_client.ping():\n","                print(\"âœ… Elasticsearch connected (API Key)\")\n","            else:\n","                es_client = None\n","        except Exception as e:\n","            es_client = None\n","\n","    if not es_client and es_config.get('password'):\n","        try:\n","            es_client = Elasticsearch(es_config['url'], basic_auth=(es_config['username'], es_config['password']), request_timeout=60)\n","            if es_client.ping():\n","                print(\"âœ… Elasticsearch connected (Basic Auth)\")\n","            else:\n","                es_client = None\n","        except Exception as e:\n","            es_client = None\n","\n","    if not es_client:\n","        try:\n","            es_client = Elasticsearch(es_config['url'], request_timeout=60)\n","            if es_client.ping():\n","                print(\"âœ… Elasticsearch connected (No Auth)\")\n","            else:\n","                es_client = None\n","        except Exception as e:\n","            es_client = None\n","\n","    if not es_client:\n","        print(f\"âš ï¸ Elasticsearch connection failed - URL: {es_config['url']}\")\n","\n","except ImportError:\n","    print(\"âš ï¸ Elasticsearch library not installed\")\n","except Exception as e:\n","    print(f\"âš ï¸ Elasticsearch error: {e}\")\n","\n","# 9ï¸âƒ£ Provider Selection\n","# User selects which providers to test interactively.\n","print(\"\\nðŸ”§ Available Providers (ALL using pre-computed vectors):\")\n","print(f\"  1. Moorcheh (Vector namespace - {EMBEDDING_MODEL} - {VECTOR_DIMENSION}D)\")\n","print(f\"  2. Pinecone (Vector index - {EMBEDDING_MODEL} - {VECTOR_DIMENSION}D + Cohere {RERANK_MODEL})\")\n","print(f\"  3. Elasticsearch (Dense vector - {EMBEDDING_MODEL} - {VECTOR_DIMENSION}D - Cosine KNN)\")\n","\n","provider_choice = input(\"\\nâž¡ï¸ Select providers (e.g., '1,2,3' or 'all'): \").strip().lower()\n","if provider_choice == 'all':\n","    selected_providers = [p for p in ['moorcheh', 'pinecone', 'elasticsearch'] if (p in clients or p == 'elasticsearch' and es_client)]\n","else:\n","    provider_map = {'1': 'moorcheh', '2': 'pinecone', '3': 'elasticsearch'}\n","    selected_providers = [provider_map[p.strip()] for p in provider_choice.split(',') if p.strip() in provider_map and (provider_map[p.strip()] in clients or (provider_map[p.strip()] == 'elasticsearch' and es_client))]\n","\n","print(f\"\\nâœ… Selected providers: {', '.join(selected_providers)}\")\n","\n","# 10. Dataset Discovery and Selection Loop\n","# This section allows the user to interactively select which datasets to run the\n","# benchmark on. The benchmark will execute for each selected dataset and provider combination.\n","print(\"\\nðŸ“Š Discovering datasets...\")\n","mair_datasets, datasets_by_category, dataset_sizes = get_mair_datasets()\n","\n","if not mair_datasets:\n","    print(\"âŒ No datasets found!\")\n","    exit(1)\n","\n","print(f\"\\nðŸ“‚ Available datasets by category ({len(mair_datasets)} total):\")\n","dataset_index = 1\n","dataset_map = {}\n","\n","for category in sorted(datasets_by_category.keys()):\n","    datasets = datasets_by_category[category]\n","    if datasets:\n","        sorted_datasets = sorted(datasets, key=lambda d: dataset_sizes.get(d, 0), reverse=True)\n","        print(f\"\\n  ðŸ“ {category} ({len(sorted_datasets)} datasets):\")\n","        for dataset in sorted_datasets:\n","            size_str = format_size(dataset_sizes.get(dataset, 0))\n","            print(f\"     {dataset_index}. {dataset} ({size_str} docs)\")\n","            dataset_map[dataset_index] = dataset\n","            dataset_index += 1\n","\n","all_results = []\n","\n","while True:\n","    choice = input(f\"\\nâž¡ï¸ Enter dataset number (1-{len(dataset_map)}) or 'stop': \").strip().lower()\n","    if choice == \"stop\":\n","        break\n","\n","    try:\n","        idx = int(choice)\n","        if idx in dataset_map:\n","            dataset_name = dataset_map[idx]\n","        else:\n","            print(f\"âš ï¸ Invalid number\")\n","            continue\n","    except ValueError:\n","        print(\"âš ï¸ Invalid input\")\n","        continue\n","\n","    results = run_benchmark_all_providers(dataset_name, selected_providers, es_client=es_client)\n","    all_results.extend(results)\n","\n","# 11. Final Summary and Analysis\n","# After all selected datasets are processed, this section provides an overall summary\n","# of the benchmark results. It includes tables of key metrics, average performance\n","# comparisons by provider, and insights into space efficiency and timing.\n","\n","if all_results:\n","    print(f\"\\n{'='*70}\")\n","    print(\"ðŸ BENCHMARK COMPLETE!\")\n","    print(f\"{'='*70}\")\n","\n","    df = pd.DataFrame(all_results)\n","    print(f\"\\nðŸ’¾ Results saved to: {CSV_PATH}\")\n","\n","    print(\"\\nðŸ“Š Summary Table:\")\n","    summary_cols = ['Dataset', 'Category', 'Provider', 'NDCG@10', 'MAP@10', 'Recall@100', 'Search_Server_Mean_ms']\n","    print(df[summary_cols].to_string(index=False))\n","\n","    print(\"\\nðŸ“ˆ Average Performance by Provider:\")\n","    numeric_cols = ['NDCG@10', 'MAP@10', 'Recall@100', 'Search_Server_Mean_ms', 'Search_Mean_ms']\n","    valid_cols = [col for col in numeric_cols if col in df.columns]\n","    avg_df = df.groupby('Provider')[valid_cols].mean()\n","    print(avg_df.to_string())\n","\n","    print(\"\\nðŸ† Overall Performance Winners:\")\n","    for metric in ['NDCG@10', 'MAP@10', 'Recall@100']:\n","        if metric in df.columns:\n","            winner_data = df.groupby('Provider')[metric].mean()\n","            if len(winner_data) > 0:\n","                winner = winner_data.idxmax()\n","                winner_score = winner_data.max()\n","                print(f\"  {metric}: {winner.upper()} ({winner_score:.4f})\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"âœ¨ Benchmark Complete - Results saved to CSV!\")\n","    print(\"=\"*70)\n","else:\n","    print(\"\\nâš ï¸ No results to display\")"],"metadata":{"id":"9AF-1GPKOwkB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vector (Non-Binary) Search with PGVector and PostgreSQL in Google Colab"],"metadata":{"id":"vtVNnyytSBg6"}},{"cell_type":"code","source":["# ============================================================\n","# MAIR Benchmark - PGVector Edition (Google Colab Optimized)\n","# Comprehensive vector search benchmarking using PostgreSQL + pgvector\n","# ============================================================\n","\n","# -------------------- STEP 1: Install PostgreSQL & pgvector in Colab ---------------------\n","# This section sets up the PostgreSQL database server and installs the pgvector extension\n","# directly within the Google Colab environment. This is a one-time setup for the session.\n","print(\"ðŸ”§ Installing PostgreSQL and pgvector in Google Colab...\")\n","print(\"This will take 2-3 minutes on first run...\n","\")\n","\n","import os\n","import subprocess\n","import time\n","\n","# Install PostgreSQL server and client utilities.\n","# `apt-get update -qq`: Updates package lists quietly.\n","# `apt-get install -y postgresql postgresql-contrib`: Installs PostgreSQL server and extensions.\n","print(\"ðŸ“¦ Installing PostgreSQL...\")\n","os.system('apt-get update -qq > /dev/null 2>&1')\n","os.system('apt-get install -y postgresql postgresql-contrib > /dev/null 2>&1')\n","\n","# Install build dependencies for pgvector (needed to compile from source).\n","# `build-essential`: Provides compilers (gcc, g++).\n","# `git`: To clone the pgvector repository.\n","# `postgresql-server-dev-14`: Development headers for PostgreSQL 14 (adjust version if using a different PG version).\n","print(\"ðŸ“¦ Installing build tools...\")\n","os.system('apt-get install -y build-essential git postgresql-server-dev-14 > /dev/null 2>&1')\n","\n","# Clone and install pgvector extension by compiling from source.\n","# This ensures compatibility and access to the latest features.\n","print(\"ðŸ“¦ Installing pgvector extension...\")\n","os.system('cd /tmp && rm -rf pgvector && git clone --quiet https://github.com/pgvector/pgvector.git')\n","os.system('cd /tmp/pgvector && make > /dev/null 2>&1 && make install > /dev/null 2>&1')\n","\n","# Start PostgreSQL service.\n","print(\"ðŸš€ Starting PostgreSQL service...\")\n","os.system('service postgresql start > /dev/null 2>&1')\n","time.sleep(2) # Give the service a moment to fully start up.\n","\n","# Configure the PostgreSQL database.\n","# `sudo -u postgres`: Executes commands as the 'postgres' user, which has administrative privileges.\n","# `DROP DATABASE IF EXISTS vectordb`: Ensures a clean slate for the 'vectordb' database.\n","# `CREATE DATABASE vectordb`: Creates a new database named 'vectordb' for our vector data.\n","# `ALTER USER postgres PASSWORD 'postgres'`: Sets a simple password for the default 'postgres' user.\n","# `CREATE EXTENSION IF NOT EXISTS vector`: Enables the pgvector extension within the 'vectordb'.\n","print(\"ðŸ”§ Configuring database...\")\n","os.system('sudo -u postgres psql -c \"DROP DATABASE IF EXISTS vectordb;\" > /dev/null 2>&1')\n","os.system('sudo -u postgres psql -c \"CREATE DATABASE vectordb;\" > /dev/null 2>&1')\n","os.system('sudo -u postgres psql -c \"ALTER USER postgres PASSWORD \\'postgres\\';\" > /dev/null 2>&1')\n","os.system('sudo -u postgres psql -d vectordb -c \"CREATE EXTENSION IF NOT EXISTS vector;\" > /dev/null 2>&1')\n","\n","print(\"âœ… PostgreSQL with pgvector is ready!\n","\")\n","\n","# -------------------- STEP 2: Install Python Dependencies ---------------------\n","# Install Python packages required for connecting to PostgreSQL, generating embeddings, and data analysis.\n","# `psycopg2-binary`: PostgreSQL adapter for Python.\n","# `pgvector`: Python client for pgvector.\n","# `cohere`: For generating embeddings.\n","# `pandas`, `numpy`, `tqdm`: Standard data science libraries.\n","print(\"ðŸ“¦ Installing Python packages...\")\n","os.system('pip install -q psycopg2-binary pgvector cohere pandas numpy tqdm')\n","print(\"âœ… Python packages installed!\n","\")\n","\n","# -------------------- STEP 3: Import Libraries ---------------------\n","# Import all necessary Python libraries for the benchmark script.\n","import gc # For garbage collection to manage memory.\n","import json # For handling JSONL dataset files.\n","import statistics # For calculating performance statistics.\n","import numpy as np # For numerical operations, especially with embeddings.\n","import pandas as pd # For data manipulation and CSV output.\n","from tqdm import tqdm # For displaying progress bars during long operations.\n","import psycopg2 # The Python PostgreSQL adapter.\n","import psycopg2.extras # For advanced psycopg2 features like execute_values.\n","from pgvector.psycopg2 import register_vector # Utility to register vector type with psycopg2.\n","\n","# -------------------- Environment Setup ---------------------\n","# This section handles Google Drive mounting and API key retrieval,\n","# adapting to whether the notebook is run in Colab or a local environment.\n","\n","# DRIVE_PATH: Defines where benchmark results will be saved. Default is the current directory.\n","# If running in Google Colab, it will be automatically updated to a path in your mounted Google Drive.\n","DRIVE_PATH = \".\"\n","\n","# MAIR_COMBINED_PATH: **User Customizable Path**\n","# This is the path to your combined MAIR datasets in Google Drive. It should match the SAVE_PATH from the\n","# 'Combine MAIR Docs and Queries' step. Ensure this path is correct for your setup.\n","MAIR_COMBINED_PATH = \"/content/gdrive/MyDrive/Moorcheh/MAIR_Datasets/MAIR-Combined\"\n","\n","try:\n","    # This block attempts to configure for Google Colab, leveraging its `drive` and `userdata` (Secrets) features.\n","    from google.colab import drive, userdata as colab_userdata\n","\n","    try:\n","        # Mount Google Drive to allow Colab to access your files.\n","        # `force_remount=True` ensures a fresh mount if needed, otherwise Colab might remember the authentication.\n","        drive.mount('/content/gdrive')\n","        print(\"âœ… Google Drive mounted successfully\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Drive mount warning: {e}\")\n","        print(\"Continuing without Drive mount...\")\n","\n","    # DRIVE_PATH: **User Customizable Path**\n","    # If running in Colab, results will be saved here within your Google Drive.\n","    # You can customize this path (e.g., '/content/gdrive/MyDrive/MyProject/BenchmarkResults')\n","    # to organize your benchmark outputs effectively.\n","    DRIVE_PATH = '/content/gdrive/MyDrive/Moorcheh/Benchmark_Results/MAIR.PGVector.Results'\n","    os.makedirs(DRIVE_PATH, exist_ok=True) # Creates the directory if it doesn't exist.\n","    print(f\"âœ… Running in Colab. Benchmark results will be saved to: {DRIVE_PATH}\")\n","\n","    # Retrieve COHERE_API_KEY from Colab secrets. **User Action Required**:\n","    # To use this, you need to add your API key to Colab's \"Secrets\" panel.\n","    # Look for the key icon (ðŸ”‘) on the left sidebar of your Colab notebook.\n","    # Name your secret EXACTLY as `COHERE_API_KEY`.\n","    COHERE_API_KEY = colab_userdata.get('COHERE_API_KEY')\n","\n","except ImportError:\n","    # This block runs if not in Google Colab (e.g., local Python environment).\n","    DRIVE_PATH = \".\" # Results will be saved in the current working directory.\n","    print(\"âš ï¸ Not running in Google Colab. Results will be saved locally. Ensure COHERE_API_KEY is set as an environment variable.\")\n","    # Retrieve COHERE_API_KEY from environment variables.\n","    COHERE_API_KEY = os.environ.get('COHERE_API_KEY')\n","\n","# PostgreSQL connection parameters.\n","# These define how the script connects to the local PostgreSQL instance set up in Step 1.\n","PG_CONN_PARAMS = {\n","    'host': 'localhost',\n","    'port': '5432',\n","    'database': 'vectordb',\n","    'user': 'postgres',\n","    'password': 'postgres'\n","}\n","\n","# -------------------- Configuration Parameters ---------------------\n","# These parameters control various aspects of the benchmark. Users can adjust these values\n","# to customize the benchmark's behavior, performance, and resource usage.\n","\n","# TOP_K_SEARCH: **User Customizable Value**\n","# The number of top-ranked results to retrieve from the vector database for each query.\n","# A higher value might improve recall but generally increases search latency and resource usage.\n","TOP_K_SEARCH = 100\n","\n","# K_VALUES: **User Customizable Value**\n","# A list of 'k' values at which retrieval metrics (NDCG, MAP, Recall, Precision) will be calculated.\n","# These values define the cut-off points for evaluation (e.g., NDCG@1, MAP@10, Recall@100).\n","# You can add or remove values based on your evaluation needs.\n","K_VALUES = [1, 3, 5, 10, 100]\n","\n","# MAX_UPLOAD_DOCS: **User Customizable Value**\n","# Limits the number of documents uploaded to pgvector. This is crucial for managing costs\n","# and execution time, especially with very large datasets. Set to a lower number (e.g., 10000)\n","# for quick tests or a very high number (e.g., 700000) for comprehensive runs. Set to `None`\n","# or a number greater than your dataset size to upload all documents.\n","MAX_UPLOAD_DOCS = 700000\n","\n","# BATCH_SIZE: **User Customizable Value**\n","# The number of embeddings to process or upload in a single API request/batch.\n","# Adjusting this value can significantly impact performance, memory usage, and API rate limits.\n","# Larger batches are generally faster due to reduced overhead but consume more RAM.\n","BATCH_SIZE = 100\n","\n","# EMBEDDING_MODEL: **User Customizable Value**\n","# The Cohere model used to generate the dense float embeddings.\n","# 'embed-v4.0' is recommended for its performance and higher dimensionality. Other options include\n","# 'embed-english-v3.0', 'embed-multilingual-v3.0', etc. Changing this will require adjusting VECTOR_DIMENSION.\n","EMBEDDING_MODEL = \"embed-v4.0\"\n","\n","# INPUT_TYPE_CORPUS: Specifies the input type for corpus documents to Cohere's embedding model.\n","# This helps Cohere optimize embedding generation for different content types (e.g., 'search_document').\n","INPUT_TYPE_CORPUS = \"search_document\"\n","\n","# INPUT_TYPE_QUERY: Specifies the input type for queries to Cohere's embedding model.\n","# Similar to corpus input type, this optimizes query embedding generation (e.g., 'search_query').\n","INPUT_TYPE_QUERY = \"search_query\"\n","\n","# VECTOR_DIMENSION: **User Customizable Value (Must match EMBEDDING_MODEL)**\n","# The dimensionality of the generated embeddings. This value is critical and MUST match the output\n","# dimension of the `EMBEDDING_MODEL` you choose. Cohere's 'embed-v4.0' has 1536 dimensions;\n","# 'embed-v3.0' has 1024 dimensions. Incorrect dimension will lead to errors in pgvector.\n","VECTOR_DIMENSION = 1536\n","\n","# CSV_PATH: **User Customizable Path**\n","# The full path where the final benchmark results will be saved in CSV format.\n","# This file will be created or appended to in your Google Drive (or local directory).\n","CSV_PATH = os.path.join(DRIVE_PATH, \"MAIR.PGVector.Cohere.V4.csv\")\n","\n","# -------------------- Cleanup Policy ---------------------\n","# This section allows the user to define how the PostgreSQL tables created by pgvector\n","# are handled after each benchmark run. This helps manage local storage and state.\n","print(\"\\nðŸ§¹ Table cleanup policy options:\")\n","print(\"  1) ask_each_time  -> Prompt after each dataset (default)\")\n","print(\"  2) always_delete  -> Automatically delete after benchmarking\")\n","print(\"  3) always_keep    -> Never delete (keep for future searches or debugging)\")\n","\n","# âž¡ï¸ USER CONFIGURATION: Choose your preferred cleanup policy.\n","cleanup_choice = input(\"Choose policy [1/2/3] (default 1): \").strip() or \"1\"\n","CLEANUP_POLICY_MAP = {\"1\": \"ask_each_time\", \"2\": \"always_delete\", \"3\": \"always_keep\"}\n","CLEANUP_POLICY = CLEANUP_POLICY_MAP.get(cleanup_choice, \"ask_each_time\")\n","print(f\"âœ… Selected cleanup policy: {CLEANUP_POLICY}\")\n","\n","# -------------------- Helper Functions ---------------------\n","# These functions facilitate data loading, processing, and metric calculation for the benchmark.\n","\n","def load_jsonl(filepath):\n","    \"\"\"Loads data from a JSONL (JSON Lines) file into a dictionary.\n","    It intelligently identifies various common ID field names to create a mapping\n","    from item ID to its content. This is flexible for MAIR's varied formats.\n","    \"\"\"\n","    data = {}\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip():\n","                    item = json.loads(line)\n","                    item_id = item.get('_id') or item.get('id') or item.get('query_id') or item.get('doc_id')\n","                    if item_id:\n","                        data[str(item_id)] = item\n","                    else:\n","                        data[str(len(data))] = item\n","        print(f\"   Loaded {len(data)} items from {os.path.basename(filepath)}\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Error loading {filepath}: {e}\")\n","    return data\n","\n","def extract_qrels_from_queries(queries):\n","    \"\"\"Extracts Qrels (query-relevance judgments) from query data structures.\n","    This function is tailored for MAIR datasets where qrels might be embedded\n","    within query files, often in fields like 'labels', 'relevance', or 'qrels'.\n","    It supports different formats for these fields (list of dicts, list of IDs, or dict).\n","    \"\"\"\n","    qrels = {}\n","    for qid, q in queries.items():\n","        if isinstance(q, dict):\n","            labels = q.get('labels') or q.get('relevance') or q.get('qrels')\n","            if labels:\n","                qrels[str(qid)] = {}\n","                if isinstance(labels, list):\n","                    for label_item in labels:\n","                        if isinstance(label_item, dict):\n","                            doc_id = label_item.get('id') or label_item.get('doc_id')\n","                            score = label_item.get('score', 1)\n","                            if doc_id:\n","                                qrels[str(qid)][str(doc_id)] = score\n","                        else:\n","                            qrels[str(qid)][str(label_item)] = 1\n","                elif isinstance(labels, dict):\n","                    for doc_id, score in labels.items():\n","                        qrels[str(qid)][str(doc_id)] = score\n","    return qrels\n","\n","# Predefined categories for MAIR datasets, used to organize their display to the user.\n","# This mapping helps in presenting a structured and navigable list of available datasets.\n","DATASET_CATEGORIES = {\n","    \"Legal & Regulatory\": [\"ACORDAR\", \"AILA2019-Case\", \"AILA2019-Statutes\", \"CUAD\", \"LeCaRDv2\", \"LegalQuAD\", \"REGIR-EU2UK\", \"REGIR-UK2EU\"],\n","    \"Medical & Clinical\": [\"CliniDS-2014\", \"CliniDS-2015\", \"CliniDS-2016\", \"ClinicalTrials-2021\", \"ClinicalTrials-2022\", \"ClinicalTrials-2023\", \"NFCorpus\", \"PrecisionMedicine\", \"Genomics-AdHoc\"],\n","    \"Code & Programming\": [\"APPS\", \"CodeEditSearch\", \"CodeSearchNet\", \"Conala\", \"HumanEval-X\", \"LeetCode\", \"MBPP\", \"RepoBench\", \"SWE-Bench-Lite\"],\n","    \"Financial\": [\"ConvFinQA\", \"FiQA\", \"FinQA\", \"FinanceBench\", \"HC3Finance\"],\n","    \"Academic & Scientific\": [\"ArguAna\", \"LitSearch\", \"ProofWiki-Proof\", \"ProofWiki-Reference\", \"Competition-Math\"],\n","    \"Conversational & Dialog\": [\"CAsT-2019\", \"CAsT-2020\", \"CAsT-2021\", \"CAsT-2022\", \"ProCIS-Dialog\", \"ProCIS-Turn\", \"SParC\", \"Quora\"],\n","    \"News & Social Media\": [\"ChroniclingAmericaQA\", \"Microblog-2011\", \"Microblog-2012\", \"Microblog-2013\", \"Microblog-2014\", \"News21\"],\n","    \"API Documentation\": [\"Apple\", \"FoodAPI\", \"HuggingfaceAPI\", \"PytorchAPI\"],\n","    \"Others\": [\"BSARD\", \"BillSum\", \"CARE\", \"CPCD\", \"CQADupStack\", \"DD\", \"ELI5\", \"ExcluIR\", \"FairRanking\", \"Fever\", \"GerDaLIR\", \"IFEval\", \"InstructIR\", \"MISeD\", \"Monant\", \"NTCIR\", \"NeuCLIR\", \"NevIR\", \"PointRec\", \"ProductSearch_2023\", \"QuanTemp\", \"Robust04\"]\n","}\n","\n","def get_dataset_category(dataset_name):\n","    \"\"\"Assigns a given dataset name to its predefined category.\"\"\"\n","    for category, datasets in DATASET_CATEGORIES.items():\n","        if dataset_name in datasets:\n","            return category\n","    return \"Others\"\n","\n","def get_dataset_size(dataset_name):\n","    \"\"\"Counts the number of documents in a MAIR dataset by reading its JSONL files.\"\"\"\n","    dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","    docs_path = os.path.join(dataset_path, 'docs')\n","    if not os.path.exists(docs_path):\n","        return 0\n","    doc_count = 0\n","    try:\n","        docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","        for file in docs_files:\n","            file_path = os.path.join(docs_path, file)\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                for line in f:\n","                    if line.strip():\n","                        doc_count += 1\n","    except Exception as e:\n","        print(f\"âš ï¸ Error counting docs in {dataset_name}: {e}\")\n","    return doc_count\n","\n","def format_size(num):\n","    \"\"\"Formats a number (document count) with K (thousands) or M (millions) suffix for readability.\"\"\"\n","    if num >= 1_000_000:\n","        return f\"{num/1_000_000:.1f}M\"\n","    elif num >= 1_000:\n","        return f\"{num/1_000:.1f}K\"\n","    return str(num)\n","\n","def get_mair_datasets():\n","    \"\"\"Discovers available MAIR datasets in the combined path, categorizes them, and gets their sizes.\"\"\"\n","    datasets_by_category = {category: [] for category in DATASET_CATEGORIES.keys()}\n","    all_datasets = []\n","    dataset_sizes = {}\n","    if os.path.exists(MAIR_COMBINED_PATH):\n","        print(f\"ðŸ” Scanning: {MAIR_COMBINED_PATH}\")\n","        try:\n","            items = os.listdir(MAIR_COMBINED_PATH)\n","            for item in sorted(items):\n","                if item.startswith('.') or item.startswith('_'):\n","                    continue\n","                dataset_path = os.path.join(MAIR_COMBINED_PATH, item)\n","                if not os.path.isdir(dataset_path):\n","                    continue\n","                docs_path = os.path.join(dataset_path, 'docs')\n","                queries_path = os.path.join(dataset_path, 'queries')\n","                if os.path.exists(docs_path) and os.path.exists(queries_path):\n","                    docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","                    queries_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","                    if docs_files and queries_files:\n","                        all_datasets.append(item)\n","                        category = get_dataset_category(item)\n","                        datasets_by_category[category].append(item)\n","                        doc_count = get_dataset_size(item)\n","                        dataset_sizes[item] = doc_count\n","                        print(f\"   âœ… {item} [{category}] - {format_size(doc_count)} docs\")\n","        except Exception as e:\n","            print(f\"   âŒ Error scanning directory: {e}\")\n","    return all_datasets, datasets_by_category, dataset_sizes\n","\n","def calculate_timing_stats(timing_list):\n","    \"\"\"Calculates basic descriptive statistics (mean, median, min, max, std dev, total sum)\n","    for a given list of numerical timings. This is used to summarize performance metrics\n","    like search times, upload durations, etc., providing a quick overview of performance.\n","    \"\"\"\n","    if not timing_list:\n","        return {\"mean\": 0.0, \"median\": 0.0, \"min\": 0.0, \"max\": 0.0, \"std\": 0.0, \"total\": 0.0}\n","    return {\n","        \"mean\": statistics.mean(timing_list),\n","        \"median\": statistics.median(timing_list),\n","        \"min\": min(timing_list),\n","        \"max\": max(timing_list),\n","        \"std\": statistics.stdev(timing_list) if len(timing_list) > 1 else 0.0,\n","        \"total\": sum(timing_list)\n","    }\n","\n","def calculate_retrieval_metrics(retrieved_results, qrels, k_values=K_VALUES):\n","    \"\"\"Calculates NDCG, MAP, Recall, and Precision @K for given retrieval results and ground truth qrels.\"\"\"\n","    ndcg_scores = {f\"NDCG@{k}\": [] for k in k_values}\n","    map_scores = {f\"MAP@{k}\": [] for k in k_values}\n","    recall_scores = {f\"Recall@{k}\": [] for k in k_values}\n","    precision_scores = {f\"P@{k}\": [] for k in k_values}\n","\n","    for qid in retrieved_results:\n","        if qid not in qrels:\n","            continue\n","        relevant_docs = set(qrels[qid].keys())\n","        retrieved_docs = list(retrieved_results[qid].keys())\n","        if len(relevant_docs) == 0:\n","            continue\n","\n","        for k in k_values:\n","            top_k = retrieved_docs[:k]\n","            hits = len(set(top_k) & relevant_docs)\n","            recall = hits / len(relevant_docs) if len(relevant_docs) > 0 else 0.0\n","            recall_scores[f\"Recall@{k}\"].append(recall)\n","            precision = hits / k if k > 0 else 0.0\n","            precision_scores[f\"P@{k}\"].append(precision)\n","\n","            ap = 0.0\n","            hits_so_far = 0\n","            for i, doc_id in enumerate(top_k, 1):\n","                if doc_id in relevant_docs:\n","                    hits_so_far += 1\n","                    ap += hits_so_far / i\n","            ap = ap / min(len(relevant_docs), k) if len(relevant_docs) > 0 else 0.0\n","            map_scores[f\"MAP@{k}\"].append(ap)\n","\n","            dcg = 0.0\n","            for i, doc_id in enumerate(top_k, 1):\n","                if doc_id in relevant_docs:\n","                    dcg += 1.0 / np.log2(i + 1)\n","            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), k)))\n","            ndcg = dcg / idcg if idcg > 0 else 0.0\n","            ndcg_scores[f\"NDCG@{k}\"].append(ndcg)\n","\n","    ndcg_avg = {k: np.mean(v) if v else 0.0 for k, v in ndcg_scores.items()}\n","    map_avg = {k: np.mean(v) if v else 0.0 for k, v in map_scores.items()}\n","    recall_avg = {k: np.mean(v) if v else 0.0 for k, v in recall_scores.items()}\n","    precision_avg = {k: np.mean(v) if v else 0.0 for k, v in precision_scores.items()}\n","    return ndcg_avg, map_avg, recall_avg, precision_avg\n","\n","def format_and_print_metrics(ndcg, _map, recall, precision, k_values=K_VALUES):\n","    \"\"\"Formats and prints retrieval metrics (NDCG, MAP, Recall, Precision) in a clean,\n","    tabular format for specified K values. This provides an immediate, human-readable\n","    summary of the retrieval quality for a benchmark run.\n","    \"\"\"\n","    print(\"\\nRetrieval Metrics:\")\n","    print(\"â”€\" * 80)\n","    for k in k_values:\n","        print(f\"NDCG@{k}: {ndcg.get(f'NDCG@{k}', 0.0):.4f} | MAP@{k}: {_map.get(f'MAP@{k}', 0.0):.4f} | Recall@{k}: {recall.get(f'Recall@{k}', 0.0):.4f} | P@{k}: {precision.get(f'P@{k}', 0.0):.4f}\")\n","\n","def extract_all_metrics(ndcg, _map, recall, precision, k_values=K_VALUES):\n","    \"\"\"Extracts all relevant retrieval metrics into a single dictionary.\n","    This structured format is ideal for storage, particularly for CSV output,\n","    ensuring all evaluation results are consistently captured.\n","    \"\"\"\n","    metrics = {}\n","    for k in k_values:\n","        metrics[f\"NDCG@{k}\"] = float(ndcg.get(f\"NDCG@{k}\", 0.0)) if ndcg.get(f\"NDCG@{k}\") is not None else 0.0\n","        metrics[f\"MAP@{k}\"] = float(_map.get(f\"MAP@{k}\", 0.0))\n","        metrics[f\"Recall@{k}\"] = float(recall.get(f\"Recall@{k}\", 0.0))\n","        metrics[f\"P@{k}\"] = float(precision.get(f\"P@{k}\", 0.0))\n","    return metrics\n","\n","def save_results_to_csv(new_result: dict, csv_path: str):\n","    \"\"\"Appends a new benchmark result entry to a CSV file. If the file doesn't exist,\n","    it creates it along with the header. Otherwise, it appends the new data,\n","    ensuring data integrity and continuity of results over multiple runs.\n","    \"\"\"\n","    new_df = pd.DataFrame([new_result])\n","    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n","    write_header = not os.path.exists(csv_path)\n","    try:\n","        if write_header:\n","            new_df.to_csv(csv_path, mode='w', header=True, index=False)\n","        else:\n","            existing_df = pd.read_csv(csv_path)\n","            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n","            combined_df.to_csv(csv_path, mode='w', header=True, index=False)\n","        print(f\"ðŸ’¾ Saved to: {csv_path}\")\n","    except Exception as e:\n","        print(f\"âŒ CSV save failed: {e}\")\n","\n","def clean_memory():\n","    \"\"\"Forces Python's garbage collector to release memory.\n","    This is particularly important in resource-constrained environments like Colab,\n","    especially when processing large datasets, to prevent out-of-memory errors.\n","    \"\"\"\n","    gc.collect()\n","\n","def should_cleanup_table(table_name, dataset_name):\n","    \"\"\"Determines whether to delete the PGVector table based on the user-selected cleanup policy.\"\"\"\n","    if CLEANUP_POLICY == \"always_delete\":\n","        return True # Automatically delete without asking\n","    elif CLEANUP_POLICY == \"always_keep\":\n","        return False # Never delete, keep the table\n","    else: # CLEANUP_POLICY == \"ask_each_time\"\n","        response = input(f\"\\nâ“ Delete PGVector table {table_name} for {dataset_name}? (y/n): \").strip().lower()\n","        return response in ['y', 'yes']\n","\n","# -------------------- PGVector Provider Class ---------------------\n","# This class encapsulates the specific logic for interacting with PostgreSQL\n","# and the pgvector extension for dense vector benchmarking.\n","\n","class PGVectorProvider:\n","    \"\"\"Manages interaction with PostgreSQL and pgvector for non-binary vector benchmarking.\"\"\"\n","    def __init__(self, conn_params, table_name, precomputed_vectors, query_embeddings, vector_dim=1536):\n","        self.conn_params = conn_params # Dictionary of PostgreSQL connection parameters\n","        self.table_name = table_name # Name of the table to create/use in PostgreSQL\n","        self.precomputed_vectors = precomputed_vectors # List of documents with their IDs and float vectors\n","        self.query_embeddings = query_embeddings # List of query float embeddings\n","        self.vector_dim = vector_dim # Dimension of the vectors (length of the float array)\n","        self.conn = None # Placeholder for the database connection object\n","        self.upload_timings = {\n","            \"table_creation_s\": 0.0, # Time to create the database table\n","            \"insert_time_s\": 0.0, # Total time for inserting vectors\n","            \"index_creation_s\": 0.0, # Time to create the HNSW index\n","            \"batch_details\": [] # Detailed timings for each insert batch\n","        }\n","        self.search_timings = [] # Timings for individual search queries\n","\n","    def connect(self):\n","        \"\"\"Establishes a connection to the PostgreSQL database using the provided parameters.\"\"\"\n","        try:\n","            self.conn = psycopg2.connect(**self.conn_params) # Connect to DB\n","            register_vector(self.conn) # Register the pgvector extension with the psycopg2 connection\n","            print(f\"âœ… Connected to PostgreSQL database\")\n","        except Exception as e:\n","            print(f\"âŒ Connection failed: {e}\")\n","            raise # Re-raise to stop execution if connection fails\n","\n","    def upload(self):\n","        \"\"\"Uploads float vectors to the PGVector table using the `vector` data type.\"\"\"\n","        if not self.conn:\n","            self.connect()\n","\n","        cur = self.conn.cursor() # Create a cursor object for executing SQL commands\n","\n","        # Drop table if it exists to ensure a clean benchmark run.\n","        try:\n","            cur.execute(f\"DROP TABLE IF EXISTS {self.table_name}\")\n","            self.conn.commit() # Commit the transaction\n","        except Exception as e:\n","            self.conn.rollback()\n","\n","        # Create table with `vector(N)` type for dense float embeddings.\n","        t0 = time.time() # Start timer for table creation\n","        try:\n","            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\") # Ensure pgvector extension is enabled\n","            cur.execute(f\"\"\"\n","                CREATE TABLE {self.table_name} (\n","                    id TEXT PRIMARY KEY,                       -- Document ID\n","                    embedding vector({self.vector_dim})           -- Float embedding stored as a vector array\n","                )\n","            \"\"\")\n","            self.conn.commit() # Commit the table creation\n","            t1 = time.time() # End timer\n","            self.upload_timings[\"table_creation_s\"] = t1 - t0\n","            print(f\"âœ… Created table: {self.table_name} (dim: {self.vector_dim})\")\n","        except Exception as e:\n","            print(f\"âŒ Table creation failed: {e}\")\n","            self.conn.rollback()\n","            raise\n","\n","        # Insert vectors in batches using `execute_values` for efficiency.\n","        batch_num = 0\n","        total_inserted = 0\n","        t_insert_start = time.time() # Start timer for total insert time\n","\n","        for i in tqdm(range(0, len(self.precomputed_vectors), BATCH_SIZE), desc=\"Uploading to PGVector\"):\n","            batch = self.precomputed_vectors[i:i+BATCH_SIZE] # Get a batch of vectors\n","            batch_num += 1\n","\n","            t_batch_start = time.time() # Start timer for current batch\n","            try:\n","                # Prepare data for `execute_values`: list of tuples (id, vector_list).\n","                values = [(v['id'], v['vector']) for v in batch]\n","                # `psycopg2.extras.execute_values` is much faster than individual INSERTs.\n","                # `template=\"(%s, %s::vector)\"` specifies how the values are formatted for the SQL query.\n","                # `::vector` casts the Python list to a pgvector array type.\n","                psycopg2.extras.execute_values(\n","                    cur,\n","                    f\"INSERT INTO {self.table_name} (id, embedding) VALUES %s\",\n","                    values,\n","                    template=\"(%s, %s::vector)\"\n","                )\n","                self.conn.commit() # Commit the batch insert\n","\n","                t_batch_end = time.time() # End timer for current batch\n","                batch_time = t_batch_end - t_batch_start\n","                total_inserted += len(batch)\n","\n","                self.upload_timings[\"batch_details\"].append({\n","                    \"batch_num\": batch_num,\n","                    \"batch_size\": len(batch),\n","                    \"insert_time_s\": batch_time\n","                })\n","\n","            except Exception as e:\n","                print(f\"\\nâŒ Batch {batch_num} failed: {e}\")\n","                self.conn.rollback() # Rollback the current batch on error\n","\n","        t_insert_end = time.time() # End timer for total insert time\n","        self.upload_timings[\"insert_time_s\"] = t_insert_end - t_insert_start\n","\n","        # Create HNSW index for Approximate Nearest Neighbor (ANN) search.\n","        # `vector_cosine_ops` tells pgvector to use cosine similarity for this index type.\n","        # `m` and `ef_construction` are HNSW parameters influencing index quality and build time.\n","        print(f\"\\nðŸ”§ Creating HNSW index...\")\n","        t_index_start = time.time() # Start timer for index creation\n","        try:\n","            # âž¡ï¸ USER CONFIGURATION: You can adjust `m` and `ef_construction` for different performance/accuracy tradeoffs.\n","            # Higher `m` and `ef_construction` lead to better accuracy but longer build times and more memory.\n","            cur.execute(f\"\"\"\n","                CREATE INDEX ON {self.table_name}\n","                USING hnsw (embedding vector_cosine_ops)\n","                WITH (m = 16, ef_construction = 64)\n","            \"\"\") # SQL command to create HNSW index\n","            self.conn.commit() # Commit the index creation\n","            t_index_end = time.time() # End timer\n","            self.upload_timings[\"index_creation_s\"] = t_index_end - t_index_start\n","            print(f\"âœ… HNSW index created in {self.upload_timings['index_creation_s']:.4f}s\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Index creation warning: {e}. Indexing is important for search performance. Proceeding without index.\")\n","            self.conn.rollback() # Rollback on error\n","\n","        cur.close() # Close the cursor\n","\n","        print(f\"\\nâ±ï¸  PGVector Upload Timing:\")\n","        print(f\"    Table Creation: {self.upload_timings['table_creation_s']:.4f}s\")\n","        print(f\"    Insert Total: {self.upload_timings['insert_time_s']:.4f}s\")\n","        print(f\"    Index Creation: {self.upload_timings['index_creation_s']:.4f}s\")\n","        print(f\"    Total: {sum([self.upload_timings['table_creation_s'], self.upload_timings['insert_time_s'], self.upload_timings['index_creation_s']]):.4f}s\")\n","\n","        return total_inserted # Return count of successfully inserted vectors\n","\n","    def search(self, query_idx, top_k=100):\n","        \"\"\"Performs a vector similarity search using cosine similarity in pgvector.\"\"\"\n","        if not self.conn:\n","            self.connect()\n","\n","        query_embedding = self.query_embeddings[query_idx] # Get the float query vector\n","        cur = self.conn.cursor() # Create a cursor\n","\n","        t_start = time.time() # Start timer for search query\n","        try:\n","            # Use `<=>` operator for cosine distance (1 - cosine similarity) in pgvector.\n","            # The query string `::vector` casts the Python list to a vector type for comparison.\n","            # We select `id` and calculate similarity as `1 - distance`.\n","            # Results are ordered by `embedding <=> query` (lower distance = more similar).\n","            cur.execute(f\"\"\"\n","                SELECT id, 1 - (embedding <=> %s::vector) as similarity\n","                FROM {self.table_name}\n","                ORDER BY embedding <=> %s::vector\n","                LIMIT %s\n","            \"\"\", (query_embedding, query_embedding, top_k))\n","\n","            results = cur.fetchall() # Fetch all results\n","            t_end = time.time() # End timer\n","\n","            search_time = t_end - t_start\n","            self.search_timings.append({\"search_time_s\": search_time}) # Record search time\n","\n","            # Format results into a dictionary of {doc_id: score}.\n","            return {str(row[0]): float(row[1]) for row in results}\n","\n","        except Exception as e:\n","            print(f\"\\nâŒ Search error: {e}\")\n","            self.search_timings.append({\"search_time_s\": 0.0}) # Log 0 time for failed searches\n","            return {} # Return empty results on error\n","        finally:\n","            cur.close() # Always close the cursor\n","\n","    def get_search_stats(self):\n","        \"\"\"Calculates and returns search timing statistics for PGVector.\"\"\"\n","        search_times = [t[\"search_time_s\"] for t in self.search_timings] # Extract all search times\n","        return calculate_timing_stats(search_times) # Use helper to calculate stats\n","\n","    def cleanup(self):\n","        \"\"\"Drops the PostgreSQL table created for the benchmark and closes the database connection.\"\"\"\n","        if not self.conn:\n","            return\n","\n","        try:\n","            cur = self.conn.cursor() # Create cursor\n","            cur.execute(f\"DROP TABLE IF EXISTS {self.table_name}\") # Drop table\n","            self.conn.commit() # Commit transaction\n","            cur.close() # Close cursor\n","            print(f\"ðŸ§¹ Deleted table: {self.table_name}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Failed to delete table: {e}\")\n","        finally:\n","            self.conn.close() # Always close the database connection\n","            self.conn = None\n","\n","# -------------------- Main Benchmark Function ---------------------\n","# This function orchestrates the entire benchmarking process for a single dataset\n","# with pgvector, including data loading, embedding generation, upload, search, and evaluation.\n","\n","def run_benchmark_pgvector(dataset_name, pg_conn_params):\n","    \"\"\"Runs a full benchmark for PGVector with float embeddings for a specified MAIR dataset.\"\"\"\n","    print(f\"\\n{'='*70}\")\n","    print(f\"ðŸš€ Dataset: {dataset_name}\")\n","    print(f\"ðŸ“Š Testing PGVector with Cohere {EMBEDDING_MODEL}\")\n","    print(f\"{'='*70}\")\n","\n","    # Construct paths to the dataset within the combined MAIR directory.\n","    dataset_path = os.path.join(MAIR_COMBINED_PATH, dataset_name)\n","    docs_path = os.path.join(dataset_path, 'docs')\n","    queries_path = os.path.join(dataset_path, 'queries')\n","\n","    # Verify dataset files exist before proceeding.\n","    if not os.path.exists(docs_path) or not os.path.exists(queries_path):\n","        print(f\"âš ï¸ Dataset not found\")\n","        return None\n","\n","    print(f\"\\nðŸ“¥ Loading dataset...\")\n","    corpus = {}\n","    docs_files = [f for f in os.listdir(docs_path) if f.endswith('.jsonl')]\n","    for file in docs_files:\n","        corpus.update(load_jsonl(os.path.join(docs_path, file))) # Load document JSONL files\n","\n","    queries = {}\n","    query_files = [f for f in os.listdir(queries_path) if f.endswith('.jsonl')]\n","    for file in query_files:\n","        queries.update(load_jsonl(os.path.join(queries_path, file))) # Load query JSONL files\n","\n","    qrels = extract_qrels_from_queries(queries) # Extract relevance judgments from queries\n","    print(f\"âœ… Loaded: {len(corpus)} docs, {len(queries)} queries, {len(qrels)} qrels\")\n","\n","    if not corpus or not queries or not qrels: # Early exit if essential data is missing\n","        print(f\"âš ï¸ Empty dataset\")\n","        return None\n","\n","    print(f\"\\nðŸ§  Generating embeddings with Cohere {EMBEDDING_MODEL}...\")\n","    # Limit the number of documents to embed and upload, based on MAX_UPLOAD_DOCS configuration.\n","    docs = list(corpus.items())[:MAX_UPLOAD_DOCS]\n","\n","    texts = []\n","    for doc_id, doc_content in docs:\n","        if isinstance(doc_content, dict):\n","            text = None\n","            for field in ['doc', 'text', 'content', 'body', 'passage', 'document', 'title', 'abstract']:\n","                if field in doc_content and doc_content[field]:\n","                    val = doc_content[field]\n","                    if isinstance(val, str) and val.strip():\n","                        text = val\n","                        break\n","            if not text:\n","                for key, val in doc_content.items():\n","                    if isinstance(val, str) and val.strip() and len(val) > 10:\n","                        text = val\n","                        break\n","            if not text:\n","                text = str(doc_content)\n","        else:\n","            text = str(doc_content)\n","\n","        texts.append(text if text else \"document\") # Default text if extraction yields empty string\n","\n","    doc_ids = [str(d[0]) for d in docs] # Ensure all document IDs are strings\n","\n","    print(f\"  ðŸ“„ Generating corpus embeddings for {len(texts)} documents...\")\n","    t_embed_start = time.time() # Start timer for corpus embedding\n","\n","    corpus_embeddings = []\n","    # Batch processing for Cohere API calls for corpus embeddings.\n","    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding corpus\"):\n","        batch_texts = texts[i:i+BATCH_SIZE]\n","        response = cohere_client.embed(texts=batch_texts, model=EMBEDDING_MODEL, input_type=INPUT_TYPE_CORPUS)\n","        corpus_embeddings.extend(response.embeddings)\n","\n","    t_embed_end = time.time() # End timer\n","    embedding_time = t_embed_end - t_embed_start\n","\n","    # Prepare precomputed vectors in the format expected by the PGVector provider.\n","    precomputed_vectors = []\n","    for doc_id, embedding in zip(doc_ids, corpus_embeddings):\n","        precomputed_vectors.append({\"id\": doc_id, \"vector\": embedding})\n","\n","    print(f\"  ðŸ” Generating query embeddings for {len(queries)} queries...\")\n","    query_ids = list(queries.keys())\n","    query_texts = []\n","    for qid in query_ids:\n","        q = queries[qid]\n","        if isinstance(q, dict):\n","            text = (q.get('text') or q.get('query') or q.get('instruction') or q.get('question') or q.get('query_text') or str(q))\n","        else:\n","            text = str(q)\n","        query_texts.append(text if text else \"query\")\n","\n","    t_query_start = time.time() # Start timer for query embedding\n","    query_embeddings = []\n","    # Batch processing for Cohere API calls for query embeddings.\n","    for i in tqdm(range(0, len(query_texts), BATCH_SIZE), desc=\"Embedding queries\"):\n","        batch_texts = [str(t) if t else \"query\" for t in query_texts[i:i+BATCH_SIZE]]\n","        response = cohere_client.embed(texts=batch_texts, model=EMBEDDING_MODEL, input_type=INPUT_TYPE_QUERY)\n","        query_embeddings.extend(response.embeddings)\n","\n","    t_query_end = time.time() # End timer\n","    query_embedding_time = t_query_end - t_query_start\n","\n","    print(f\"\\nâ±ï¸  Embedding Generation Time:\")\n","    print(f\"    Corpus: {embedding_time:.4f}s, Queries: {query_embedding_time:.4f}s, Total: {embedding_time + query_embedding_time:.4f}s\")\n","\n","    # Generate a safe table name for PostgreSQL from the dataset name.\n","    # PostgreSQL table names have a maximum length of 63 characters.\n","    safe_dataset_name = dataset_name.replace('/', '_').replace('-', '_').lower()\n","    table_name = f\"mair_{safe_dataset_name}_v4\"[:63]\n","\n","    try:\n","        # Initialize the PGVector provider class.\n","        provider = PGVectorProvider(\n","            conn_params=pg_conn_params,\n","            table_name=table_name,\n","            precomputed_vectors=precomputed_vectors,\n","            query_embeddings=query_embeddings,\n","            vector_dim=VECTOR_DIMENSION\n","        )\n","\n","        print(f\"\\nðŸ“¤ Uploading vectors to PGVector table: {table_name}...\")\n","        num_uploaded = provider.upload() # Upload vectors to PostgreSQL\n","\n","        print(f\"\\nðŸ” Performing vector similarity search...\")\n","        results = {} # Dictionary to store retrieved results {query_id: {doc_id: score}}\n","        for i, qid in enumerate(tqdm(query_ids, desc=\"Searching PGVector\")):\n","            try:\n","                results[qid] = provider.search(i, top_k=TOP_K_SEARCH)\n","            except Exception as e:\n","                print(f\"\\nâŒ Query {i+1} failed: {e}\")\n","                results[qid] = {} # Log empty results for failed queries\n","\n","        search_stats = provider.get_search_stats() # Get search timing statistics\n","        print(f\"\\nâ±ï¸  PGVector Search Timing:\")\n","        print(f\"    Total: {search_stats['total']:.4f}s\")\n","        print(f\"    Mean: {search_stats['mean']:.4f}s ({search_stats['mean']*1000:.2f}ms)\")\n","        print(f\"    Median: {search_stats['median']:.4f}s\")\n","        print(f\"    Min: {search_stats['min']:.4f}s, Max: {search_stats['max']:.4f}s\")\n","\n","        print(f\"\\nðŸ“Š Evaluating retrieval quality...\")\n","        ndcg, _map, recall, precision = calculate_retrieval_metrics(results, qrels, K_VALUES) # Calculate metrics\n","        format_and_print_metrics(ndcg, _map, recall, precision) # Print formatted metrics\n","\n","        metrics = extract_all_metrics(ndcg, _map, recall, precision) # Extract all metrics into a dictionary\n","        dataset_category = get_dataset_category(dataset_name) # Get dataset category for result entry\n","\n","        result = { # Compile all results and metadata into a dictionary\n","            \"Dataset\": dataset_name,\n","            \"Category\": dataset_category,\n","            \"Provider\": \"pgvector\",\n","            \"Num_Corpus\": len(corpus),\n","            \"Num_Uploaded\": num_uploaded,\n","            \"Num_Queries\": len(queries),\n","            \"Embedding_Model\": EMBEDDING_MODEL,\n","            \"Vector_Dimension\": VECTOR_DIMENSION,\n","            \"Batch_Size\": BATCH_SIZE,\n","            \"Cleanup_Policy\": CLEANUP_POLICY,\n","            \"Embedding_Generation_Time_s\": round(embedding_time, 4),\n","            \"Query_Embedding_Time_s\": round(query_embedding_time, 4),\n","            \"Total_Embedding_Time_s\": round(embedding_time + query_embedding_time, 4),\n","            \"Upload_Table_Creation_s\": round(provider.upload_timings[\"table_creation_s\"], 4),\n","            \"Upload_Insert_Total_s\": round(provider.upload_timings[\"insert_time_s\"], 4),\n","            \"Upload_Index_Creation_s\": round(provider.upload_timings[\"index_creation_s\"], 4),\n","            \"Upload_Total_s\": round(\n","                provider.upload_timings[\"table_creation_s\"] +\n","                provider.upload_timings[\"insert_time_s\"] +\n","                provider.upload_timings[\"index_creation_s\"], 4\n","            ),\n","            \"Search_Total_s\": round(search_stats['total'], 4),\n","            \"Search_Mean_s\": round(search_stats['mean'], 4),\n","            \"Search_Median_s\": round(search_stats['median'], 4),\n","            \"Search_Min_s\": round(search_stats['min'], 4),\n","            \"Search_Max_s\": round(search_stats['max'], 4),\n","            \"Search_Std_s\": round(search_stats['std'], 4),\n","            \"Search_Mean_ms\": round(search_stats['mean'] * 1000, 2),\n","            \"Index_Type\": \"HNSW\",\n","            \"Similarity_Metric\": \"cosine\"\n","        }\n","\n","        result.update(metrics) # Add the retrieval metrics to the result dictionary\n","        save_results_to_csv(result, CSV_PATH) # Save this run's results to the CSV file\n","\n","        should_delete = should_cleanup_table(table_name, dataset_name) # Determine cleanup action based on policy\n","        if should_delete:\n","            provider.cleanup()\n","        else:\n","            print(f\"ðŸ’¾ Keeping PGVector table: {table_name}\")\n","\n","        del provider # Explicitly delete provider object\n","        del results # Clear search results\n","        clean_memory() # Force garbage collection\n","\n","        print(f\"\\nâœ… PGVector benchmark completed for {dataset_name}\")\n","        return result # Return the result entry for this dataset\n","\n","    except Exception as e:\n","        print(f\"âŒ Error during benchmark: {e}\")\n","        import traceback\n","        traceback.print_exc() # Print full traceback for debugging\n","        clean_memory()\n","        return None # Return None on error\n","    finally:\n","        # Clean up large objects from memory regardless of success or failure.\n","        del precomputed_vectors\n","        del query_embeddings\n","        del corpus_embeddings\n","        clean_memory()\n","\n","\n","# -------------------- MAIN EXECUTION BLOCK ---------------------\n","# This block handles the overall flow of the benchmark, including API key checks,\n","# client initialization, dataset discovery, and the interactive loop for running benchmarks.\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ðŸŽ¯ MAIR BENCHMARK - PGVECTOR EDITION (Google Colab)\")\n","print(\"=\"*70)\n","\n","print(\"\\nðŸ”‘ Checking API Keys and Database Connection...\")\n","\n","# Check Cohere API key availability.\n","if not COHERE_API_KEY:\n","    print(\"\\nâŒ Cohere API key required!\")\n","    print(\"Add it to Colab Secrets with key 'COHERE_API_KEY'\")\n","    exit(1) # Stop execution if key is missing\n","\n","import cohere\n","cohere_client = cohere.Client(COHERE_API_KEY) # Initialize Cohere client.\n","print(f\"âœ… Cohere client initialized\")\n","\n","# Test PostgreSQL connection.\n","try:\n","    test_conn = psycopg2.connect(**PG_CONN_PARAMS) # Attempt to connect to DB\n","    register_vector(test_conn) # Register vector type\n","    cur = test_conn.cursor() # Create cursor\n","    cur.execute(\"SELECT version();\") # Get PostgreSQL version\n","    pg_version = cur.fetchone()[0]\n","    cur.execute(\"SELECT extversion FROM pg_extension WHERE extname = 'vector';\") # Get pgvector version\n","    pgvector_version = cur.fetchone()\n","    cur.close() # Close cursor\n","    test_conn.close() # Close connection\n","    print(f\"âœ… PostgreSQL connection successful\")\n","    print(f\"   PostgreSQL: {pg_version.split(',')[0]}\")\n","    if pgvector_version:\n","        print(f\"   pgvector: v{pgvector_version[0]}\")\n","    else:\n","        print(f\"   âš ï¸ pgvector extension not found\")\n","except Exception as e:\n","    print(f\"âŒ PostgreSQL connection failed: {e}\")\n","    exit(1) # Stop execution if DB connection fails\n","\n","# Discover available MAIR datasets.\n","print(\"\\nðŸ“Š Discovering datasets...\")\n","mair_datasets, datasets_by_category, dataset_sizes = get_mair_datasets() # Use helper function\n","\n","if not mair_datasets: # If no datasets are found, inform the user and exit.\n","    print(\"âŒ No datasets found!\")\n","    print(f\"Please ensure datasets are in: {MAIR_COMBINED_PATH}\")\n","    exit(1)\n","\n","print(f\"\\nðŸ“‚ Available datasets by category ({len(mair_datasets)} total):\")\n","dataset_index = 1\n","dataset_map = {} # Map user-friendly index to actual dataset name\n","\n","# Display datasets grouped by category and sorted by size for easy selection.\n","for category in sorted(datasets_by_category.keys()):\n","    datasets = datasets_by_category[category]\n","    if datasets:\n","        # Sort datasets within each category by document count (descending).\n","        sorted_datasets = sorted(datasets, key=lambda d: dataset_sizes.get(d, 0), reverse=True)\n","        print(f\"\\n  ðŸ“ {category} ({len(sorted_datasets)} datasets):\")\n","        for dataset in sorted_datasets:\n","            size_str = format_size(dataset_sizes.get(dataset, 0)) # Format document count for display\n","            print(f\"     {dataset_index}. {dataset} ({size_str} docs)\")\n","            dataset_map[dataset_index] = dataset\n","            dataset_index += 1\n","\n","all_results = [] # List to store all benchmark results.\n","\n","# Main interactive benchmarking loop.\n","print(\"\\n\" + \"=\"*70)\n","print(\"ðŸš€ READY TO BENCHMARK!\")\n","print(\"=\"*70)\n","\n","while True:\n","    choice = input(f\"\\nâž¡ï¸ Enter dataset number (1-{len(dataset_map)}) or 'stop': \").strip().lower()\n","    if choice == \"stop\":\n","        break # Exit loop if user types 'stop'\n","\n","    try:\n","        idx = int(choice)\n","        if idx in dataset_map:\n","            dataset_name = dataset_map[idx] # Get the selected dataset name\n","        else:\n","            print(f\"âš ï¸ Invalid number. Please choose 1-{len(dataset_map)}\")\n","            continue # Ask for input again\n","    except ValueError:\n","        print(\"âš ï¸ Invalid input. Enter a number or 'stop'\")\n","        continue # Ask for input again\n","\n","    # Run the benchmark for the selected dataset.\n","    result = run_benchmark_pgvector(dataset_name, PG_CONN_PARAMS)\n","    if result: # If the benchmark run was successful, add its result.\n","        all_results.append(result)\n","\n","# -------------------- Final Summary and Analysis ---------------------\n","# After all selected datasets are processed, this section provides an overall summary\n","# of the benchmark results. It includes tables of key metrics, average performance\n","# comparisons, and insights into timing.\n","\n","if all_results:\n","    print(f\"\\n{'='*70}\")\n","    print(\"ðŸ BENCHMARK COMPLETE!\")\n","    print(f\"{'='*70}\")\n","\n","    df = pd.DataFrame(all_results) # Create a DataFrame from collected results.\n","    print(f\"\\nðŸ’¾ Results saved to: {CSV_PATH}\")\n","\n","    print(\"\\nðŸ“Š Summary Table:\")\n","    # Display key retrieval metrics and performance times.\n","    summary_cols = ['Dataset', 'Category', 'NDCG@10', 'MAP@10', 'Recall@100', 'Search_Mean_ms']\n","    available_cols = [col for col in summary_cols if col in df.columns] # Ensure columns exist\n","    print(df[available_cols].to_string(index=False))\n","\n","    print(\"\\nðŸ“ˆ Average Performance:\")\n","    # Calculate averages for numeric metrics.\n","    numeric_cols = ['NDCG@10', 'MAP@10', 'Recall@100', 'Search_Mean_ms', 'Upload_Total_s']\n","    valid_cols = [col for col in numeric_cols if col in df.columns]\n","    if valid_cols:\n","        avg_vals = df[valid_cols].mean()\n","        print(\"\\n  Retrieval Quality:\")\n","        for col in ['NDCG@10', 'MAP@10', 'Recall@100']:\n","            if col in valid_cols:\n","                print(f\"    {col}: {avg_vals[col]:.4f}\")\n","        print(\"\\n  Performance:\")\n","        if 'Search_Mean_ms' in valid_cols:\n","            print(f\"    Avg Search Time: {avg_vals['Search_Mean_ms']:.2f}ms\")\n","        if 'Upload_Total_s' in valid_cols:\n","            print(f\"    Avg Upload Time: {avg_vals['Upload_Total_s']:.2f}s\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"âœ¨ PGVector Benchmark Complete!\")\n","    print(\"=\"*70)\n","    print(\"\\nðŸ“Š Key Insights:\")\n","    print(f\"  â€¢ Tested {len(all_results)} dataset(s)\")\n","    print(f\"  â€¢ Using {EMBEDDING_MODEL} embeddings ({VECTOR_DIMENSION}D)\")\n","    print(f\"  â€¢ HNSW index for fast similarity search\")\n","    print(f\"  â€¢ Results: {CSV_PATH}\")\n","else:\n","    print(\"\\nâš ï¸ No results to display\")\n","\n","print(\"\\nðŸ’¡ Tip: PostgreSQL + pgvector is running locally in this Colab instance.\")\n","print(\"   Tables are temporary and will be lost when the runtime disconnects.\")"],"metadata":{"id":"EA-gkiHcSE7G"},"execution_count":null,"outputs":[]}]}